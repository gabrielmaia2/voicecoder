IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

Subscribe

        Cart 
        Create Account
        Personal Sign In 

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Institutional Sign In
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Conferences > 2021 IEEE Spoken Language Tec...
How Far Are We from Robust Voice Conversion: A Survey
Publisher: IEEE
Cite This
PDF
Tzu-hsien Huang ; Jheng-hao Lin ; Hung-yi Lee
All Authors
5
Cites in
Papers
576
Full
Text Views

    Alerts

Abstract
Document Sections

    1.
    INTRODUCTION
    2.
    VOICE CONVERSION
    3.
    EXPERIMENTAL SETTINGS
    4.
    ROBUSTNESS OF VOICE CONVERSION MODELS
    5.
    INFLUENCE OF MODEL COMPONENTS

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
Abstract:
Voice conversion technologies have been greatly improved in recent years with the help of deep learning, but their capabilities of producing natural sounding utterances in different conditions remain unclear. In this paper, we gave a thorough study of the robustness of known VC models. We also modified these models, such as the replacement of speaker embeddings, to further improve their performances. We found that the sampling rate and audio duration greatly influence voice conversion. All the VC models suffer from unseen data, but AdaIN-VC is relatively more robust. Also, the speaker embedding jointly trained is more suitable for voice conversion than those trained on speaker identification.
Published in: 2021 IEEE Spoken Language Technology Workshop (SLT)
Date of Conference: 19-22 January 2021
Date Added to IEEE Xplore : 25 March 2021
ISBN Information:
INSPEC Accession Number: 20511565
DOI: 10.1109/SLT48900.2021.9383498
Publisher: IEEE
Conference Location: Shenzhen, China
1. INTRODUCTION

Voice conversion (VC) techniques aim to convert the speaker characteristic of an utterance into that of the target speaker while preserving linguistic content information. In previous work, paired data of speakers were required to achieve VC. Recently, several models were proposed to utilize non-parallel data [1], [2], [3], [4], [5]. DGAN-VC [6] learns disentangled content and speaker information by adversarial training. StarGAN-VC [7] adopts conditional input to achieve many-to-many speaker voice conversion. However, both are restricted to performing VC among seen speakers during training. Zero-shot approaches [8], [9], [10], [11], [12] are then considered, where the models can perform VC among any speakers without fine-tuning. AdaIN-VC [13] applies instance normalization to meet this requirement. AUTOVC [14] employs pretrained d-vector [15] and information bottleneck for this purpose.
Sign in to Continue Reading
Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
More Like This
Integration strategies for audio-visual speech processing: applied to text-dependent speaker recognition

IEEE Transactions on Multimedia

Published: 2005
Perturbation and pitch normalization as enhancements to speaker recognition

2009 IEEE International Conference on Acoustics, Speech and Signal Processing

Published: 2009
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

Â© Copyright 2023 IEEE - All rights reserved.
