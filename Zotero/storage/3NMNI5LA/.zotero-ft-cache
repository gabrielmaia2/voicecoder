One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization
Ju-chieh Chou, Cheng-chieh Yeh, Hung-yi Lee
College of Electrical Engineering and Computer Science, National Taiwan University
{r06922020,r06942067,hungyilee}@ntu.edu.tw

arXiv:1904.05742v4 [cs.LG] 22 Aug 2019

Abstract
Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers. However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC. In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training. This is achieved by disentangling speaker and content representations with instance normalization (IN). Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker. In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision. Index Terms: Voice conversion, disentangled representations, generative model.
1. Introduction
VC aims to convert the non-linguistic information of the speech signals while maintaining the linguistic content the same. The non-linguistic information may refer to speaker identity [1, 2, 3], accent or pronunciation [4, 5] to name a few. VC can be useful in some down-stream tasks like multi-speaker text-tospeech [6, 7] and expressive speech synthesis [8, 9], and also some applications like speech enhancement [10, 11, 12] or pronunciation correction [4], and so on. In this paper, we will focus on the problem of speaker identity conversion.
Prior works on VC can be roughly categorized into two types, a supervised one and an unsupervised one. Supervised VC has achieved great performance [13, 14, 15, 16]. However, it requires frame-level alignment between source and target utterance. If there is a huge gap between source and target domain, inaccurate alignment may hurt the performance of the conversion. More important, collecting parallel data is difﬁcult and time-consuming, which make supervised VC not a desirable framework if we want to have the ﬂexibility of adapting it to some new domains.
Unsupervised VC recently became an actively investigated problem due to its efﬁciency in data collection. It means that we do not have to collect parallel data, but to utilize non-parallel data to train the VC system. Some works try to incorporate ASR system to perform unsupervised VC [17, 18, 19]. By translating the speech to phoneme posterior sequences, and then synthesizing the speech with the target domain synthesizer, unsupervised VC can be achieved. However, The performances of this kind of approaches highly depend on the accuracy of the ASR system, and will corrupt if the ASR system is not well-functioned. Some other works try to utilize deep generative model like VAE [20]

or GAN [21] to do unsupervised VC [22, 23, 24, 25, 26]. These works formulate VC as a domain mapping problem, aiming to learn networks that can transfer utterances among different domains. These works are able to generate speech with good quality and can convert the speaker characteristics successfully. However, the major limitation of these works is that they can not synthesize the voice of the speakers who were never seen in training phase.
Speech signals inherently carry both static information and linguistic information. The static part such as speaker, acoustic condition is time-independent and merely changes during the whole utterance, while the linguistic part may change dramatically every several frames. Here we assume an utterance can be factorized into a speaker representation plus a content representation. To disentangle speaker and content representation, our model consists of three components: a speaker encoder, a content encoder and a decoder in Fig. 1. The speaker encoder is trained to encode the speaker information into the speaker representation. The content encoder is trained to encode only the linguistic information into the content representation. And then the task of the decoder is to synthesize the voice back by combining these two representations. We utilize instance normalization [27] without afﬁne transformation in the content encoder to normalize the channel statistics, which control the global information. In this way, the global information such as speaker information is removed from the representation encoded by the content encoder. And also, adaptive instance normalization [28] is utilized in the decoder, the corresponding afﬁne parameters are provided by the speaker encoder. By doing this, the global information needed in the decoder is controlled by the speaker encoder. With the designed architecture, our model is encouraged to learn factorized representations. This kind of factorization enable our model to perform one-shot voice conversion as follows: with one utterance from source speaker and another utterance from target speaker, we ﬁrst extract the speaker representation from the target utterance, and then extract the content representation from the source utterance, and ﬁnally combine them with the decoder to generate the converted result as in Fig. 1. It is worth mentioning that our model does not require any speaker label of the utterances during the training process, which makes the data collection easier. Interestingly, the speaker encoder learns a meaningful speaker embeddings even if we do not provide any speaker label.
In terms of applying factorization techniques to speech, some prior works proposed using adversarial training to remove certain attributes from an utterance [29, 26]. However, with the cost of training an extra discriminator network, lots more computational resources are used. Also, adversarial training suffers from instability problem, which makes the training process difﬁcult. In our proposed approach, we simply use the technique of instance normalization instead of adversarial training to remove the speaker information in an utterance, which substantially re-

Figure 1: Model overview. Es is speaker encoder; Ec is content encoder and D is decoder. IN is instance normalization layer without afﬁne transformation. AdaIN represents adaptive instance normalization layer.

duces the computation and makes the training process easier. Our contribution is three-fold:
1. Our proposed model is able to do one-shot VC without any supervision.
2. The efﬁcacy of instance normalization on disentangling representations for VC is veriﬁed.
3. We demonstrate that our model is able to learn meaningful speaker embedding as a side effect.

2. Proposed Approach
2.1. Variational autoencoder
Let x be the acoustic feature segment, and X be the collection of all the acoustic segments in the training data. Let Es be the speaker encoder, Ec be the content encoder, and D be the decoder. Es is trained to generate the speaker representation zs. And Ec is trained to generate content representation zc. We assume that p(zc|x) is a conditionally independent Gaussian distribution with unit variance as in [30], which means p(zc|x) = N (Ec(x), I). The reconstruction loss is given as in Eq. 1.

Lrec(θEs , θEc , θD) =

E

[

D(Es(x), zc) − x

1 1

].

x∼p(x),zc ∼p(zc |x)

(1)

We uniformly sample an acoustic segment x from X during training process (that is, p(x) in Eq. 1 is an uniform distribution over X ). To match the posterior distribution p(zc|x) to the prior N (0, I), the KL divergence loss will be minimized. Since we assume unit variance, the KL divergence reduces to L2 regularization. The KL divergence term is given as in Eq. 2.

Lkl(θEc ) =

E

[

Ec(x)2

2 2

].

(2)

x∼p(x)

The objective function for VAE training is to minimize the combination of the two terms with weighted hyper-parameters λrec and λkl.

min
θEs ,θEc ,θD

L(θEs ,

θEc ,

θD)

=

λrec Lrec

+

λkl Lkl

(3)

2.2. Instance Normalization for Feature Disentanglement

At the ﬁrst glance, it is unclear how could the two encoders Es and Ec encode speaker and content information respectively based on the description in Section 2.1. In this paper, we ﬁnd that simply adding Instance normalization (IN) without afﬁne transformation to Ec can remove the speaker information while preserving the content information. Similar idea has been veriﬁed to be effective for style transfer in computer vision [28].
The formula of instance normalization (IN) without afﬁne transformation is given as in Eq. 5. Here M is the feature map of the output of the previous convolutional layer, and Mc represents the c-th channel, which is a W -dimensional array. Here each channel is an array instead of a matrix because 1-D convolution is applied rather than 2-D. To apply IN, we have to compute the mean µc and standard variation σc of the c-th channel ﬁrst.

1W

µc = W

Mc[w],

w=1

(4)

σc =

1 W

W
(Mc[w] − µc)2 + ,

w=1

where Mc[w] is the w-th element in Mc. in Eq. 4 is simply a small value to avoid numerical instability. Then in IN, each element in the array Mc is normalized into Mc as below.

Mc[w]

=

Mc[w] − σc

µc

(5)

The normalized Mc are processed by the following deep network layers. We utilize IN layer in content encoder to prevent the content encoder from learning domain information. So as to enforce the model to extract speaker information from speaker encoder and content information from content encoder respectively.
To further enforce the speaker encoder to generate speaker representation, we provide the speaker information to decoder by adaptive instance normalization (adaIN) layer [28]. In adaIN layer, the decoder ﬁrst normalizes the global information by IN, and the speaker encoder provide the global information. The formula is given as followed.

Mc[w]

=

γc

Mc

[w] − σc

µc

+ βc.

(6)

µc and σc are computed as Eq. 4. γc and βc for each channel are the linear transformation of the output of speaker encoder Es.

3. Implementation Details
3.1. Architecture
We use Conv1d layers in encoders and decoder to process all the frequency information at a time as in Fig. 2. The ConvBank layer is used in both the speaker encoder and content encoder to better capture long-term information [31]. We apply average pooling over time to the speaker encoder so as to enforce the speaker encoder to learn global information only. Instance normalization layers are used in content encoder to normalize the global information. PixelShufﬂe1d [32] layer is used in the decoder for upsampling. adaIN layer is used to provide global information to the decoder. The speaker representation zs is ﬁrst processed by a residual DNN, and then transformed by an afﬁne layer before get into each adaIN layer.

Figure 2: The architecture of the encoders and decoder.
3.2. Acoustic feature
We use mel-scale spectrograms as the acoustic feature. We ﬁrst trimmed out the silence and normalize the volume, and then convert the audio to 24kHz. After that, we perform STFT to the audio with a 50 milliseconds window length, a 12.5 milliseconds hop length, and a 2048 STFT window size. And then transformed magnitude of the spectrograms to 512-bin melscale spectrograms. The mel-scale spectrograms are normalized by subtract mean and divide standard deviation. To convert the mel-scale spectrograms back to waveform, we apply the approximate inverse linear transformation to recover the linearscale spectrograms [35]. And phase is reconstructed by GrifﬁnLim algorithm with 100 iterations.
3.3. Training details
We trained the proposed model by ADAM optimizer with a 0.0005 learning rate, and β1 = 0.9, β2 = 0.999. We set the batch size to 256. To prevent the model from overﬁtting, we apply dropout to each layers with a 0.5 dropout rate and a 0.0001 weight decay. λrec is set to 10 and λkl is set to 0.01. We trained the model for 200000 iterations (mini-batches). Further details may be found in our implementation code: https://github.com/jjery2243542/ adaptive_voice_conversion
4. Experiments
We evaluated our model on CSTR VCTK Corpus [34]. The audio data were produced by 109 speakers in English with different accents. We randomly selected 20 speakers’ utterances as our testing set, and the rest utterances will be split to 90% training set and 10% validation set. While we set the segment length to be 128 during training, because of the fully-convolutional architecture, the model can process input with any length at in-

ference stage. After removing all the utterances less than 128 frames, the training set contains about 16000 utterances.
4.1. Evaluation of disentanglement
To see the effect of IN layer, we performed an ablation study to verify it could help content encoder remove the information of speaker characteristics. We trained another network (5layer DNN with 1024 neurons and ReLU activation) to classify speaker identity given the latent representation encoded by the content encoder. We compared the classiﬁcation accuracy under three settings which were ”content encoder with IN”, ”content encoder without IN” and ”content encoder without IN while speaker encoder with IN”, respectively. The results are shown in Table 1. We can see that the classiﬁcation accuracy is apparently lower when IN is applied to the content encoder. But we also found the accuracy was not as high as expected even if we did not apply IN to the content encoder. This was probably because by the fact that the speaker encoder is able to control the channel statistics of decoder by adaIN, the whole model tends to learn speaker information from the speaker encoder rather than from the content encoder. To further conﬁrm this assumption, we tested the classiﬁcation accuracy under the third settings mentioned above, which was not to apply IN to the content encoder but applied it to the speaker encoder. As we can see, due to the average pooling over time property combined with IN layer (output zero-vector), the speaker encoder could no longer possess the complete speaker information, thus the whole model tended to ”ﬂow” more speaker information through content encoder, increasing the classiﬁcation accuracy.
Table 1: The accuracy for speaker identity prediction on content representation. Smaller value means less speaker information in the content representation.

Ec with IN Ec w/o IN Ec w/o IN + Es with IN

0.375

0.658

0.746

4.2. Speaker embedding visualization
We found that the speaker encoder learned meaningful embeddings related to speakers even if we did not explicitly add any objective or constraint to the encoder [36]. We inputted both seen and unseen (during training) speakers’ utterances through speaker encoder and plotted their embeddings in 2D space with t-SNE in Fig. 3. We found that utterances spoken by different speakers were well-separated. We also conducted experiments on classifying speaker id with these embeddings. The setting was the same as subsection 4.1. Seen speakers achieved 0.9973 accuracy and unseen speakers achieved 0.9998 accuracy, indicating that the speaker encoder learned reasonable representations in the embedding space.
4.3. Objective evaluation
4.3.1. Global variance
To show that our model is able to convert speaker characteristic, we used the global variance (GV) as the visualization of spectral distribution. Global variance has been used as a way to see whether voice conversion result match to the target speaker in terms of variance distribution [37]. We evaluated the global variance for each of the frequency index for 4 conversion examples: male to male, male to female, female to male, and female

Figure 3: The visualization of speaker embedding. ’x’ are female speakers and ’o’ are male speakers. Segments are randomly sampled from validation set and testing set.

Figure 5: The heatmaps of the spectrogram: the upper left is an utterance spoken by a female speaker. The upper right is the converted result to a male speaker. The lower left is an utterance spoken by a male speaker. The lower right is the converted result to a female speaker.

Figure 4: The variance distribution of converted result and target speaker utterances. 100 randomly chosen utterances and converted result are used to calculate the variance.
to female. The results are shown in Fig. 4, and we found that our generated samples did match to the target speaker in terms of variance distribution.
4.3.2. Spectrograms example
Some examples of spectrogram heatmaps are shown in Fig. 5. We can see that our model is able to transform the fundamental frequency (f0) and keep the original phonetic content in both male to female conversion and female to male conversion.
4.4. Subjective evaluation
Subjective evaluation was performed on converted voice (including male to male, male to female, female to male and female to female, in total four pairs of speakers). The speakers of these four pairs were all unseen during training time, so the converted result of each pair was outputted from our proposed approach by using only one source utterance and one target ut-

Figure 6: Similarity test. The left one is the comparison to source speaker’s utterance. The right one is the comparison to target speaker’s utterance.
terance. We then asked the human participants to evaluate the similarity between two utterances with a 4-scale score indicating same absolutely sure, same not sure, different not sure, and different absolutely sure. The two utterances were one converted result with either one source speaker utterance or one target speaker utterance. The results are in Fig. 6. Our model is able to generate the voice similar to target speaker’s. The demo can be found at https://jjery2243542.github.io/ one-shot-vc-demo/.
5. Conclusion
We proposed a novel approach to tackle one-shot unsupervised VC by applying instance normalization to enforce the model to learn factorized representations. In this way, we can perform VC to unseen speakers with only one utterance. Subjective and objective evaluations showed good result in terms of similarity to target speakers. And also, the disentanglement experiments and visualization showed that in our proposed approach, the speaker encoder learns a meaningful embedding space without any supervision.

6. Acknowledgement
This work is supported by Nvidia.
7. References
[1] H. Miyoshi, Y. Saito, S. Takamichi, and H. Saruwatari, “Voice conversion using sequence-to-sequence learning of context posterior probabilities,” arXiv preprint arXiv:1704.02360, 2017.
[2] T. Nakashika, T. Takiguchi, and Y. Ariki, “Voice conversion based on speaker-dependent restricted boltzmann machines,” IEICE TRANSACTIONS on Information and Systems, vol. 97, no. 6, pp. 1403–1410, 2014.
[3] T. Kinnunen, L. Juvela, P. Alku, and J. Yamagishi, “Non-parallel voice conversion using i-vector plda: Towards unifying speaker veriﬁcation and transformation,” in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE, 2017, pp. 5535–5539.
[4] N. Hojo, H. Kameoka, K. Tanaka, and T. Kaneko, “Automatic speech pronunciation correction with dynamic frequency warping-based spectral conversion,” in 2018 26th European Signal Processing Conference (EUSIPCO). IEEE, 2018, pp. 2310– 2314.
[5] L.-W. Chen, H.-Y. Lee, and Y. Tsao, “Generative adversarial networks for unpaired voice transformation on impaired speech,” arXiv preprint arXiv:1810.12656, 2018.
[6] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen, R. Pang, I. L. Moreno, Y. Wu et al., “Transfer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis,” in Advances in Neural Information Processing Systems, 2018, pp. 4485–4495.
[7] A. Tjandra, S. Sakti, and S. Nakamura, “Machine speech chain with one-shot speaker adaptation,” arXiv preprint arXiv:1803.10525, 2018.
[8] R. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stanton, J. Shor, R. J. Weiss, R. Clark, and R. A. Saurous, “Towards end-to-end prosody transfer for expressive speech synthesis with tacotron,” arXiv preprint arXiv:1803.09047, 2018.
[9] Y. Wang, D. Stanton, Y. Zhang, R. Skerry-Ryan, E. Battenberg, J. Shor, Y. Xiao, F. Ren, Y. Jia, and R. A. Saurous, “Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,” arXiv preprint arXiv:1803.09017, 2018.
[10] S. A. Kumar and C. S. Kumar, “Improving the intelligibility of dysarthric speech towards enhancing the effectiveness of speech therapy,” in Advances in Computing, Communications and Informatics (ICACCI), 2016 International Conference on. IEEE, 2016, pp. 1000–1005.
[11] T. Kaneko, H. Kameoka, N. Hojo, Y. Ijima, K. Hiramatsu, and K. Kashino, “Generative adversarial network-based postﬁlter for statistical parametric speech synthesis,” in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE, 2017, pp. 4910–4914.
[12] K. Tanaka, T. Kaneko, N. Hojo, and H. Kameoka, “Wavecyclegan: Synthetic-to-natural speech waveform conversion using cycle-consistent adversarial networks,” arXiv preprint arXiv:1809.10288, 2018.
[13] S. H. Mohammadi and A. Kain, “Voice conversion using deep neural networks with speaker-independent pre-training,” in Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 19–23.
[14] T. Nakashika, T. Takiguchi, and Y. Ariki, “High-order sequence modeling using speaker-dependent recurrent temporal restricted boltzmann machines for voice conversion,” in Fifteenth Annual Conference of the International Speech Communication Association, 2014.
[15] L. Sun, S. Kang, K. Li, and H. Meng, “Voice conversion using deep bidirectional long short-term memory based recurrent neural networks,” in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4869– 4873.

[16] L.-H. Chen, Z.-H. Ling, L.-J. Liu, and L.-R. Dai, “Voice conversion using deep neural networks with layer-wise generative training,” IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 22, no. 12, pp. 1859–1872, 2014.
[17] C.-c. Yeh, P.-c. Hsu, J.-c. Chou, H.-y. Lee, and L.-s. Lee, “Rhythm-ﬂexible voice conversion without parallel data using cycle-gan over phoneme posteriorgram sequences,” arXiv preprint arXiv:1808.03113, 2018.
[18] L. Sun, H. Wang, S. Kang, K. Li, and H. M. Meng, “Personalized, cross-lingual tts using phonetic posteriorgrams.” in INTERSPEECH, 2016, pp. 322–326.
[19] F.-L. Xie, F. K. Soong, and H. Li, “A kl divergence and dnn-based approach to voice conversion without parallel training sentences.” in INTERSPEECH, 2016, pp. 287–291.
[20] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013.
[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in neural information processing systems, 2014, pp. 2672–2680.
[22] T. Kaneko and H. Kameoka, “Parallel-data-free voice conversion using cycle-consistent adversarial networks,” arXiv preprint arXiv:1711.11293, 2017.
[23] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, “Stargan-vc: Non-parallel many-to-many voice conversion with star generative adversarial networks,” arXiv preprint arXiv:1806.02169, 2018.
[24] ——, “Acvae-vc: Non-parallel many-to-many voice conversion with auxiliary classiﬁer variational autoencoder,” arXiv preprint arXiv:1808.05092, 2018.
[25] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang, “Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks,” arXiv preprint arXiv:1704.00849, 2017.
[26] J.-c. Chou, C.-c. Yeh, H.-y. Lee, and L.-s. Lee, “Multi-target voice conversion without parallel data by adversarially learning disentangled audio representations,” arXiv preprint arXiv:1804.02812, 2018.
[27] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Instance normalization: The missing ingredient for fast stylization,” arXiv preprint arXiv:1607.08022, 2016.
[28] X. Huang and S. Belongie, “Arbitrary style transfer in real-time with adaptive instance normalization,” in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1501– 1510.
[29] W.-N. Hsu, Y. Zhang, R. J. Weiss, Y.-A. Chung, Y. Wang, Y. Wu, and J. Glass, “Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization,” 2018.
[30] M.-Y. Liu, T. Breuel, and J. Kautz, “Unsupervised image-toimage translation networks,” in Advances in Neural Information Processing Systems, 2017, pp. 700–708.
[31] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., “Tacotron: Towards end-to-end speech syn,” arXiv preprint arXiv:1703.10135, 2017.
[32] W. Shi, J. Caballero, F. Husza´r, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang, “Real-time single image and video super-resolution using an efﬁcient sub-pixel convolutional neural network,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1874–1883.
[33] J. Engel, K. K. Agrawal, S. Chen, I. Gulrajani, C. Donahue, and A. Roberts, “Gansynth: Adversarial neural audio synthesis,” arXiv preprint arXiv:1902.08710, 2019.
[34] C. Veaux, J. Yamagishi, K. MacDonald et al., “Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit,” 2016.

[35] J. Engel, K. K. Agrawal, S. Chen, I. Gulrajani, C. Donahue, and A. Roberts, “Gansynth: Adversarial neural audio synthesis,” arXiv preprint arXiv:1902.08710, 2019.
[36] M. Ravanelli and Y. Bengio, “Learning speaker representations with mutual information,” arXiv preprint arXiv:1812.00271, 2018.
[37] T. Toda and K. Tokuda, “A speech parameter generation algorithm considering global variance for hmm-based speech synthesis,” IEICE TRANSACTIONS on Information and Systems, vol. 90, no. 5, pp. 816–824, 2007.

