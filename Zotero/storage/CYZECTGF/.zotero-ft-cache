Pruning and Quantization for Deep Neural Network Acceleration: A Survey
Tailin Lianga,b, John Glossnera,b,c, Lei Wanga, Shaobo Shia,b and Xiaotong Zhanga,∗
aSchool of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China bHua Xia General Processor Technologies, Beijing 100080, China cGeneral Processor Technologies, Tarrytown, NY 10591, United States

ARTICLE INFO
Keywords: convolutional neural network neural network acceleration neural network quantization neural network pruning low-bit mathematics

ABSTRACT
Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the ﬁeld of computer vision. However, complex network architectures challenge eﬃcient real-time deployment and require signiﬁcant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed oﬄine or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-oﬀs in element-wise, channel-wise, shape-wise, ﬁlter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.

arXiv:2101.09671v3 [cs.CV] 15 Jun 2021

1. Introduction
Deep Neural Networks (DNNs) have shown extraordinary abilities in complicated applications such as image classiﬁcation, object detection, voice synthesis, and semantic segmentation [138]. Recent neural network designs with billions of parameters have demonstrated human-level capabilities but at the cost of signiﬁcant computational complexity. DNNs with many parameters are also time-consuming to train [26]. These large networks are also diﬃcult to deploy in embedded environments. Bandwidth becomes a limiting factor when moving weights and data between Compute Units (CUs) and memory. Over-parameterization is the property of a neural network where redundant neurons do not improve the accuracy of results. This redundancy can often be removed with little or no accuracy loss [225].
Figure 1 shows three design considerations that may contribute to over-parameterization: 1) network structure, 2) network optimization, and 3) hardware accelerator design. These design considerations are speciﬁc to Convolutional Neural Networks (CNNs) but also generally relevant to DNNs.
Network structure encompasses three parts: 1) novel components, 2) network architecture search, and 3) knowledge distillation. Novel components is the design of eﬃcient blocks such as separable convolution, inception blocks, and residual blocks. They are discussed in Section 2.4. Network components also encompasses the types of connections within layers. Fully connected deep neural networks require 2
∗Corresponding author tailin.liang@xs.ustb.edu.cn (T. Liang); jglossner@ustb.edu.cn (J.
Glossner); wanglei@ustb.edu.cn (L. Wang); sbshi@hxgpt.com (S. Shi); zxt@ies.ustb.edu.cn (X. Zhang)
ORCID(s): 0000-0002-7643-912X (T. Liang)

connections between neurons. Feed forward layers reduce connections by considering only connections in the forward path. This reduces the number of connections to . Other types of components such as dropout layers can reduce the number of connections even further.
Network Architecture Search (NAS) [63], also known as network auto search, programmatically searches for a highly eﬃcient network structure from a large predeﬁned search space. An estimator is applied to each produced architecture. While time-consuming to compute, the ﬁnal architecture often outperforms manually designed networks.
Knowledge Distillation (KD) [80, 206] evolved from knowledge transfer [27]. The goal is to generate a simpler compressed model that functions as well as a larger model. KD trains a student network that tries to imitate a teacher network. The student network is usually but not always smaller and shallower than the teacher. The trained student model should be less computationally complex than the teacher.
Network optimization [137] includes: 1) computational convolution optimization, 2) parameter factorization, 3) network pruning, and 4) network quantization. Convolution operations are more eﬃcient than fully connected computations because they keep high dimensional information as a 3D tensor rather than ﬂattening the tensors into vectors that removes the original spatial information. This feature helps CNNs to ﬁt the underlying structure of image data in particular. Convolution layers also require signiﬁcantly less coeﬃcients compared to Fully Connected Layers (FCLs). Computational convolution optimizations include Fast Fourier Transform (FFT) based convolution [168], Winograd convolution [135], and the popular image to column (im2col) [34] approach. We discuss im2col in detail in Section 2.3 since it is directly

T Liang et al.: Preprint submitted to Elsevier

Page 1 of 41

Survey on pruning and quantization CNN Acceleration [40, 39, 142, 137, 194, 263, 182]

Network Structure Novel Components Network Architecture Search [63] Knowledge Distillation [80, 206]

Network Optimization Convolution Optimization
Factorization Pruning [201, 24, 12, 250]
Quantization [131, 87]

Hardware Accelerator [151, 202]

Platform CPU GPU ASIC
FPGA [86, 3, 234, 152]

Optimization Lookup Table Computation Reuse Memory Optimization
...

Figure 1: CNN Acceleration Approaches: Follow the sense from designing to implementing, CNN acceleration could fall into three categories, structure design (or generation), further optimization, and specialized hardware.

related to general pruning techniques. Parameter factorization is a technique that decomposes
higher-rank tensors into lower-rank tensors simplifying memory access and compressing model size. It works by breaking large layers into many smaller ones, thereby reducing the number of computations. It can be applied to both convolutional and fully connected layers. This technique can also be applied with pruning and quantization.
Network pruning [201, 24, 12, 250] involves removing parameters that don’t impact network accuracy. Pruning can be performed in many ways and is described extensively in Section 3.
Network quantization [131, 87] involves replacing datatypes with reduced width datatypes. For example, replacing 32-bit Floating Point (FP32) with 8-bit Integers (INT8). The values can often be encoded to preserve more information than simple conversion. Quantization is described extensively in Section 4.
Hardware accelerators [151, 202] are designed primarily for network acceleration. At a high level they encompass entire processor platforms and often include hardware optimized for neural networks. Processor platforms include specialized Central Processing Unit (CPU) instructions, Graphics Processing Units (GPUs), Application Speciﬁc Integrated Circuits (ASICs), and Field Programmable Gate Arrays (FPGAs).
CPUs have been optimized with specialized Artiﬁcial Intelligence (AI) instructions usually within specialized Single Instruction Multiple Data (SIMD) units [49, 11]. While CPUs can be used for training, they have primarily been used for inference in systems that do not have specialized inference accelerators.
GPUs have been used for both training and inference. nVidia has specialized tensor units incorporated into their GPUs that are optimized for neural network acceleration [186]. AMD [7], ARM [10], and Imagination [117] also have GPUs with instructions for neural network acceleration.
Specialized ASICs have also been designed for neural network acceleration. They typically target inference at the edge, in security cameras, or on mobile devices. Examples

include: General Processor Technologies (GPT) [179], ARM, nVidia, and 60+ others [202] all have processors targeting this space. ASICs may also target both training and inference in datacenters. Tensor processing units (TPU) from Google [125], Habana from Intel [169], Kunlun from Baidu [191], Hanguang from Alibaba [124], and Intelligence Processing Unit (IPU) from Graphcore [121].
Programmable reconﬁgurable FPGAs have been used for neural network acceleration [86, 3, 234, 152]. FPGAs are widely used by researchers due to long ASIC design cycles. Neural network libraries are available from Xilinx [128] and Intel [69]. Speciﬁc neural network accelerators are also being integrated into FPGA fabrics [248, 4, 203]. Because FPGAs operate at the gate level, they are often used in low-bit width and binary neural networks [178, 267, 197].
Neural network speciﬁc optimizations are typically incorporated into custom ASIC hardware. Lookup tables can be used to accelerate trigonometric activation functions [46] or directly generate results for low bit-width arithmetic [65], partial products can be stored in special registers and reused [38], and memory access ordering with specialized addressing hardware can all reduce the number of cycles to compute a neural network output [126]. Hardware accelerators are not the primary focus of this paper. However, we do note hardware implementations that incorporate speciﬁc acceleration techniques. Further background information on eﬃcient processing and hardware implementations of DNNs can be found in [225].
We summarize our main contributions as follows:
• We provide a review of two network compression techniques: pruning and quantization. We discuss methods of compression, mathematical formulations, and compare current State-Of-The-Art (SOTA) compression methods.
• We classify pruning techniques into static and dynamic methods, depending if they are done oﬄine or at runtime, respectively.
• We analyze and quantitatively compare quantization

T Liang et al.: Preprint submitted to Elsevier

Page 2 of 41

Survey on pruning and quantization

techniques and frameworks.
• We provide practical guidance on quantization and pruning.
This paper focuses primarily on network optimization for convolutional neural networks. It is organized as follows: In Section 2 we give an introduction to neural networks and speciﬁcally convolutional neural networks. We also describe some of the network optimizations of convolutions. In Section 3 we describe both static and dynamic pruning techniques. In Section 4 we discuss quantization and its effect on accuracy. We also compare quantization libraries and frameworks. We then present quantized accuracy results for a number of common networks. We present conclusions and provide guidance on appropriate application use in Section 5. Finally, we present concluding comments in Section 7.
2. Convolutional Neural Network
Convolutional neural networks are a class of feed-forward DNNs that use convolution operations to extract features from a data source. CNNs have been most successfully applied to visual-related tasks however they have found use in natural language processing [95], speech recognition [2], recommendation systems [214], malware detection [223], and industrial sensors time series prediction [261]. To provide a better understanding of optimization techniques, in this section, we introduce the two phases of CNN deployment - training and inference, discuss types of convolution operations, describe Batch Normalization (BN) as an acceleration technique for training, describe pooling as a technique to reduce complexity, and describe the exponential growth in parameters deployed in modern network structures.
2.1. Deﬁnitions This section summarizes terms and deﬁnitions used to
describe neural networks as well as acronyms collected in Table 1.
• Coeﬃcient - A constant by which an algebraic term is multiplied. Typically, a coeﬃcient is multiplied by the data in a CNN ﬁlter.
• Parameter - All the factors of a layer, including coeﬃcients and biases.
• Hyperparameter - A predeﬁned parameter before network training, or ﬁne-tunning (re-training).
• Activation ( ∈ ℝℎ× × ) - The activated (e.g., ReLu, Leaky, Tanh, etc.) output of one layer in a multi-layer network architecture, typically in height ℎ, width , and channel . The ℎ × matrix is sometimes called an activation map. We also denote activation as output ( ) when the activation function does not matter.
• Feature ( ∈ ℝℎ× × ) - The input data of one layer, to distinguish the output . Generally the feature for the current layer is the activation of the previous layer.

• Kernel ( ∈ ℝ 1× 2 ) - Convolutional coeﬃcients for a channel, excluding biases. Typically they are square (e.g. 1 = 2) and sized 1, 3, 7.
• Filter ( ∈ ℝ 1× 2× × ) - Comprises all of the kernels corresponding to the channels of input features. The ﬁlter’s number, , results in diﬀerent output channels.
• Weights - Two common uses: 1) kernel coeﬃcients when describing part of a network, and 2) all the trained parameters in a neural network model when discussing the entire network.
2.2. Training and Inference CNNs are deployed as a two step process: 1) training and
2) inference. Training is performed ﬁrst with the result being either a continuous numerical value (regression) or a discrete class label (classiﬁcation). Classiﬁcation training involves applying a given annotated dataset as an input to the CNN, propagating it through the network, and comparing the output classiﬁcation to the ground-truth label. The network weights are then updated typically using a backpropagation strategy such as Stochastic Gradient Descent (SGD) to reduce classiﬁcation errors. This performs a search for the best weight values. Backpropogation is performed iteratively until a minimum acceptable error is reached or no further reduction in error is achieved. Backpropagation is compute intensive and traditionally performed in data centers that take advantage of dedicated GPUs or specialized training accelerators such as TPUs.
Fine-tuning is deﬁned as retraining a previously trained model. It is easier to recover the accuracy of a quantized or pruned model with ﬁne-tuning versus training from scratch.
CNN inference classiﬁcation takes a previously trained classiﬁcation model and predicts the class from input data not in the training dataset. Inference is not as computationally intensive as training and can be executed on edge, mobile, and embedded devices. The size of the inference network executing on mobile devices may be limited due to memory, bandwidth, or processing constraints [79]. Pruning discussed in Section 3 and quantization discussed in Section 4 are two techniques that can alleviate these constraints.
In this paper, we focus on the acceleration of CNN inference classiﬁcation. We compare techniques using standard benchmarks such as ImageNet [122], CIFAR [132], and MNIST [139]. The compression techniques are general and the choice of application domain doesn’t restrict its use in object detection, natural language processing, etc.
2.3. Convolution Operations The top of Figure 2 shows a 3-channel image (e.g., RGB)
as input to a convolutional layer. Because the input image has 3 channels, the convolution kernel must also have 3 channels. In this ﬁgure four 2 × 2 × 3 convolution ﬁlters are shown, each consisting of three 2 × 2 kernels. Data is received from all 3 channels simultaneously. 12 image values are multiplied with the kernel weights producing a single output. The kernel is moved across the 3-channel image sharing the 12 weights.

T Liang et al.: Preprint submitted to Elsevier

Page 3 of 41

Survey on pruning and quantization

Table 1 Acronyms and Abbreviations

Acronym
2D 3D FP16 FP32 INT16 INT8 IR OFA RGB SOTA
AI BN CBN CNN DNN EBP FCL FCN FLOP GAP GEMM GFLOP ILSVRC Im2col KD LRN LSTM MAC NAS NN PTQ QAT ReLU RL RNN SGD STE
ASIC AVX-512 CPU CU FPGA GPU HSA ISA PE SIMD SoC
DPP FFT FMA KL-divergence LASSO MDP OLS

Explanation
Two Dimensional Three Dimensional 16-Bit Floating-Point 32-Bit Floating-Point 16-Bit Integer 8-Bit Integer Intermediate Representation One-For-All Red, Green, And Blue State of The Art
Artiﬁcial Inteligence Batch Normalization Conditional Batch Normalization Convolutional Neural Network Deep Neural Network Expectation Back Propagation Fully Connected Layer Fully Connected Networks Floating-Point Operation Global Average Pooling General Matrix Multiply Giga Floating-Point Operation Imagenet Large Visual Recognition Challenge Image To Column Knowledge Distillation Local Response Normalization Long Short Term Memory Multiply Accumulate Network Architecture Search Neural Network Post Training Quantization Quantization Aware Training Rectiﬁed Linear Unit Reinforcement Learning Recurrent Neural Network Stochastic Gradient Descent Straight-Through Estimator
Application Speciﬁc Integrated Circuit Advance Vector Extension 512 Central Processing Unit Computing Unit Field Programmable Gate Array Graphic Processing Unit Heterogeneous System Architecture Instruction Set Architectures Processing Element Single Instruction Multiple Data System on Chip
Determinantal Point Process Fast Fourier Transfer Fused Multiply-Add Kullback-Leibler Divergence Least Absolute Shrinkage And Selection Operator Markov Decision Process Ordinary Least Squares

If the input image is 12 × 12 × 3 the resulting output will be 11 × 11 × 1 (using a stride of 1 and no padding). The ﬁlters work by extracting multiple smaller bit maps known as feature maps. If more ﬁlters are desired to learn diﬀerent features they can be easily added. In this case 4 ﬁlters are shown resulting in 4 feature maps.
The standard convolution operation can be computed in

Standard Convolution

Separable Convolution

Depth-wise Convolution

Point-wise Convolution

Figure 2: Separable Convolution: A standard convolution is decomposed into depth-wise convolution and point-wise convolution to reduce both the model size and computations.

parallel using a GEneral Matrix Multiply (GEMM) library [60]. Figure 3 shows a parallel column approach. The 3D tensors are ﬁrst ﬂattened into 2D matrices. The resulting matrices are multiplied by the convolutional kernel which takes each input neuron (features), multiplies it, and generates output neurons (activations) for the next layer [138].

Output Features

14 20 15 24

12 24 17 26

Kernels

11 11 01 22 11 10

10 21 12 01 21 20

Input Features (Activations)

120 113 022

021 032 110

120 013 332

121102031201
201331322113
* 1 1 0 2 0 3 1 1 0 1 3 3
132222101332

11 10 20 21 12 11 12 11 01 12 12 00

14 12
20 24
= 15 17
24 26

Figure 3: Convolution Performance Optimization: From traditional convolution (dot squared) to image to column (im2col) GEMM approach, adopted from [34]. The red and green boxes indicate ﬁlter-wise and shape-wise elements, respectively.

+1 =

∑ = activate

∗+

(1)

=1

Equation 1 shows the layer-wise mathematical representation of the convolution layer where represents the weights (ﬁlters) of the tensor with input channels and output channels, represents the bias vector, and represents the input feature tensor (typically from the activation of previous layer
−1). is the activated convolutional output. The goal of compression is to reduce the size of the and (or ) without aﬀecting accuracy.

T Liang et al.: Preprint submitted to Elsevier

Page 4 of 41

Survey on pruning and quantization
as in the top of Figure 5. However, the number of computations can be reduced by expanding the network width with four types of ﬁlters as shown in Figure 5. The concatenated result performs better than one convolutional layer with same computation workloads [226].

Figure 4: Fully Connected Layer: Each node in a layer connects to all the nodes in the next layer, and every line corresponds to a weight value

……… ………

Figure 4 shows a FCL - also called dense layer or dense connect. Every neuron is connected to each other neuron in a crossbar conﬁguration requiring many weights. As an example, if the input and output channel are 1024 and 1000, respectively, the number of parameters in the ﬁlter will be a million by 1024 × 1000. As the image size grows or the number of features increase, the number of weights grows rapidly.
2.4. Eﬃcient Structure The bottom of Figure 2 shows separable convolution im-
plemented in MobileNet [105]. Separable convolution assembles a depth-wise convolution followed by a point-wise convolution. A depth-wise convolution groups the input feature by channel, and treats each channel as a single input tensor generating activations with the same number of channels. Point-wise convolution is a standard convolution with 1 × 1 kernels. It extracts mutual information across the channels with minimum computation overhead. For the 12×12×3 image previously discussed, a standard convolution needs 2 × 2 × 3 × 4 multiplies to generate 1 × 1 outputs. Separable convolution needs only 2 × 2 × 3 for depth-wise convolution and 1 × 1 × 3 × 4 for point-wise convolution. This reduces computations by half from 48 to 24. The number of weights is also reduced from 48 to 24.

1c

3c

5c

1c 3c 5c 3p

………
Figure 6: Conventional Network Block (top), Residual Network Block (middle), and Densely Connected Network Block (bottom)
A residual network architecture block [98] is a feed forward layer with a short circuit between layers as shown in the middle of Figure 6. The short circuit keeps information from the previous block to increase accuracy and avoid vanishing gradients during training. Residual networks help deep networks grow in depth by directly transferring information between deeper and shallower layers.
The bottom of Figure 6 shows the densely connected convolutional block from DenseNets [109], this block extends both the network depth and the receptive ﬁeld by delivering the feature of former layers to all the later layers in a dense block using concatenation. ResNets transfer outputs from a single previous layer. DenseNets build connections across layers to fully utilize previous features. This provides weight eﬃciencies.
2.5. Batch Normalization BN was introduced in 2015 to speed up the training phase,
and to improve the neural network performance [119]. Most SOTA neural networks apply BN after a convolutional layer. BN addresses internal covariate shift (an altering of the network activation distribution caused by modiﬁcations to parameters during training) by normalizing layer inputs. This has been shown to reduce training time up to 14×. Santurkar [210] argues that the eﬃciency of BN is from its ability to smooth values during optimization.

Figure 5: Inception Block: The inception block computes multiple convolutions with one input tensor in parallel, which extends the receptive ﬁeld by mixing the size of kernels. The yellow - brown coloured cubes are convolutional kernels sized 1, 3, and 5. The blue cube corresponds to a 3 × 3 pooling operation.
The receptive ﬁeld is the size of a feature map used in a convolutional kernel. To extract data with a large receptive ﬁled and high precision, cascaded layers should be applied

−

=⋅

+

√

(2)

2+

Equation 2 gives the formula for computing inference BN, where and are the input feature and the output of BN, and are learned parameters, and are the mean value and standard deviation calculated from the training set, and is the additional small value (e.g., 1e-6) to prevent the denominator from being 0. The variables of Equation 2 are determined in the training pass and integrated into the trained

T Liang et al.: Preprint submitted to Elsevier

Page 5 of 41

Survey on pruning and quantization

weights. If the features in one channel share the same parameters, then it turns to a linear transform on each output channel. Channel-wise BN parameters potentially helps channel-wise pruning. BN could also raise the performance of the clusterbased quantize technique by reducing parameter dependency [48].
Since the parameters of the BN operation are not modiﬁed in the inference phase, they may be combined with the trained weights and biases. This is called BN folding or BN merging. Equation 3 show an example of BN folding. The new weight
′ and bias ′ are calculated using the pretrained weights and BN parameters from Equation 2. Since the new weight is computed after training and prior to inference, the number of multiplies are reduced and therefore BN folding decreases inference latency and computational complexity.

′= ⋅

, ′= ⋅ − +

(3)

√

√

2+

2+

2.6. Pooling Pooling was ﬁrst published in the 1980s with neocogni-
tron [71]. The technique takes a group of values and reduces them to a single value. The selection of the single replacement value can be computed as an average of the values (average pooling) or simply selecting the maximum value (max pooling).
Pooling destroys spatial information as it is a form of down-sampling. The window size deﬁnes the area of values to be pooled. For image processing it is usually a square window with typical sizes being 2 × 2, 3 × 3 or 4 × 4. Small windows allow enough information to be propagated to successive layers while reducing the total number of computations [224].
Global pooling is a technique where, instead of reducing a neighborhood of values, an entire feature map is reduced to a single value [154]. Global Average Pooling (GAP) extracts information from multi-channel features and can be used with dynamic pruning [153, 42].
Capsule structures have been proposed as an alternative to pooling. Capsule networks replace the scalar neuron with vectors. The vectors represent a speciﬁc entity with more detailed information, such as position and size of an object. Capsule networks void loss of spatial information by capturing it in the vector representation. Rather than reducing a neighborhood of values to a single value, capsule networks perform a dynamic routing algorithm to remove connections [209].

2.7. Parameters Figure 7 show top-1 accuracy percent verses the number
of operations needed for a number of popular neural networks [23]. The number of parameters in each network is represented by the size of the circle. A trend (not shown in the ﬁgure) is a yearly increase in parameter complexity. In 2012, AlexNet [133] was published with 60 million parameters. In 2013, VGG [217] was introduced with 133 million parameters and achieved 71.1% top-1 accuracy. These were part of the ImageNet large scale visual recognition challenge (ILSVRC) [207]. The competition’s metric was top-1 absolute accuracy.

Figure 7: Popular CNN Models: Top-1 accuracy vs GFLOPs and model size, adopted from [23]
Execution time was not a factor. This incentivized neural network designs with signiﬁcant redundancy. As of 2020, models with more than 175 billion parameters have been published [26].
Networks that execute in data centers can accommodate models with a large number of parameters. In resource constrained environments such as edge and mobile deployments, reduced parameter models have been designed. For example, GoogLeNet [226] achieves similar top-1 accuracy of 69.78% as VGG-16 but with only 7 million parameters. MobileNet [105] has 70% top-1 accuracy with only 4.2 million parameters and only 1.14 Giga FLoating-point OPerations (GFLOPs). A more detailed network comparison can be found in [5].
3. Pruning
Network pruning is an important technique for both memory size and bandwidth reduction. In the early 1990s, pruning techniques were developed to reduce a trained large network into a smaller network without requiring retraining [201]. This allowed neural networks to be deployed in constrained environments such as embedded systems. Pruning removes redundant parameters or neurons that do not signiﬁcantly contribute to the accuracy of results. This condition may arise when the weight coeﬃcients are zero, close to zero, or are replicated. Pruning consequently reduces the computational complexity. If pruned networks are retrained it provides the possibility of escaping a previous local minima [43] and further improve accuracy.
Research on network pruning can roughly be categorized as sensitivity calculation and penalty-term methods [201]. Signiﬁcant recent research interest has continued showing improvements for both network pruning categories or a fur-

T Liang et al.: Preprint submitted to Elsevier

Page 6 of 41

Survey on pruning and quantization

Static Pruning Network Model
Dynamic Pruning Network Model

Target Locating
Pruning Strategy

Network Pruning
Decision Componets

Training/Tuning Runtime Pruning

Figure 8: Pruning Categories: Static pruning is performed oﬄine prior to inference while Dynamic pruning is performed at runtime.

ther combination of them. Recently, new network pruning techniques have been cre-
ated. Modern pruning techniques may be classiﬁed by various aspects including: 1) structured and unstructured pruning depending if the pruned network is symmetric or not, 2) neuron and connection pruning depending on the pruned element type, or 3) static and dynamic pruning. Figure 8 shows the processing diﬀerences between static and dynamic pruning. Static pruning has all pruning steps performed oﬄine prior to inference while dynamic pruning is performed during runtime. While there is overlap between the categories, in this paper we will use static pruning and dynamic pruning for classiﬁcation of network pruning techniques.
Figure 9 shows a granularity of pruning opportunities. The four rectangles on the right side correspond to the four brown ﬁlters in the top of Figure 2. Pruning can occur on an element-by-element, row-by-row, column-by-column, ﬁlter-by-ﬁlter, or layer-by-layer basis. Typically element-byelement has the smallest sparsity impact, and results in a unstructured model. Sparsity decreases from left-to-right in Figure 9.
element-wise channel-wise shape-wise filter-wise layer-wise
Figure 9: Pruning Opportunities: Diﬀerent network sparsity results from the granularity of pruned structures. Shape-wise pruning was proposed by Wen [241].
arg min = ( ; ) − ( ; ) (4)
where ( ; ) = ( ( ; ))
Independent of categorization, pruning can be described mathematically as Equation 4. represents the entire neural

network which contains a series of layers (e.g., convolutional layer, pooling layer, etc.) with as input. represents the pruned network with performance loss compared to the unpruned network. Network performance is typically deﬁned as accuracy in classiﬁcation. The pruning function, (⋅), results in a diﬀerent network conﬁguration along with the pruned weights . The following sections are primarily concerned with the inﬂuence of (⋅) on . We also consider how to obtain .
3.1. Static Pruning Static pruning is a network optimization technique that
removes neurons oﬄine from the network after training and before inference. During inference, no additional pruning of the network is performed. Static pruning commonly has three parts: 1) selection of parameters to prune, 2) the method of pruning the neurons, and 3) optionally ﬁne-tuning or retraining [92]. Retraining may improve the performance of the pruned network to achieve comparable accuracy to the unpruned network but may require signiﬁcant oﬄine computation time and energy.
3.1.1. Pruning Criteria As a result of network redundancy, neurons or connec-
tions can often be removed without signiﬁcant loss of accuracy. As shown in Equation 1, the core operation of a network is a convolution operation. It involves three parts: 1) input features as produced by the previous layer, 2) weights produced from the training phase, and 3) bias values produced from the training phase. The output of the convolution operation may result in either zero valued weights or features that lead to a zero output. Another possibility is that similar weights or features may be produced. These may be merged for distributive convolutions.
An early method to prune networks is brute-force pruning. In this method the entire network is traversed element-wise and weights that do not aﬀect accuracy are removed. A disadvantage of this approach is the large solution space to traverse. A typical metric to determine which values to prune is given by the -norm, s.t. ∈ { , ∞}, where is natural number. The -norm of a vector which consists of elements is

T Liang et al.: Preprint submitted to Elsevier

Page 7 of 41

Survey on pruning and quantization

mathematically described by Equation 5.

1

∑ ‖‖ = | |

(5)

||

=1

Among the widely applied measurements, the 1-norm is also known as the Manhattan norm and the 2-norm is also known as the Euclidean norm. The corresponding 1 and 2 regularization have the names LASSO (least absolute shrinkage and selection operator) and Ridge, respectively
[230]. The diﬀerence between the 2-norm pruned tensor and an unpruned tensor is called the 2-distance. Sometimes researchers also use the term 0-norm deﬁned as the total number of nonzero elements in a vector.

⎧

2⎫

arg min ⎪∑

∑ −−

⎪

,⎨

⎬

⎪ =1

=1

⎪

(6)

⎩

⎭

subject

to

∑

| |

| |

⩽

||

Equation Equation 6 mathematically describes 2 LASSO regularization. Consider a sample consisting of cases, each
of which consists of covariates and a single outcome .
Let = ( 1, ..., ) be the standardized covariate vector for the -th case (input feature in DNNs), so we have ∑ ∕ = 0, ∑ 2 ∕ = 1. represents the coeﬃcients
= ( 1, ..., ) (weights) and is a predeﬁned tunning parameter that determines the sparsity. The LASSO estimate
is 0 when the average of is 0 because for all , the solution for is = . If the constraint is ∑ 2 ⩽ then the Equa-
tion 6 becomes Ridge regression. Removing the constraint
will results in the Ordinary Least Squares (OLS) solution.

min
∈ℝ

1 ‖

−

2
‖2

+

‖ ‖1

(7)

Equation 6 can be simpliﬁed into the so-called Lagrangian form shown in Equation 7. The Lagrangian multiplier translates the objective function ( ) and constraint ( ) = 0 into the format of ( , ) = ( ) − ( ), Where the ‖ ⋅ ‖ is the standard -norm, the is the covariate matrix that contains
, and is the data dependent parameter related to from Equation 6.
Both magnitude-based pruning and penalty based pruning may generate zero values or near-zero values for the weights. In this section we discuss both methods and their impact.

Magnitude-based pruning: It has been proposed and is widely accepted that trained weights with large values are more important than trained weights with smaller values [143]. This observation is the key to magnitude-based methods. Magnitude-based pruning methods seek to identify unneeded weights or features to remove them from runtime evaluation. Unneeded values may be pruned either in the kernel

or at the activation map. The most intuitive magnitude-based pruning methods is to prune all zero-valued weights or all weights within an absolute value threshold.
LeCun as far back as 1990 proposed Optimal Brain Damage (OBD) to prune single non-essential weights [140]. By using the second derivative (Hessian matrix) of the loss function, this static pruning technique reduced network parameters by a quarter. For a simpliﬁed derivative computation, OBD functions under three assumptions: 1) quadratic - the cost function is near-quadratic, 2) extremal - the pruning is done after the network converged, and 3) diagonal - sums up the error of individual weights by pruning the result of the error caused by their co-consequence. This research also suggested that the sparsity of DNNs could provide opportunities to accelerate network performance. Later Optimal Brain Surgeon (OBS) [97] extended OBD with a similar secondorder method but removed the diagonal assumption in OBD. OBS considers the Hessian matrix is usually non-diagonal for most applications. OBS improved the neuron removal precision with up to a 90% reduction in weights for XOR networks.
These early methods reduced the number of connections based on the second derivative of the loss function. The training procedure did not consider future pruning but still resulted in networks that were amenable to pruning. They also suggested that methods based on Hessian pruning would exhibit higher accuracy than those pruned with only magnitudebased algorithms [97]. More recent DNNs exhibit larger weight values when compared to early DNNs. Early DNNs were also much shallower with orders of magnitude less neurons. GPT-3 [26], for example, contains 175-billion parameters while VGG-16 [217] contains just 133-million parameters. Calculating the Hessian matrix during training for networks with the complexity of GPT-3 is not currently feasible as it has the complexity of ( 2). Because of this simpler magnitude-based algorithms have been developed [177, 141].
Filter-wise pruning [147] uses the 1-norm to remove ﬁlters that do not aﬀect the accuracy of the classiﬁcation. Pruning entire ﬁlters and their related feature maps resulted in a reduced inference cost of 34% for VGG-16 and 38% for ResNet-110 on the CIFAR-10 dataset with improved accuracy 0.75% and 0.02%, respectively.
Most network pruning methods choose to measure weights rather than activations when rating the eﬀectiveness of pruning [88]. However, activations may also be an indicator to prune corresponding weights. Average Percentage Of Zeros (APoZ) [106] was introduced to judge if one output activation map is contributing to the result. Certain activation functions, particularly rectiﬁcation such as Rectiﬁed Linear Unit (ReLU), may result in a high percentage of zeros in activations and thus be amenable to pruning. Equation 8 shows the deﬁnition of APoZ( ) of the -th neuron in the -th layer, where ( ) denotes the activation, is the number of calibration (validation) images, and is the dimension of

T Liang et al.: Preprint submitted to Elsevier

Page 8 of 41

Survey on pruning and quantization

Wn(ll,):,:,:

(1)

activation map. (true) = 1 and (false) = 0.

channel-wise W:(,cl)l,:,:

(2)

shortcut

W:(,cl)l,ml,kl

(3)

	   	  

∑∑ APoZ( ) = APoZ ( ) = =0 =0

(

) ,

(

)=0

(8) ×

Similarly, inbound pruning [195], also an activation technique, considers channels that do not contribute to the result. If the top activation channel in the standard convolution of

	  

	   	   	  

	   	  

W (l) filter-wise Wn(ll,):,:,:

	   	  

…	  

	   	  
	   	  

(4) Wn(ll,):,:,:
shWap:e(,cl-)lw,:,:ise (1) W:(,cl)l,ml,kl
W (l)

	   	  

	   	   	   	  

	   	  

(1)

Wn(ll,):,:,:

(2)

	   	  

W:(,cl)l,:,:

(3)

W:(,cl)l,ml,kl

depth-wise W (l) (4)

W:(,cl)l,:,:

(2)

Figure 10: Types of W:(,cl)l,ml,kl Sparsity(3)Geometry, adopted from [241]

W (l)

(4)

(1) (2) (3) (4)

Figure 2 are determined to be less contributing, the corre-

sponding channel of the ﬁlter in the bottom of the ﬁgure will be removed. After pruning this technique achieved about 1.5× compression.
Filter-wise pruning using a threshold from the sum of

as a whole. Equation 9 gives the pruning constraint where and in Equation 7 are replaced by the higher dimensional
and for the groups.

ﬁlters’ absolute values can directly take advantage of the structure in the network. In this way, the ratio of pruned to unpruned neurons (i.e. the pruning ratio) is positively correlated to the percentage of kernel weights with zero values, which can be further improved by penalty-based methods.

1

⎧

‖

min

⎪‖ ‖

∑ −

∈ℝ ⎨‖

‖ ⎪

=1

‖

⎩

2

⎫

‖

‖ ‖

+

‖

∑ ‖
‖

‖ ‖

⎪ ⎬

(9)

‖‖2

=1 ‖ ‖ ⎪ 1

1

⎭

Penalty-based pruning: In penalty-based pruning, the goal is to modify an error function or add other constraints, known as bias terms, in the training process. A penalty value is used to update some weights to zero or near zero values. These values are then pruned.
Hanson [96] explored hyperbolic and exponential bias terms for pruning in the late 80s. This method uses weight decay in backpropagation to determine if a neuron should be pruned. Low-valued weights are replaced by zeros. Residual zero valued weights after training are then used to prune unneeded neurons.
Feature selection [55] is a technique that selects a subset of relevant features that contribute to the result. It is also known as attribute selection or variable selection. Feature selection helps algorithms avoiding over-ﬁtting and accelerates both training and inference by removing features and/or connections that don’t contribute to the results. Feature selection also aids model understanding by simplifying them to the most important features. Pruning in DNNs can be considered to be a kind of feature selection [123].
LASSO was previously introduced as a penalty term. LASSO shrinks the least absolute valued feature’s corresponding weights. This increases weight sparsity. This operation is also referred to as LASSO feature selection and has been shown to perform better than traditional procedures such as OLS by selecting the most signiﬁcantly contributed variables instead of using all the variables. This lead to approximately 60% more sparsity than OLS [181].
Element-wise pruning may result in an unstructured network organizations. This leads to sparse weight matrices that are not eﬃciently executed on instruction set processors. In addition they are usually hard to compress or accelerate without specialized hardware support [91]. Group LASSO [260] mitigates these ineﬃciencies by using a structured pruning method that removes entire groups of neurons while maintaining structure in the network organization [17].
Group LASSO is designed to ensure that all the variables sorted into one group could be either included or excluded

Figure 10 shows Group LASSO with group shapes used in Structured S1 parsity Learning (SSL) [241]. Weights are split into multiple groups. Unneeded groups of weights are removed using LASSO feature selection. Groups may be determined based on geometry, computational complexity, group sparsity, etc. SSL describes an example where group sparsity in row and column directions may be used to reduce the execution time of GEMM. SSL has shown improved inference times on AlexNet with both CPUs and GPUs by 5.1× and 3.1×, respectively.
Group-wise brain damage [136] also introduced the group LASSO constraint but applied it to ﬁlters. This simulates brain damage and introduces sparsity. It achieved 2× speedup with 0.7% ILSVRC-2012 accuracy loss on the VGG Network.
Sparse Convolutional Neural Networks (SCNN) [17] take advantage of two-stage tensor decomposition. By decomposing the input feature map and convolutional kernels, the tensors are transformed into two tensor multiplications. Group LASSO is then applied. SCNN also proposed a hardware friendly algorithm to further accelerate sparse matrix computations. They achieved 2.47× to 6.88× speed-up on various types of convolution.
Network slimming [158] applies LASSO on the scaling factors of BN. BN normalizes the activation by statistical parameters which are obtained during the training phase. Network slimming has the eﬀect of introducing forward invisible additional parameters without additional overhead. Speciﬁcally, by setting the BN scaler parameter to zero, channel-wise pruning is enabled. They achieved 82.5% size reduction with VGG and 30.4% computation compression without loss of accuracy on ILSVRC-2012.
Sparse structure selection [111] is a generalized network slimming method. It prunes by applying LASSO to sparse scaling factors in neurons, groups, or residual blocks. Using an improved gradient method, Accelerated Proximal Gradient (APG), the proposed method shows better performance without ﬁne-tunning achieving 4× speed-up on VGG-16 with 3.93% ILSVRC-2012 top-1 accuracy loss.

T Liang et al.: Preprint submitted to Elsevier

Page 9 of 41

Survey on pruning and quantization

Dropout: While not speciﬁcally a technique to prune networks, dropout does reduce the number of parameters [222]. It was originally designed as a stochastic regularizer to avoid over-ﬁtting of data [103]. The technique randomly omits a percentage of neurons typically up to 50%, This dropout operation breaks oﬀ part of the connections between neurons to avoid co-adaptations. Dropout could also be regarded as an operation that separately trains many sub-networks and takes the average of them during the inference phase. Dropout increases training overhead but it does not aﬀect the inference time.
Sparse variational dropout [176] added a dropout hyperparameter called the dropout rate to reduce the weights of VGG-like networks by 68×. During training the dropout rate can be used to identify single weights to prune. This can also be applied with other compression approaches for further reduction in weights.
Redundancies: The goal of norm-based pruning algorithms is to remove zeros. This implies that the distribution of values should wide enough to retain some values but contain enough values close to zero such that a smaller network organization is still accurate. This does not hold in some circumstances. For example, ﬁlters that have small norm deviations or a large minimum norm have small search spaces making it diﬃcult to prune based on a threshold [100]. Even when parameter values are wide enough, in some networks smaller values may still play an important role in producing results. One example of this is when large valued parameters saturate [64]. In these cases magnitude-based pruning of zero values may decrease result accuracy.
Similarly, penalty-based pruning may cause network accuracy loss. In this case, the ﬁlters identiﬁed as unneeded due to similar coeﬃcient values in other ﬁlters may actually be required. Removing them may signiﬁcantly decrease network accuracy [88]. Section 3.1.2 describes techniques to undo pruning by tuning the weights to minimize network loss while this section describes redundancy based pruning.
Using BN parameters, feature map channel distances can be computed by layer [266]. Using a clustering approach for distance, nearby features can be tuned. An advantage of clustering is that redundancy is not measured with an absolute distance but a relative value. With about 60 epochs of training they were able to prune the network resulting in a 50% reduction in FLOPs (including non-convolutional operations) with a reduction in accuracy of only 1% for both top-1 and top-5 on the ImageNet dataset.
Filter pruning via geometric median (FPGM) [100] identiﬁes ﬁlters to prune by measuring the 2-distance using the geometric median. FPGM found 42% FLOPs reduction with 0.05% top-1 accuracy drop on ILSVRC-2012 with ResNet101.
The reduce and reused (also described as outbound) method [195] prunes entire ﬁlters by computing the statistical variance of each ﬁlter’s output using a calibration set. Filters with low variance are pruned. The outbound method obtained 2.37× acceleration with 1.52% accuracy loss on

Labeled Faces in the Wild (LFW) dataset [110] in the ﬁled of face recognition.
A method that iteratively removes redundant neurons for FCLs without requiring special validation data is proposed in [221]. This approach measures the similarity of weight groups after a normalization. It removes redundant weights and merges the weights into a single value. This lead to a 34.89% reduction of FCL weights on AlexNet with 2.24% top-1 accuracy loss on ILSVRC-2012.
Comparing with the similarity based approach above, DIVersity NETworks (DIVNET) [167] considers the calculation redundancy based on the activations. DIVNET introduces Determinantal Point Process (DPP) [166] as a pruning tool. DPP sorts neurons into categories including dropped and retained. Instead of forcing the removal of elements with low contribution factors, they fuse the neurons by a process named re-weighting. Re-weighting works by minimizing the impact of neuron removal. This minimizes pruning inﬂuence and mitigates network information loss. They found 3% loss on CIFAR-10 dataset when compressing the network into half weight.
ThiNet [164] adopts statistics information from the next layer to determine the importance of ﬁlters. It uses a greedy search to prune the channel that has the smallest reconstruction cost in the next layer. ThiNet prunes layer-by-layer instead of globally to minimize large errors in classiﬁcation accuracy. It also prunes less during each training epoch to allow for coeﬃcient stability. The pruning ratio is a predeﬁned hyper-parameter and the runtime complexity is directly related to the pruning ratio. ThiNet compressed ResNet-50 FLOPs to 44.17% with a top-1 accuracy reduction of 1.87%.
He [101] adopts LASSO regression instead of a greedy algorithm to estimate the channels. Speciﬁcally, in one iteration, the ﬁrst step is to evaluate the most important channel using the 1-norm. The next step is to prune the corresponding channel that has the smallest Mean Square Error (MSE). Compared to an unpruned network, this approach obtained 2× acceleration of ResNet-50 on ILSVRC-2012 with about 1.4% accuracy loss on top-5, and a 4× reduction in execution time with top-5 accuracy loss of 1.0% for VGG-16. The authors categorize their approach as dynamic inference-time channel pruning. However it requires 5000 images for calibration with 10 samples per image and more importantly results in a statically pruned network. Thus we have placed it under static pruning.
3.1.2. Pruning combined with Tuning or Retraining Pruning removes network redundancies and has the bene-
ﬁt of reducing the number of computations without signiﬁcant impact on accuracy for some network architectures. However, as the estimation criterion is not always accurate, some important elements may be eliminated resulting in a decrease in accuracy. Because of the loss of accuracy, time-consuming ﬁne-tuning or re-training may be employed to increase accuracy [258].
Deep compression [92], for example, describes a static method to prune connections that don’t contribute to classi-

T Liang et al.: Preprint submitted to Elsevier

Page 10 of 41

Survey on pruning and quantization

ﬁcation accuracy. In addition to feature map pruning they ResNet classiﬁcation accuracy with only 5% to 10% size of

also remove weights with small values. After pruning they original weights.

re-train the network to improve accuracy. This process is

AutoPruner [163] integrated the pruning and ﬁne-tuning

performed iteratively three times resulting in a 9× to 13× of a three-stage pipeline as an independent training-friendly

reduction in total parameters with no loss of accuracy. Most layer. The layer helped gradually prune during training even-

of the removed parameters were from FCLs.

tually resulting in a less complex network. AutoPruner pruned

73.59% of compute operations on VGG-16 with 2.39% ILSVRC-

Recoverable Pruning: Pruned elements usually cannot be 2012 top-1 loss. ResNet-50 resulted in a 65.80% of compute

recovered. This may result in reduced network capability. operations with 3.10% loss of accuracy.

Recovering lost network capability requires signiﬁcant re-

training. Deep compression required millions of iterations to Training from Scratch: Observation shows that network

retrain the network [92]. To avoid this shortcoming, many ap- training eﬃciency and accuracy is inversely proportional

proaches adopt recoverable pruning algorithms. The pruned to structure sparsity. The more dense the network, the less

elements may also be involved in the subsequent training training time [94, 147, 70]. This is one reason that current

process and adjust themselves to ﬁt the pruned network.

pruning techniques tend to follow a train-prune-tune pipeline

Guo [88] describes a recoverable pruning method using rather than training a pruned structure from scratch.

binary mask matrices to indicate whether a single weight

However, the lottery ticket hypothesis [70] shows that it is

value is pruned or not. The 1-norm pruned weights can be not of primary importance to preserve the original weights but stochastically spliced back into the network. Using this ap- the initialization. Experiments show that dense, randomly-

proach AlexNet was able to be reduced by a factor of 17.7× initialized pruned sub-networks can be trained eﬀectively

with no accuracy loss. Re-training iterations were signiﬁ- and reach comparable accuracy to the original network with

cantly reduced to 14.58% of Deep compression [92]. How- the same number of training iterations. Furthermore, stan-

ever this type of pruning still results in an asymmetric network dard pruning techniques can uncover the aforementioned

complicating hardware implementation.

sub-networks from a large oversized network - the Winning

Soft Filter Pruning (SFP) [99] further extended recov- Tickets. In contrast with current static pruning techniques,

erable pruning using a dimension of ﬁlter. SFP obtained the lottery ticket hypothesis after a period of time drops all

structured compression results with an additional beneﬁt or well-trained weights and resets them to an initial random

reduced inference time. Furthermore, SFP can be used on state. This technique found that ResNet-18 could maintain

diﬃcult to compress networks achieving a 29.8% speed-up comparable performance with a pruning ratio up to 88.2% on

on ResNet-50 with 1.54% ILSVRC-2012 top-1 accuracy loss. the CIFAR-10 dataset.

Comparing with Guo’s recoverable weight [88] technique, SFP achieves inference speed-ups closer to theoretical results on general purpose hardware by taking advantage of the structure of the ﬁlter.

Towards Better Accuracy: By reducing the number of network parameters, pruning techniques can also help to reduce over-ﬁtting. Dense-Sparse-Dense (DSD) training [93] helps various network improve classiﬁcation accuracy by 1.1% to

Increasing Sparsity: Another motivation to apply ﬁne-tuning 4.3%. DSD uses a three stage pipeline: 1) dense training to

is to increase network sparsity. Sparse constraints [270] ap- identify important connections, 2) prune insigniﬁcant weights

plied low rank tensor constraints [157] and group sparsity and sparse training with a sparsity constraint to take reduce

[57] achieving a 70% reduction of neurons with a 0.57% drop the number of parameters, and 3) re-dense the structure to

of AlexNet in ILSVRC-2012 top-1 accuracy.

recover the original symmetric structure, this also increase

Adaptive Sparsity: No matter what kind of pruning criteria is applied, a layer-wise pruning ratio usually requires a human decision. Too high a ratio resulting in very high sparsity may cause the network to diverge requiring heavy re-tuning.

the model capacity. The DSD approach has also shown impressive performance on the other type of deep networks such as Recurrent Neural Networks (RNNs) and Long Short Term Memory networks (LSTMs).

Network slimming [158], previously discussed, addresses 3.2. Dynamic Pruning

this problem by automatically computing layer-wise sparsity.

Except for recoverable techniques, static pruning perma-

This achieved a 20× model size compression, 5× computing nently destroys the original network structure which may lead

reduction, and less than 0.1% accuracy loss on the VGG to a decrease in model capability. Techniques have been re-

network.

searched to recover lost network capabilities but once pruned

Pruning can also be performed using a min-max optimiza- and re-trained, the static pruning approach can’t recover de-

tion module [218] that maintains network accuracy during stroyed information. Additionally, observations shows that

tuning by keeping a pruning ratio. This technique compressed the importance of neuron binding is input-independent [73].

the VGG network by a factor of 17.5× and resulted in a theo-

Dynamic pruning determines at runtime which layers,

retical execution time (FLOPs) of 15.56% of the unpruned channels, or neurons will not participate in further activity.

network. A similar approach was proposed with an estima- Dynamic pruning can overcome limitations of static prun-

tion of weights sets [33]. By avoiding the use of a greedy ing by taking advantage of changing input data potentially

search to keep the best pruning ratio, they achieved the same reducing computation, bandwidth, and power dissipation. Dy-

T Liang et al.: Preprint submitted to Elsevier

Page 11 of 41

Network Data Decision Data Network Info

Survey on pruning and quantization

Decision Components
147 3

Side Network
Additional Connections attached to Network A

25
Pruning Decision

1.Additional connections or side networks? 2.Layer-wise pruning or channel-wise? 3.One-shot information input or layer-wise? 4.How to calculate the score? 5.Predefined thresholds or dynamical? 6.Continue, skip or exit computing? 7.How to train the decision components?

Input Image(s)
6
Network A
Cascade Network
Exit

6

...

Network B

... Network C

Exit

Exit

Exit

Exit

Figure 11: Dynamic Pruning System Considerations

namic pruning typically doesn’t perform runtime ﬁne-tuning or re-training. In Figure 11, we show an overview of dynamic pruning systems. The most important consideration is the decision system that decides what to prune. The related issues are:
1. The type of the decision components: a) additional connections attached to the original network used during the inference phase and/or the training phase, b) characteristics of the connections that can be learned by standard backpropagation algorithms [73], and c) a side decision network which tends to perform well but is often diﬃcult to train [153].
2. The pruning level (shape): a) channel-wise [153, 73, 42], b) layer-wise [145], c) block-wise [246], or d) network-wise [25]. The pruning level chosen inﬂuences hardware design.
3. Input data: a) one-shot information feeding [246] feeds the entire input to the decision system, and b) layerwise information feeding [25, 68] where a window of data is iteratively fed to the decision system along with the forwarding.
4. Computing a decision score: -norm [73], or b) other approaches [108].
5. Score comparison: a) human experience/experiment results [145] or b) automatic threshold or dynamic mechanisms [108].
6. Stopping criteria: a) in the case of layer-wise and network-wise pruning, some pruning algorithms skip the pruned layer/network [19, 246], b) some algorithms dynamically choose the data path [189, 259], and c)

ending the computation and outputing the predicting results [68, 145, 148]. In this case the remaining layers are considered to be pruned. 7. Training the decision component: a) attached connections can be trained along with the original network [145, 148, 73], b) side networks are typically trained using reinforcement learning (RL) algorithms [19, 153, 189, 246].
For instruction set processors, feature maps or the number of ﬁlters used to identify objects is a large portion of bandwidth usage [225] - especially for depth-wise or point-wise convolutions where features consume a larger portion of the bandwidth [47]. Dynamic tuning may also be applied to statically pruned networks potentially further reducing compute and bandwidth requirements.
A drawback of dynamic pruning is that the criteria to determine which elements to prune must be computed at runtime. This adds overhead to the system requiring additional compute, bandwidth, and power. A trade-oﬀ between dynamic pruning overhead, reduced network computation, and accuracy loss, should be considered. One method to mitigate power consumption inhibits computations from 0-valued parameters within a Processing Element (PE) [153].
3.2.1. Conditional Computing Conditional computing involves activating an optimal
part of a network without activating the entire network. Nonactivated neurons are considered to be pruned. They do not participate in the result thereby reducing the number of computations required. Conditional computing applies to

T Liang et al.: Preprint submitted to Elsevier

Page 12 of 41

Survey on pruning and quantization

training and inference [20, 56]. Conditional computing has a similarity with RL in that
they both learn a pattern to achieve a reward. Bengio [19] split the network into several blocks and formulates the block chosen policies as an RL problem. This approach consists of only fully connected neural networks and achieved a 5.3× speed-up on CIFAR-10 dataset without loss of accuracy.
3.2.2. Reinforcement Learning Adaptive Networks Adaptive networks aim to accelerating network inference
by conditionally determining early exits. A trade-oﬀ between network accuracy and computation can be applied using thresholds. Adaptive networks have multiple intermediate classiﬁers to provide the ability of an early exit. A cascade network is a type of adaptive network. Cascade networks are the combinations of serial networks which all have output layers rather than per-layer outputs. Cascade networks have a natural advantage of an early exit by not requiring all output layers to be computed. If the early accuracy of a cascade network is not suﬃcient, inference could potentially be dispatched to a cloud device [145, 25]. A disadvantage of adaptive networks is that they usually need hyper-parameters optimized manually (e.g., conﬁdence score [145]). This introduces automation challenges as well as classiﬁcation accuracy loss. They found 28.75% test error on CIFAR-10 when setting the threshold to 0.5. A threshold of 0.99 lowered the error to 15.74% at a cost of 3x to inference time.
A cascading network [189] is an adaptive network with an RL trained Composer that can determine a reasonable computation graph for each input. An adaptive controller Policy Preferences is used to intelligently enhance the Composer allowing an adjustment of the network computation graph from sub-graphs. The Composer performs much better in terms of accuracy than the baseline network with the same number of computation-involved parameters on a modiﬁed dataset, namely Wide-MNIST. For example, when invoking 1k parameters, the baseline achieves 72% accuracy while the Composer obtained 85%.
BlockDrop [246] introduced a policy network that trained using RL to make an image-speciﬁc determination whether a residual network block should participate in the following computation. While the other approaches compute an exit conﬁdence score per layer, the policy network runs only once when an image is loaded. It generates a boolean vector that indicates which residual blocks are activate or inactive. BlockDrop adds more ﬂexibility to the early exit mechanism by allowing a decision to be made on any block and not just early blocks in Spatially Adaptive Computation Time (SACT) [68]. This is discussed further in Section 3.2.3. BlockDrop achieves an average speed-up of 20% on ResNet101 for ILSVRC-2012 without accuracy loss. Experiments using the CIFAR dataset showed better performance than other SOTA counterparts at that time [68, 82, 147].
Runtime Neural Pruning (RNP) [153] is a framework that prunes neural networks dynamically. RNP formulates the feature selection problem as a Markov Decision Process (MDP) and then trains an RNN-based decision network by

RL. The MDP reward function in the state-action-reward sequence is computation eﬃciency. Rather than removing layers, a side network of RNP predicts which feature maps are not needed. They found 2.3× to 5.9× reduction in execution time with top-5 accuracy loss from 2.32% to 4.89% for VGG16.

3.2.3. Diﬀerentiable Adaptive Networks Most of the aforementioned decision components are non-
diﬀerential, thus computationally expensive RL is adopted for training. A number of techniques have been developed to reduce training complexity by using diﬀerentiable methods.
Dynamic channel pruning [73] proposes a method to dynamically select which channel to skip or to process using Feature Boosting and Suppression (FBS). FBS is a side network that guides channel ampliﬁcation and omission. FBS is trained along with convolutional networks using SGD with LASSO constraints. The selecting indicator can be merged into BN parameters. FBS achieved 5× acceleration on VGG16 with 0.59% ILSVRC-2012 top-5 accuracy loss, and 2× acceleration on ResNet-18 with 2.54% top-1, 1.46% top-5 accuracy loss.
Another approach, Dynamic Channel Pruning (DCP) [42] dynamically prunes channels using a channel threshold weighting (T-Weighting) decision. Speciﬁcally, this module prunes the channels whose score is lower than a given threshold. The score is calculated by a T-sigmoid activation function, which is mathematically described in Equation 10, where ( ) = 1∕(1 + − ) is the sigmoid function. The input to the T-sigmoid activation function is down sampled by a FCL from the feature maps. The threshold is found using iterative training which can be a computationally expensive process. DCP increased VGG-16 top-5 error by 4.77% on ILSVRC-2012 for 5× computation speed-up. By comparison, RNP increased VGG-16 top-5 error by 4.89% [153].

( ), if >

ℎ( ) = 0,

otherwise

(10)

The cascading neural network by Leroux [145] reduced the average inference time of overfeat network [211] by 40% with a 2% ILSVRC-2012 top-1 accuracy loss. Their criteria for early exit is based on the conﬁdence score generated by an output layer. The auxiliary layers were trained with general backpropagation. The adjustable score threshold provides a trade-oﬀ between accuracy and eﬃciency.
Bolukbasi [25] reports a system that contains a combination of other SOTA networks (e.g., AlexNet, ResNet, GoogLeNet, etc.). A policy adaptively chooses a point to exit early. This policy can be trained by minimizing its cost function. They format the system as a directed acyclic graph with various pre-trained networks as basic components. They evaluate this graph to determine leaf nodes for early exit. The cascade of acyclic graphs with a combination of various networks reduces computations while maintaining prediction accuracy. ILSVRC-2012 experiments show ResNet-50 acceleration of 2.8× with 1% top-5 accuracy loss and 1.9× speed-up with no accuracy loss.

T Liang et al.: Preprint submitted to Elsevier

Page 13 of 41

Survey on pruning and quantization

Considering the similarity of RNNs and residual networks [83], Spatially Adaptive Computation Time (SACT) [68] explored an early stop mechanism of residual networks in the spatial domain. SACT can be applied to various tasks including image classiﬁcation, object detection, and image segmentation. SACT achieved about 20% acceleration with no accuracy loss for ResNet-101 on ILSVRC-2012.
To meet the computation constraints, Multi-Scale Dense Networks (MSDNets) [108] designed an adaptive network using two techniques: 1) an anytime-prediction to generate prediction results at many nodes to facilitate the network’s early exit and 2) batch computational budget to enforce a simpler exit criteria such as a computation limit. MSDNets combine multi-scale feature maps [265] and dense connectivity [109] to enable accurate early exit while maintaining higher accuracy. The classiﬁers are diﬀerentiable so that MSDNets can be trained using stochastic gradient descent. MSDNets achieve 2.2× speed-up at the same accuracy for ResNet-50 on ILSVRC-2012 dataset.
To address the training complexity of adaptive networks, Li [148] proposed two methods. The ﬁrst method is gradient equilibrium (GE). This technique helps backbone networks converge by using multiple intermediate classiﬁers across multiple diﬀerent network layers. This improves the gradient imbalance issue found in MSDNets [108]. The second method is an Inline Subnetwork Collaboration (ISC) and a One-For-All knowledge distillation (OFA). Instead of independently training diﬀerent exits, ISC takes early predictions into later predictors to enhance their input information. OFA supervises all the intermediate exits using a ﬁnal classiﬁer. At a same ILSVRC-2012 top-1 accuracy of 73.1%, their network takes only one-third the computational budget of ResNet.
Slimmable Neural Networks (SNN) [259] are a type of networks that can be executed at diﬀerent widths. Also known as switchable networks, the network enables dynamically selecting network architectures (width) without much computation overhead. Switchable networks are designed to adaptively and eﬃciently make trade-oﬀs between accuracy and on-device inference latency across diﬀerent hardware platforms. SNN found that the diﬀerence of feature mean and variance may lead to training faults. SNN solves this issue with a novel switchable BN technique and then trains a wide enough network. Unlike cascade networks which primarily beneﬁt from speciﬁc blocks, SNN can be applied with many more types of operations. As BN already has two parameters as mentioned in Section 2, the network switch that controls the network width comes with little additional cost. SNN increased top-1 error by 1.4% on ILSVRC-2012 while achieving about 2× speed-up.
3.3. Comparisons Pruning techniques are diverse and diﬃcult to compare.
Shrinkbench [24] is a uniﬁed benchmark framework aiming to provide pruning performance comparisons.
There exist ambiguities about the value of the pre-trained weights. Liu [160] argues that the pruned model could be trained from scratch using a random weight initialization.

This implies the pruned architecture itself is crucial to success. By this observation, the pruning algorithms could be seen as a type of NAS. Liu concluded that because the weight values can be re-trained, by themselves they are not eﬃcacious. However, the lottery ticket hypothesis [70] achieved comparable accuracy only when the weight initialization was exactly the same as the unpruned model. Glae [72] resolved the discrepancy by showing that what really matters is the pruning form. Speciﬁcally, unstructured pruning can only be ﬁne-tuned to restore accuracy but structured pruning can be trained from scratch. In addition, they explored the performance of dropout and 0 regularization. The results showed that simple magnitude based pruning can perform better. They developed a magnitude based pruning algorithm and showed the pruned ResNet-50 obtained higher accuracy than SOTA at the same computational complexity.
4. Quantization
Quantization is known as the process of approximating a continuous signal by a set of discrete symbols or integer values. Clustering and parameter sharing also fall within this deﬁnition [92]. Partial quantization uses clustering algorithms such as k-means to quantize weight states and then store the parameters in a compressed ﬁle. The weights can be decompressed using either a lookup table or a linear transformation. This is typically performed during runtime inference. This scheme only reduces the storage cost of a model. This is discussed in Section 4.2.4. In this section we focus on numerical low-bit quantization.
Compressing CNNs by reducing precision values has been previously proposed. Converting ﬂoating-point parameters into low numerical precision datatypes for quantizing neural networks was proposed as far back as the 1990s [67, 14]. Renewed interest in quantization began in the 2010s when 8bit weight values were shown to accelerate inference without a signiﬁcant drop in accuracy [233].
Historically most networks are trained using FP32 numbers [225]. For many networks an FP32 representation has greater precision than needed. Converting FP32 parameters to lower bit representations can signiﬁcantly reduce bandwidth, energy, and on-chip area.
Figure 12 shows the evolution of quantization techniques. Initially, only weights were quantized. By quantizing, clustering, and sharing, weight storage requirements can be reduced by nearly 4×. Han [92] combined these techniques to reduce weight storage requirements from 27MB to 6.9MB. Post training quantization involves taking a trained model, quantizing the weights, and then re-optimizing the model to generate a quantized model with scales [16]. Quantization-aware training involves ﬁne-tuning a stable full precision model or retraining the quantized model. During this process real-valued weights are often down-scaled to integer values - typically 8-bits [120]. Saturated quantization can be used to generate feature scales using a calibratation algorithm with a calibration set. Quantized activations show similar distributions with previous real-valued data [173]. Kullback-Leibler di-

T Liang et al.: Preprint submitted to Elsevier

Page 14 of 41

Survey on pruning and quantization

weights

cluster/ sharing

activation

floating point

post train quantize
floating point

quantize-aware training
non-saturated

quantize-aware training
calibrated saturated

Figure 12: Quantization Evolution: The development of quantization techniques, from left to right. Purple rectangles indicated quantized data while blue rectangles represent full precision 32-bit ﬂoating point format.

vergence (KL-divergence, also known as relative entropy or information divergence) calibrated quantization is typically applied and can accelerate the network without accuracy loss for many well known models [173]. Fine-tuning can also be applied with this approach.
KL-divergence is a measure to show the relative entropy of probability distributions between two sets. Equation 11 gives the equation for KL-divergence. and are deﬁned as discrete probability distributions on the same probability space. Speciﬁcally, is the original data (ﬂoating-point) distribution that falls in several bins. is the quantized data histogram.

∑

()

KL( ‖ ) =

( ) log

=0

()

(11)

Depending upon the processor and execution environment, quantized parameters can often accelerate neural network inference.
Quantization research can be categorized into two focus areas: 1) quantization aware training (QAT) and 2) post training quantization (PTQ). The diﬀerence depends on whether training progress is is taken into account during training. Alternatively, we could also categorize quantization by where data is grouped for quantization: 1) layer-wise and 2) channelwise. Further, while evaluating parameter widths, we could further classify by length: N-bit quantization.
Reduced precision techniques do not always achieve the expected speedup. For example, INT8 inference doesn’t achieve exactly 4× speedup over 32-bit ﬂoating point due to the additional operations of quantization and dequantization. For instance, Google’s TensorFlow-Lite [227] and nVidia’s Tensor RT [173] INT8 inference speedup is about 2-3×. Batch size is the capability to process more than one image in the forward pass. Using larger batch sizes, Tensor RT does achieve 3-4× acceleration with INT8 [173].
Section 8 summarizes current quantization techniques used on the ILSVRC-2012 dataset along with their bit-widths for weights and activation.

4.1. Quantization Algebra

= ( × ( )+ )

(12)

There are many methods to quantize a given network. Generally, they are formulated as Equation 12 where is a scalar that can be calculated using various methods. (⋅) is the clamp function applied to ﬂoating-point values performing the quantization. is the zero-point to adjust the true zero in some asymmetrical quantization approaches. (⋅) is the rounding function. This section introduces quantization using the mathematical framework of Equation 12.

( , , ) = ( ( , ), )

(13)

Equation 13 deﬁnes a clamp function. The min-max method is given by Equation 14 where [ , ] are the bounds for the minimum and maximum values of the parameters, respectively. is the maximum representable number derived from the bit-width (e.g., 256 = 28 in case of 8-bit), and , are the same as in Equation 12. is typically non-zero in the min-max method [120].

( )=

(, , )

= − 1 , = × (1 − )

(14)

−

−

where = min{ }, = max{ }

The max-abs method uses a symmetry bound shown in Equation 15. The quantization scale is calculated from the largest one among the data to be quantized. Since the bound is symmetrical, the zero point will be zero. In such a situation, the overhead of computing an oﬀset-involved convolution will be reduced but the dynamic range is reduced since the valid range is narrower. This is especially noticeable for ReLU activated data where all of which values fall on the positive axis.

( )=

( ,− , )

= − 1, = 0

(15)

where = max{ { }}

Quantization can be applied on input features , weights , and biases . Taking feature and weights as an example (ignoring the biases) and using the min-max method gives Equation 16. The subscripts and denote the realvalued and quantized data, respectively. The suﬃx is

T Liang et al.: Preprint submitted to Elsevier

Page 15 of 41

Survey on pruning and quantization

from in Equation 15, while = ( − 1)∕ ( − 1)∕ .

= −1 × ,

= −1 ×

,= (16)

Integer quantized convolution is shown in Equation 17 and follows the same form as convolution with real values. In Equation 17, the ∗ denotes the convolution operation, the feature, the weights, and , the quantized convolution result. Numerous third party libraries support this type of integer quantized convolution acceleration. They are discussed in Section 4.3.2.

=∗

s.t. , ∈ ℤ

(17)

De-quantizing converts the quantized value back to ﬂoating-point using the feature scales and weights scales . A symmetric example with = 0 is shown in Equation 18. This is useful for layers that process ﬂoating-point tensors. Quantization libraries are discussed in Section 4.3.2.

=

=×

×

(18)

×

( − 1) ( − 1)

In most circumstances, consecutive layers can compute with quantized parameters. This allows dequantization to be merged in one operation as in Equation 19. +1 is the
quantized feature for next layer and +1 is the feature scale
for next layer.

× +1

+1 =

(19)

×

The activation function can be placed following either the quantized output , the de-quantized output , or after
a re-quantized output +1. The diﬀerent locations may lead to diﬀerent numerical outcomes since they typically have diﬀerent precision.
Similar to convolutional layers, FCLs can also be quantized. K-means clustering can be used to aid in the compression of weights. In 2014 Gong [76] used k-means clustering on FCLs and achieved a compression ratio of more than 20× with 1% top-5 accuracy loss.
Bias terms in neural networks introduce intercepts in linear equations. They are typically regarded as constants that help the network to train and best ﬁt given data. Bias quantization is not widely mentioned in the literature. [120] maintained 32-bit biases while quantizing weights to 8-bit. Since biases account for minimal memory usage (e.g. 12 values for a 10-in/12-out FCL vs 120 weight values) it is recommended to leave biases in full precision. If bias quantization is performed it can be a multiplication by both the feature scale and weight scale [120], as shown in Equation 20. However, in some circumstances they may have their own scale factor. For example, when the bit-lengths are limited to be shorter than the multiplication results.

= ×, = ×

(20)

4.2. Quantization Methodology We describe PTQ and QAT quantization approaches based
on back-propagation use. We can also categorize them based on bit-width. In the following subsections, we introduce common quantization methods. In Section 4.2.1 low bit-width quantization is discussed. In Section 4.2.2 and Section 4.2.3 special cases of low bit-width quantization is discussed. In Section 4.2.5 diﬃculties with training quantized networks are discussed. Finally, in Section 4.2.4, alternate approached to quantization are discussed.
4.2.1. Lower Numerical Precision Half precision ﬂoating point (16-bit ﬂoating-point, FP16)
has been widely used in nVidia GPUs and ASIC accelerators with minimal accuracy loss [54]. Mixed precision training with weights, activations, and gradients using FP16 while the accumulated error for updating weights remains in FP32 has shown SOTA performance - sometimes even improved performance [172].
Researchers [165, 98, 233] have shown that FP32 parameters produced during training can be reduced to 8-bit integers for inference without signiﬁcant loss of accuracy. Jacob [120] applied 8-bit integers for both training and inference, with an accuracy loss of 1.5% on ResNet-50. Xilinx [212] showed that 8-bit numerical precision could also achieve lossless performance with only one batch inference to adjust quantization parameters and without retraining.
Quantization can be considered an exhaustive search optimizing the scale found to reduce an error term. Given a ﬂoating-point network, the quantizer will take an initial scale, typically calculated by minimizing the 2-error, and use it to quantize the ﬁrst layer weights. Then the quantizer will adjust the scale to ﬁnd the lowest output error. It performans this operation on every layer.
Integer Arithmetic-only Inference (IAI) [120] proposed a practical quantization scheme able to be adopted by industry using standard datatypes. IAI trades oﬀ accuracy and inference latency by compressing compact networks into integers. Previous techniques only compressed the weights of redundant networks resulting in better storage eﬃciency. IAI quantizes ≠ 0 in Equation 12 requiring additional zeropoint handling but resulting in higher eﬃciency by making use of unsigned 8-bit integers. The data-ﬂow is described in Figure 13. TensorFlow-Lite [120, 131] deployed IAI with an accuracy loss of 2.1% using ResNet-150 on the ImageNet dataset. This is described in more detail in Section 4.3.2.
Datatypes other than INT8 have been used to quantize parameters. Fixed point, where the radix point is not at the right-most binary digit, is one format that has been found to be useful. It provides little loss or even higher accuracy but with a lower computation budget. Dynamic scaled ﬁxed-point representation [233] obtained a 4× acceleration on CPUs. However, it requires specialized hardware including 16-bit ﬁxed-point [89], 16-bit ﬂex point [130], and 12-bit operations using dynamic ﬁxed-point format (DFXP) [51]. The specialized hardware is mentioned in Section 4.3.3.

T Liang et al.: Preprint submitted to Elsevier

Page 16 of 41

Survey on pruning and quantization

weights uint8
feature uint8

biases uint32

conv

+

ReLU6

activation uint8

Figure 13: Integer Arithmetic-only Inference: The convolution operation takes unsigned int8 weights and inputs, accumulates them to unsigned int32, and then performs a 32-bit addition with biases. The ReLU6 operation outputs 8-bit integers. Adopted from [120]

4.2.2. Logarithmic Quantization

Bit-shift operations are inexpensive to implement in hard-

ware compared to multiplication operations. FPGA imple-

mentations [6] speciﬁcally beneﬁt by converting ﬂoating-

point multiplication into bit shifts. Network inference can

be further optimized if weights are also constrained to be

power-of-two with variable-length encoding. Logarithmic

quantization takes advantage of this by being able to express

a larger dynamic range compared to linear quantization.

Inspired by binarized networks [52], introduced in Sec-

tion 4.2.3, Lin [156] forced the neuron output into a power-

of-two value. This converts multiplications into bit-shift

operations by quantizing the representations at each layer of

the binarized network. Both training and inference time are

thus reduced by eliminating multiplications.

Incremental Network Quantization (INQ) [269] replaces

weights with power-of-two values. This reduces computa-

tion time by converting multiplies into shifts. INQ weight

quantization is performed iteratively. In one iteration, weight

pruning-inspired weight partitioning is performed using group-

wise quantization. These weights are then ﬁne-tuned by using

a pruning-like measurement [92, 88]. Group-wise retraining

ﬁne-tunes a subset of weights in full precision to preserve

ensemble accuracy. The other weights are converted into

power-of-two format. After multiple iterations most of the

full precision weights are converted to power-of-two. The

ﬁnal networks have weights from 2 (ternary) to 5 bits with

values near zero set to zero. Results of group-wise iterative

quantization show lower error rates than a random power-of-

two strategy. Speciﬁcally, INQ obtained 71× compression

with 0.52% top-1 accuracy loss on the ILSVRC-2012 with

AlexNet.

Logarithmic Neural Networks (LogNN) [175] quantize

weights and features into a log-based representation. Loga-

rithmic backpropagation during training is performed using

shift operations. Bases other than

2 can be used.

√ 2

based arithmetic is described as a trade-oﬀ between dynamic

range and representation precision. 2 showed 7× compression with 6.2% top-5 accuracy loss on AlexNet, while √
2
showed 1.7% top-5 accuracy loss.

Shift convolutional neural networks (ShiftCNN) [84] im-

prove eﬃciency by quantizing and decomposing the real-

valued weights matrix into an times ranged bit-shift,

and encoding them with code-books as shown in Equa-

tion 21. ( ) is the index for the ℎ weights in the ℎ code-book. Each coded weight can be indexed by the NB-bit expression.

∑

=

idx ( )

=1
(21) = 0, ±2− +1, ±2− , ±2− −1, … , ±2− −⌊ ∕2⌋+2

where = 2 − 1

Note that the number of code-books can be greater than one. This means the encoded weight might be a combination of multiple shift operations. This property allows ShiftCNN to expand to a relatively large-scale quantization or to shrink to binarized or ternary weights. We discuss ternary weights in Section 4.2.3. ShiftCNN was deployed on an FPGA platform and achieved comparable accuracy on the ImageNet dataset with 75% power saving and up to 1090× clock cycle speed-up. ShiftCNN achieves this impressive result without requiring retraining. With = 2 and = 4 encoding, SqueezeNet [115] has only 1.01% top-1 accuracy loss. The loss for GoogLeNet, ResNet-18, and ResNet-50 is 0.39%, 0.54%, and 0.67%, respectively, While compressing the weights into 7/32 of the original size. This implies that the weights have signiﬁcant redundancy.
Based on LogNN, Cai [30] proposed improvements by disabling activation quantization to reduce overhead during inference. This also reduced the clamp bound hyperparameter tuning during training. These changes resulted in many lowvalued weights that are rounded to the nearest value during encoding. As 2 s.t. ∈ increases quantized weights sparsity as increases. In this research, is allowed to be real-valued numbers as ∈ to quantize the weights. This makes weight quantization more complex. However, a codebook helps to reduce the complexity.
In 2019, Huawei proposed DeepShift, a method of saving computing power by shift convolution [62]. DeepShift removed all ﬂoating-point multiply operations and replaced them with bit reverse and bit shift. The quantized weight
transformation is shown mathematically in Equation 22, where is a sign matrix, is a shift matrix, and is the set of integers.

= × 2 , s.t. ∈ ℤ, ∈ {−1, 0, +1} (22)

Results indicate that DeepShift networks cannot be easily trained from scratch. They also show that shift-format networks do not directly learn for lager datasets such as Imagenet. Similar to INQ, they show that ﬁne-tuning a pretrained network can improve performance. For example, with the same conﬁguration of 32-bit activations and 6-bit shift-format weights, the top-1 ILSVRC-2012 accuracy loss on ResNet-18 for trained from scratch and tuned from a pretrained model are 4.48% and 1.09%, respectively.
DeepShift proposes models with diﬀerential backpropagation for generating shift coeﬃcients during the retraining process. DeepShift-Q [62] is trained with ﬂoating-point parameters in backpropagation with values rounded to a suitable

T Liang et al.: Preprint submitted to Elsevier

Page 17 of 41

Survey on pruning and quantization

format during inference. DeepShift-PS directly adopts the shift and sign parameters as trainable parameters.
Since logarithmic encoding has larger dynamic range, redundant networks particularly beneﬁt. However, less redundant networks show signiﬁcant accuracy loss. For example, VGG-16 which is a redundant network shows 1.31% accuracy loss on top-1 while DenseNet-121 shows 4.02% loss.

4.2.3. Plus-minus Quantization Plus-minus quantization was in 1990 [208]. This tech-
nique reduces all weights to 1-bit representations. Similar to logarithmic quantization, expensive multiplications are removed. In this section, we provide an overview of signiﬁcant binarized network results. Simons [216] and Qin [198] provide an in-depth review of BNNs.
Binarized neural networks (BNN) have only 1-bit weights and often 1-bit activations. 0 and 1 are encoded to represent -1 and +1, respectively. Convolutions can be separated into multiplies and additions. In binary arithmetic, single bit operations can be performed using and, xnor, and bit-count. We follow the introduction from [273] to explain bit-wise operation. Single bit ﬁxed point dot products are calculated as in Equation 23, where and is a bit-wise AND operation and bitcount counts the number of 1’s in the bit string.

⋅ = bitcount(and( , )), s.t. ∀ , , ∈ {0, 1} (23)

This can be extended into multi-bit computations as in Equa-

tion 24 [53]. and are M-bit and K-bit ﬁxed point inte-

gers, subject to

=∑

−1 =0

( )2 and

=

∑

−1 =0

( )2

, where (

(

))

−1 =0

and

(

(

))

−1 =0

are

bit

vectors.

−1 −1

x⋅y =

∑

∑ 2

+

bitcount

and

=0 =0

(x),

(y)

, (24)

s.t. (x) , (y) ∈ {0, 1}∀ , , .

By removing complicated ﬂoating-point multiplications, networks are dramatically simpliﬁed with simple accumulation hardware. Binarization not only reduces the network size by up-to 32×, but also drastically reduces memory usage resulting in signiﬁcantly lower energy consumption [174, 112]. However, reducing 32-bit parameters into a single bit results in a signiﬁcant loss of information, which decreases prediction accuracy. Most quantized binary networks signiﬁcantly under-perform compared to 32-bit competitors.
There are two primary methods to reduce ﬂoating-point values into a single bit: 1) stochastic and 2) deterministic [52]. Stochastic methods consider global statistics or the value of input data to determine the probability of some parameter to be -1 or +1. Deterministic binarization directly computes the bit value based on a threshold, usually 0, resulting in a sign function. Deterministic binarization is much simpler to implement in hardware.
Binary Connect (BC), proposed by Courbariaux [52], is an early stochastic approach to binarize neural networks. They binarized the weights both in forward and backward propagation. Equation 25 shows the stochastic binarization

with a hard sigmoid probability ( ). Both the activations and the gradients use 32-bit single precision ﬂoating point. The trained BC network shows 1.18% classiﬁcation error on the small MNIST dataset but 8.27% classiﬁcation on the larger CIFAR-10 dataset.

+1, with probability = ( )

=

−1, with probability 1 −

(25)

where ( ) = clamp + 1 , 0, 1 2

Courbariaux extended BC networks by binarizing the activations. He named them BinaryNets [53], which is recognized as the ﬁrst BNN. They also report a customized binary matrix multiplication GPU kernel that accelerates the calculation by 7×. BNN is considered the ﬁrst binarized neural network where both weights and activations are quantized to binary values [216]. Considering the hardware cost of stochastic binarization, they made a trade-oﬀ to apply deterministic binarization in most circumstances. BNN reported 0.86% error on MNIST, 2.53% error on SVHN, and 10.15% error on CIFAR-10. The ILSVRC-2012 dataset accuracy results for binarized AlexNet and GoogleNet are 36.1% top-1 and 47.1%, respectively while the FP32 original networks achieve 57% and 68%, respectively [112].
Rastegari [200] explored binary weight networks (BWN) on the ILSVRC dataset with AlexNet and achieved the same classiﬁcation accuracy as the single precision version. The key is a scaling factor ∈ ℝ+ applied to an entire layer of binarized weights . This results in similar weights values as if they were computed using FP32 ≈ . They also applied weight binarization on ResNet-18 and GoogLeNet, resulting in 9.5% and 5.8% top-1 accuracy loss compared to the FP32 version, respectively. They also extended binarization to activations called XNOR-Net and evaluated it on the large ILSVRC-2012 dataset. Compared to BNN, XNORNet also applied a scaling factor on the input feature and a rearrangement of the network structure (swapping the convolution, activation, and BN). Finally, XNOR-Net achieved 44.2% top-1 classiﬁcation accuracy on ILSVRC-2012 with AlexNet, while accelerating execution time 58× on CPUs. The attached scaling factor extended the binarized value expression, which reduced the network distortion and lead to better ImageNet accuracy.
DoReFa-Net [272] also adopts plus-minus arithmetic for quantized network. DoReFa additionally quantizes gradients to low-bit widths within 8-bit expressions during the backward pass. The gradients are quantized stochastically in back propagation. For example, it takes 1 bit to represent weights layer-wise, 2-bit activations, and 6-bits for gradients. We describe training details in Section 4.2.5. They found 9.8% top-1 accuracy loss on AlexNet with ILSVRC-2012 using the 1-2-6 combination. The result for the 1-4-32 combination is 2.9%.
Li [146] and Leng [144] showed that for ternary weights (−1, 0, and + 1), in Ternary Weight Networks (TWN), only a slight accuracy loss was realized. Compared to BNN, TWN has an additional value to reduce information loss while still

T Liang et al.: Preprint submitted to Elsevier

Page 18 of 41

Survey on pruning and quantization

keeping computational complexity similar to BNN’s. Ternary logic may be implemented very eﬃciently in hardware, as the additional value (zero) do not actually participate in computations [50]. TWN adopts the 2-distance to ﬁnd the scale and formats the weights into −1, 0, and + 1 with a threshold generated by an assumption that the weighs are uniformly distributed such as in [− , ]. This resulted in up to 16× model compression with 3.6% ResNet-18 top-1 accuracy loss on ILSVRC-2012.
Trained Ternary Quantization (TTQ) [274] extended TWN by introducing two dynamic constraints to adjust the quantization threshold. TTQ outperformed the full precision AlexNet on the ILSVRC-2012 top-1 classiﬁcation accuracy by 0.3%. It also outperformed TWN by 3%.
Ternary Neural Networks (TNN) [6] extend TWN by quantizing the activations into ternary values. A teacher network is trained with full precision and then using transfer learning the same structure is used but replacing the full precision values with a ternarized student in a layer-wise greedy method. A small diﬀerence between the real-valued teacher network and the ternarized student network is that they activate the output with a ternary output activation function to simulate the real TNN output. TNN achieves 1.67% MNIST classiﬁcation error and 12.11% classiﬁcation error on CIFAR10. TNN has slightly lower accuracy compared to TWN (an additional 1.02% MNIST error).
Intel proposed Fine-Grained Quantization (FGQ) [170] to generalize ternary weights by splitting them into several groups and with independent ternary values. The FGQ quantized ResNet-101 network achieved 73.85% top-1 accuracy on the ImageNet dataset (compared with 77.5% for the baseline) using four groups weights and without re-training. FGQ also showed improvements in (re)training demonstrating a top-1 accuracy improvement from 48% on non-trained to 71.1% top-1 on ResNet-50. ResNet-50’s baseline accuracy is 75%. Four groups FGQ with ternary weights and low bit-width activations achieves about 9× acceleration.
MeliusNet [21] is a binary neural network that consist of two types of binary blocks. To mitigate drawbacks of low bit width networks, reduced information quality, and reduced network capacity, MeliusNet used a combination of dense block [22] which increases network channels by concatenating derived channels from the input to improve capacity and improvement block [161] which improves the quality of features by adding additional convolutional activations onto existing extra channels from dense block. They achieved accuracy results comparable to MobileNet on the ImageNet dataset with MeliusNet-59 reporting 70.7% top1 accuracy while requiring only 0.532 BFLOPs. A similar sized 17MB MobileNet required 0.569 BFLOPs achieving 70.6% accuracy.
AdderNet [35] is another technique that replaces multiply arithmetic but allows larger than 1-bit parameters. It replaces all convolutions with addition. Equation 26 shows that for a standard convolution, AdderNet formulates it as a similarity

measure problem

(

∑∑∑ , , )=

(

(

+,

+,

),

(, ,

, )) (26)

=0 =0 =0

where ∈ ℝ × × × out is a ﬁlter, is the kernel size, is an input channel and out is an output channel. ∈ ℝℎ× × stands for the input feature height ℎ and width . With this formulation, the output is calculated with the similarity
(⋅, ⋅), i.e., ( , ) = × for conventional convolution where the similarity measure is calculated by cross correlation. Equation 27 mathematically describes AdderNet, which replaces the multiply with subtraction. The 1-distance is applied to calculate the distance between the ﬁlter and the input feature. By replacing multiplications with subtractions, AdderNet speeds up inference by transforming 3.9 billion multiplications into subtractions with a loss in ResNet-50 accuracy of 1.3%.

∑∑∑

( , , )=−

| ( + , + , ) − ( , , , )|

=0 =0 =0

(27)

NAS can be applied to BNN construction. Shen [213] adopted evolutionary algorithms to ﬁnd compact but accurate models achieving 69.65% top-1 accuracy on ResNet-18 with ImageNet at 2.8× speed-up. This is better performance than the 32-bit single precision baseline ResNet-18 accuracy of 69.6%. However, the search approach is time consuming taking 1440 hours on an nVidia V100 GPU to search 50k ImageNet images to process an initial network.

4.2.4. Other Approaches to Quantization Weight sharing by vector quantization can also be consid-
ered a type of quantization. In order to compress parameters to reduce memory space usage, parameters can be clustered and shared. K-means is a widely used clustering algorithm and has been successfully applied to DNNs with minimal loss of accuracy [76, 243, 143] achieving 16-24 times compression with 1% accuracy loss on the ILSVRC-2012 dataset [76, 243].
HashNet [37] uses a hash to cluster weights. Each hash group is replaced with a single ﬂoating-point weight value. This was applied to FCLs and shallow CNN models. They found a compression factor of 64× outperforms equivalentsized networks on MNIST and seven other datasets they evaluated.
In 2016 Han applied Huﬀman coding with Deep Compression [92]. The combination of weight sharing, pruning, and huﬀman coding achieved 49× compression on VGG-16 with no loss of accuracy on ILSVRC-2012, which was SOTA at the time.
The Hessian method was applied to measure the importance of network parameters and therefore improve weight quantization [45]. They minimized the average Hessian weighted quantization errors to cluster parameters. They found compression ratios of 40.65 on AlexNet with 0.94%

T Liang et al.: Preprint submitted to Elsevier

Page 19 of 41

Survey on pruning and quantization

accuracy loss on ILSVRC-2012. Weight regularization can slightly improve the accuracy of quantized networks by penalizing weights with large magnitudes [215]. Experiments showed that 2 regularization improved 8-bit quantized MobileNet top-1 accuracy by 0.23% on ILSVRC-2012.
BN has proved to have many advantages including addressing the internal covariate shift issue [119]. It can also be considered a type of quantization. However, quantization performed with BN may have numerical instabilities. The BN layer has nonlinear square and square root operations. Low bit representations may be problematic when using nonlinear operations. To solve this, 1-norm BN [245] has only linear operations in both forward and backward training. It provided 1.5× speedup at half the power on FPGA platforms and can be used with both training and inference.
4.2.5. Quantization-aware Training Most quantization methods use a global (layer-wise) quan-
tization to reduce the full precision model into a reduced bit model. Thus can result in non-negligible accuracy loss. A signiﬁcant drawback of quantization is information loss caused by the irreversible precision reducing transform. Accuracy loss is particularly visible in binary networks and shallow networks. Applying binary weights and activations to ResNet-34 or GoogLeNet resulted in 29.10% and 24.20% accuracy loss, respectively [53]. It has been shown that backward propagation ﬁne-tunes (retrains) a quantized network and can recover losses in accuracy caused by the quantization process [171]. The retraining is even resilient to binarization information distortions. Thus training algorithms play a crucial role when using quantization. In this section, we introduce (re)training of quantized networks.
BNN Training: For a binarized network that has binary valued weights it is not eﬀective to update the weights using gradient decent methods due to typically small derivatives. Early quantized networks were trained with a variation of Bayesian inference named Expectation Back Propagation (EBP) [220, 41]. This method assigns limited parameter precision (e.g., binarized) weights and activations. EBP infers networks with quantized weights by updating the posterior distributions over the weights. The posterior distributions are updated by diﬀerentiating the parameters of the backpropagation.
BinaryConnect [52] adopted the probabilistic idea of EBP but instead of optimizing the weights posterior distribution, BC preserved ﬂoating-point weights for updates and then quantized them into binary values. The real-valued weights update using the back propagated error by simply ignoring the binarization in the update.
A binarized Network has only 1-bit parameters - ±1 quantized from a sign function. Single bit parameters are nondiﬀerentiable and therefore it is not possible to calculate gradients needed for parameter updating [208]. SGD algorithms have been shown to need 6 to 8 bits to be eﬀective [180]. To work around these limitations the Straight-Through Estimator (STE), previously introduced by Hinton [102], was ap-

plied for propagating gradients by using discretization [112]. Equation 28 show the STE for sign binarization, where denotes the cost function, is the real-valued weights, and
is the binarized weight produced by the sign function. STE bypasses the binarization function to directly calculate real-valued gradients. The ﬂoating-point weights are then updated using methods like SGD. To avoid real-valued weights approaching inﬁnity, BNNs typically clamp ﬂoating-point weights to the desired range of ±1 [112].

Forward : = sign

(28)

Backward :

=

| |≤1

Unlike the forward phase where weights and activations are produced with deterministic quantization, in the gradient phase, the low bit gradients should be generated by stochastic quantization [89, 271]. DoReFa [272] ﬁrst successfully trained a network with gradient bit-widths less than eight and achieved a comparable result with -bit quantization arithmetic. This low bit-width gradient scheme could accelerate training in edge devices with little impact to network accuracy but minimal inference acceleration compared to BNNs. DoReFa quantizes the weights, features, and gradients into many levels obtaining a larger dynamic range than BNNs. They trained AlexNet on ImageNet from scratch with 1-bit weights, 2-bit activations, and 6-bit gradients. They obtained 46.1% top-1 accuracy (9.8% loss comparing with the full precision counterpart). Equation 29 shows the weight quantizing approach. is the weights (the same as in Equation 28), limit is a limit function applied to the weights keeping them in the range of [0, 1], and quantize quantizes the weights into -levels. Feature quantization is performed using the
= quantize function.

= 2 quantize limit( ) − 1

where quantize ( ) = 1 round 2 − 1 2 −1

and limit( ) =

tanh( )

+1

2 max(| tanh( )|) 2

, (29)

In DoReFa, gradient quantization is shown in Equation 30, where d = ∕ is the backprogagated gradient of the cost function to output .

̃ = 2 max0(|d |) quantize

d

+1 −1

2 max0(|d |) 2 2

(30)

As in deep feed forward networks, the exploding gradient problem can cause BNN’s not to train. To address this issue, Hou [104] formulated the binarization eﬀect on the network loss as an optimization problem which was solved by a proximal Newton’s algorithm with diagonal Hessian approximation that directly minimizes the loss with respect to the binary weights. This optimization found 0.09% improvement on MNIST dataset compared with BNN.

T Liang et al.: Preprint submitted to Elsevier

Page 20 of 41

Survey on pruning and quantization

Alpha-Blending (AB) [162] was proposed as a replacement for STE. Since STE directly sets the quantization function gradients to 1, a hypothesis was made that STE tuned networks could suﬀer accuracy losses. Figure 14 shows that AB introduces an additional scale coeﬃcient . Real-valued weights and quantized weights are both kept. During training
is gradually raised to 1 until a fully quantized network is realized.

weights FP32

optimizer loss scaling

quantizer float2half

input layer feature FP32
quantizer float2half

F32

F16

weights float STE
quantizer

weights binary

feature float

conv

weights float

AB quantizer

1-α weights binary
α +

feature float

conv

weights FP16

feature FP16

F16

F16

conv
activation FP16

(a) Straight-through Estimator

(b) Alpha-Blending Approach

Figure 14: STE and AB: STE directly bypasses the quantizer while AB calculates gradients for real-valued weights by introducing additional coeﬃcients [162]

Figure 15: Mixed Precision Training [172]: FP16 is applied in the forward and backward pass, while FP32 weights are maintained for the update.

Low Numerical Precision Training: Training with low numerical precision involves taking the low precision values into both forward and backward propagation while maintaining the full precision accumulated results. Mixed Precision [172, 54] training uses FP16 or 16-bit integer (INT16) for weight precision. This has been shown to be inaccurate for gradient values. As shown in Figure 15, full precision weights are maintained for gradient updating, while other operands use half-ﬂoat. A loss scaling technique is applied to keep very small magnitude gradients from aﬀecting the computation since any value less than 2−24 becomes zero in half-precision [172]. Speciﬁcally, a scaler is introduced to the loss value before backpropagation. Typically, the scaler is a bit-shift optimal value 2 obtained empirically or by statistical information.
In TensorFlow-Lite [120], training proceeds with real values while quantization eﬀects are simulated in the forward pass. Real-valued parameters are quantized to lower precision before convolutional layers. BN layers are folded into convolution layers. More details are described in Section 4.3.2.
As in binarized networks, STE can also be applied to reduced precision training such as 8-bit integers [131].
4.3. Quantization Deployment In this section, we describe implementations of quanti-
zation deployed in popular frameworks and hardware. In Section 4.3.1 we give an introduction to deployment issues.

In Section 4.3.2, we discuss deep learning libraries and frameworks. We introduce their speciﬁcation in Table 2 and then compare their performance in Table 3. We also discuss hardware implementations of DNNs in Section 4.3.3. Dedicated hardware is designed or programmed to support eﬃcient processing of quantized networks. Specialized CPU and GPU operations are discussed. Finally, in Section 4.3.4 we discuss DNN compilers.
4.3.1. Deployment Introduction With signiﬁcant resource capability, large organizations
and institutions usually have their own proprietary solutions for applications and heterogeneous platforms. Their support to the quantization is either inference only or as well as training. The frameworks don’t always follow the same idea of quantization. Therefore there are diﬀerences between them, so performs.
With DNNs being applied in many application areas, the issue of eﬃcient use of hardware has received considerable attention. Multicore processors and accelerators have been developed to accelerate DNN processing. Many types of accelerators have been deployed, including CPUs with instruction enhancements, GPUs, FPGAs, and specialized AI accelerators. Often accelerators are incorporated as part of a heterogeneous system. A Heterogeneous System Architecture (HSA) allows the diﬀerent processors to integrate into a system to simultaneously access shared memory. For example, CPUs and GPUs using cache coherent shared virtual memory on the same System of Chip (SoC) or connected by

T Liang et al.: Preprint submitted to Elsevier

Page 21 of 41

Survey on pruning and quantization

Table 2 Low Precision Libraries Using Quantization: QAT is quantization-aware training, PTQ is post-training quantization, and oﬀset indicates the zero point in Equation 12.

Name
ARM CMSIS NN [129] MACE [247] MKL-DNN [204] NCNN [229] Paddle [13] QNNPACK [61] Ristretto [90] SNPE [228] Tensor-RT [173] TF-Lite [1]

Institution
Arm XiaoMi Intel Tencent Baidu Fackbook LEPS Qualcomm nVidia Google

Core Lib
CMSIS gemm gemmlowp

Precision
8-bit 8-bit 8-bit 8-bit 8-bit 8-bit 3 method 16/8-bit 8-bit 8-bit

Method
deploy only QAT and PTQ PTQ, mixed oﬀset, and QAT PTQ w/o oﬀset QAT and PTQ w/o oﬀset PTQ w/ oﬀset QAT PTQ w/ oﬀset, max-min PTQ w/o oﬀset PTQ w/ oﬀset

Platform
Arm Cortex-M Processor Mobile - CPU, Hexagon Chips, MTK APU Intel AVX Core Mobile Platform Mobile Platform Mobile Platform Desktop Platform Snapdragon CPU, GPU, DSP nVidia GPU Mobile Platform

Open-sourced
No Yes Yes Yes Yes Yes Yes No Yes Yes

PCIe with platform atomics can share the same address space [74]. Floating-point arithmetic units consume more energy and take longer to compute compared to integer arithmetic units. Consequently, low-bitwidth architectures are designed to accelerate computation [179]. Specialized algorithms and eﬃcient hardware can accelerate neural network processing during both training and inference [202].
4.3.2. Eﬃcient Kernels Typically low precision inference in only executed on
convolutional layers. Intermediate values passed between layers use 32-bit ﬂoating-point. This makes many of the frameworks amenable to modiﬁcations.
Table 2 gives a list of major low precision acceleration frameworks and libraries. Most of them use INT8 precision. We will next describe some popular and open-source libraries in more detail.
Tensor RT [232, 242] is an nVidia developed C++ library that facilitates high-performance inference on NVIDIA GPUs. It is a low precision inference library that eliminates the bias term in convolutional layers. It requires a calibration set to adjust the quantization thresholds for each layer or channel. Afterwards the quantized parameters are represented by 32bit ﬂoating-point scalar and INT8 weights.
Tensor RT takes a pre-trained ﬂoating-point model and generates a reusable optimized 8-bit integer or 16-bit half ﬂoat model. The optimizer performs network proﬁling, layer fusion, memory management, and operation concurrency. Equation 31 shows the convolution-dequantization dataﬂow in Tensor RT for 8-bit integers. The intermediate result of convolution by INT8 input feature 8 and weights 8 are accumulated into INT32 tensor 32. They are dequantized by dividing by the feature and weight scales , .

32 = 8 ∗ 8,

32 =

32
×

(31)

Tensor RT applies a variant of max-abs quantization to reduce storage requirements and calculation time of the zero point term in Equation 15 by ﬁnding the proper threshold instead of the absolute value in the ﬂoating-point tensor. KL-divergence is introduced to make a trade-oﬀ between numerical dynamic range and precision of the INT8 represen-

tation [173]. KL calibration can signiﬁcantly help to avoid accuracy loss.
The method traverses a predeﬁned possible range of scales and calculates the KL-divergences for all the points. It then selects the scale which minimizes the KL-divergence. KLdivergence is widely used in many post training acceleration frameworks. nVidia found a model calibrated with 125 images showed only 0.36% top-1 accuracy loss using GoogLeNet on the Imagenet dataset.
Intel MKL-DNN [204] is an optimized computing library for Intel processors with Intel AVX-512, AVX-2, and SSE4.2 Instruction Set Architectures (ISA). The library uses FP32 for training and inference. Inference can also be performed using 8-bits in convolutional layers, ReLU activations, and pooling layers. It also uses Winograd convolutions. MKL-DNN uses max-abs quantization shown in Equation 15, where the feature adopts unsigned 8-bit integer = 256 and signed 8-bit integer weights = 128. The rounding function (⋅) in Equation 12 uses nearest integer rounding. Equation 32 shows the quantization applied on a given tensor or each channel in a tensor. The maximum of weights and features is calculated from the maximum of the absolute value (nearest integer rounding) of the tensor and . The feature scale and weights scale are generated using and . Then quantized 8-bit signed integer weights 8, 8-bit unsigned integer feature 8 and 32-bit unsigned integer biases 32 are generated using the scales and a nearest rounding function ‖ ⋅ ‖.

{ , } = (( ( { , })) = 255 , = 127

8 = ‖ × 32‖ ∈ [−127, 127]

(32)

8 = ‖ × 32‖ ∈ [0, 255]

32 = ‖ × × 32‖ ∈ [−231, 231 − 1]

An aﬃne transformation using 8-bit multipliers and 32bit accumulates results in Equation 33 with the same scale factors as deﬁned in Equation 32 and ∗ denoting convolution.

T Liang et al.: Preprint submitted to Elsevier

Page 22 of 41

Survey on pruning and quantization

It is an approximation since rounding is ignored.

produce the quantized output . , , are the same as in Equation 35.

32 = 8 ∗ 8 + 32

≈

32 ∗ 32 + 32

(33)

= × × 32

Equation 34 is the aﬃne transformation with FP32 format. is the dequantization factor.

=( + × )∗( + × )

=∗

+ ××

(36)

+ ××

+ × ××

32 = 32 ∗ 32 + 32

≈1

32 = × 32

(34)

where = 1

Weight quantization is done prior to inference. Activation quantization factors are prepared by sampling the validation dataset to ﬁnd a suitable range (similar to Tensor RT). The quantization factors can be either FP32 in the supported devices, or rounded to the nearest power-of-two format to enable bit-shifts. Rounding reduces accuracy by about 1%.
MKL-DNN assumes activations are non-negative (ReLU activated). Local Response Normalization (LRN), a function to pick the local maximum in a local distribution, is used to avoid over-ﬁtting. BN, FCL, and soft-max using 8-bit inference are not currently supported.
TensorFlow-Lite (TF-Lite) [1] is an open source framework by Google for performing inference on mobile or embedded devices. It consists of two sets of tools for converting and interpreting quantized networks. Both PTQ and QAT are available in TF-Lite.
GEMM low-precision (Gemmlowp) [78] is a Google open source gemm library for low precision calculations on mobile and embedded devices. It is used in TF-Lite. Gemmlowp uses asymmetric quantzation as shown in Equation 35 where
, , denotes feature, weights and output, respectively. , are the scales for feature and weights, respectively. 32 is Feature value in 32-bit ﬂoating. Similarly, 32 is the Weight value in 32-bit ﬂoating point. , are the quantized Features and Weights, respectively. Asymmetric quantization introduces the zero points ( and ). This produces a more accurate numerical encoding.

32 = 32 ∗ 32

= ×( + )∗ ×( + )

(35)

= × ×( + )∗( + )

The underlined part in Equation 35 is the most computationally intensive. In addition to the convolution, the zero point also requires calculation. Gemmlowp reduces many multi-add operations by multiplying an all-ones matrix as the bias matrix and in Equation 36. This allows four multiplies to be dispatched in a three stage pipeline [131], to

Ristretto [90] is a tool for Caﬀe quantization. It uses retraining to adjust the quantized parameters. Ristretto uses a three-part quantization strategy: 1) a modiﬁed ﬁxed-point format Dynamic Fixed Point (DFP) which permits the limited bit-width precision to dynamically carry data, 2) bit-width reduced ﬂoating-point numbers called mini ﬂoat which follows the IEEE-754 standard [219], and 3) integer power of 2 weights that force parameters into power of 2 values to replace multiplies with bit shift operations.
DPF is shown in Equation 37 where takes one sign bit, FL denotes the fractional length, and is the mantissa. The total bit-width is . This quantization can encode data from various ranges to a proper format by adjusting the fractional length.

−2

(−1) ⋅ 2-FL ∑ 2 ⋅

(37)

=0

A bit shift convolution conversion is shown in Equation 38. The convolution by input and weights and bias are transformed into shift arithmetic by rounding the weights to the nearest power of 2 values. Power of 2 weights provides inference acceleration while dynamic ﬁxed point provides better accuracy.

∑ =

⋅

+

∑

(38)

≈

≪ round log2

+

NCNN [229] is a standalone framework from Tencent for efﬁcient inference on mobile devices. Inspired by Ristretto and Tensor-RT, it works with multiple operating systems and supports low precision inference [28]. It performs channel-wise quantization with KL calibration. The quantization results in 0.04% top-1 accuracy loss on ILSVRC-2012. NCNN has implementations optimized for ARM NEON. NCNN also replaces 3 × 3 convolutions with simpler Winograd convolutions [135].
Mobile AI Compute Engine (MACE) [247] from Xiaomi supports both post-training quantization and quantizationaware training. Quantization-aware training is recommended as it exhibits lower accuracy loss . Post-training quantization requires statistical information from activations collected

T Liang et al.: Preprint submitted to Elsevier

Page 23 of 41

Survey on pruning and quantization

while performing inference. This is typically performed with batch calibration of input data. MACE also supports processor implementations optimized for ARM NEON and Qualcomm’s Hexagon digital signal processor. OpenCL acceleration is also supported. Winograd convolutions can be applied for further acceleration as discussed in Section 4.2.2.
Quantized Neural Network PACKage (QNNPACK) [61] is a Facebook produced open-source library optimized for edge computing especially for mobile low precision neural network inference. It has the same method of quantization as TF-Lite including using a zero-point. The library has been integrated into PyTorch [193] to provide users a high-level interface. In addition to Winograd and FFT convolution operations, the library has optimized gemm for cache indexing and feature packing. QNNPACK has a full compiled solution for many mobile devices and has been deployed on millions of devices with Facebook applications.
Panel Dot product (PDOT) is a key feature of QNNPACK’s highly eﬃcient gemm library. It assumes computing eﬃciency is limited with memory, cache, and bandwidth instead of Multiply and Accumulate (MAC) performance. PDOT computes multiple dot products in parallel as shown in Figure 16. Rather than loading just two operands per MAC operation, PDOT loads multiple columns and rows. This improves convolution performance about 1.41× 2.23× speedup for MobileNet on mobile devices [61].

=

×

the quantized or the quantize-dequantized weights.

32

=

( (

× − 1)

)∗(

×

( − 1)

) (39)

Paddle uses max-abs in three ways to quantize parameters: 1) the average of the max absolute value in a calculation window, 2) the max absolute value during a calculation window, and 3) a sliding average of the max absolute value of the window. The third method is described in Equation 40, where is the max absolute value in the current batch, is the average value of the sliding window, and is a coeﬃcient chosen by default as 0.9.
The Paddle framework uses a specialized toolset, PaddleSlim, which supports Quantization, Pruning, Network Architecture Search, and Knowledge Distilling. They found 86.47% size reduction of ResNet-50, with 1.71% ILSVRC2012 top-1 accuracy loss.

= (1 − ) × + × −1

(40)

4.3.3. Hardware Platforms Figure 17 shows AI chips, cards, and systems plotted
by peak operations verses power in log scale originally published in [202]. Three normalizing lines are shown at 100 GOPS/Watt, 1 TOP/Watt, and 10 TOPs/Watt. Hardware platforms are classiﬁed along several dimensions including: 1) training or inference, 2) chip, card, or system form factors, 3) datacenter or mobile, and 4) numerical precision. We focus on low precision general and specialized hardware in this section.

=

×

Figure 16: PDOT: computing dot product for several points in parallel.

Paddle [13] applies both QAT and PTQ quantization with using zero-points. The dequantization operation can be performed prior to convolution as shown in Equation 39. Paddle uses this feature to do ﬂoating-point gemm-based convolutions with quantize-dequantized weights and features within the framework data-path. It introduces quantization error while maintaining the data in format of ﬂoating-point. This quantize-dequantize-convolution pipeline is called simuquantize and its results are approximately equal to a FP32>INT8->Convolutional->FP32 (quantize - convolutional dequantize) three stage model.
Simu-quantize maintains the data at each phase in 32bit ﬂoating-point facilitating backward propagation. In the Paddle framework, during backpropagation, gradients are added to the original 32-bit ﬂoating-point weights rather than

Programmable Hardware: Quantized networks with less than 8-bits of precision are typically implemented in FPGAs but may also be executed on general purpose processors.
BNN’s have been implemented on a Xilinx Zynq heterogeneous FPGA platform [267]. They have also been implemented on Intel Xeon CPUs and Intel Arria 10 FPGA heterogeneous platforms by dispatching bit operation to FPGAs and other operations to CPUs [178]. The heterogeneous system shares the same memory address space. Training is typically mapped to CPUs. FINN [231] is a specialized framework for BNN inference on FPGAs. It contains binarized fully connected, convolutional, and pooling layers. When deployed on a Zynq-7000 SoC, FINN has achieved 12.36 million images per second on the MNIST dataset with 4.17% accuracy loss.
Binarized weights with 3-bit features have been implemented on Xilinx Zynq FPGAs and Arm NEON processors [196]. The ﬁrst and last layer of the network use 8-bit quantities but all other layers use binary weights and 3-bit activation values. On an embedded platform, Zynq XCZU3EG, they performed 16 images per second for inference. To accelerate Tiny-YOLO inference, signiﬁcant eﬀorts were taken including: 1) replacing max-pool with stride 2 convolution, 2) replacing leaky ReLU with ReLU, and 3) revising the hidden layer output channel. The improved eﬃciency on the

T Liang et al.: Preprint submitted to Elsevier

Page 24 of 41

Survey on pruning and quantization

Table 3 Low Precision Libraries versus Accuracy for Common Networks in Multiple Frameworks.

Name AlexNet
GoogleNet
Inception v3 MobileNet v1
MobileNet v2 ResNet-101 ResNet-152 ResNet-18

Framework
TensorRT [173] Ristretto [90] Ristretto [90] Ristretto [90]
NCNN [28] TensorRT [173] Ristretto [90] Ristretto [90] Ristretto [90]
TF-Lite [77] TF-Lite [77]
NCNN [28] Paddle [13] TF-Lite [77] TF-Lite [77]
QNNPACK [61] TF-Lite [77] TF-Lite [77]
TensorRT [173] TF-Lite [77]
TensorRT [173]
NCNN [28]

Method
PTQ, w/o oﬀset Dynamic FP Miniﬂoat Pow-of-two
PTQ, w/o oﬀset PTQ, w/o oﬀset Dynamic FP Miniﬂoat Pow-of-two
PTQ QAT
PTQ, w/o oﬀset QAT+Pruning PTQ QAT
PTQ, w/ oﬀset PTQ QAT
PTQ, w/o oﬀset PTQ
PTQ, w/o oﬀset
PTQ, w/o oﬀset

Accuracy Float

Top-1 Top-5

57.08% 56.90% 56.90% 56.90%

80.06% 80.09% 80.09% 80.09%

68.50% 68.57% 68.93% 68.93% 68.93%

88.84% 88.83% 89.16% 89.16% 89.16%

78.00% 93.80% 78.00% 93.80%

67.26% 70.91% 70.90% 70.90%

87.92% -

71.90% 71.90% 71.90% -

74.39% 91.78% 77.00% -

74.78% 91.82%

65.49% 86.56%

Accuracy Quant

Top-1 Top-5

57.05% 56.14% 52.26% 53.57%

80.06% 79.50% 78.23% 78.25%

68.62% 68.12% 68.37% 64.02% 57.63%

88.68% 88.64% 88.63% 87.69% 81.38%

77.20% 77.50% 93.70%

66.74% 69.20% 65.70% 70.00%

87.43% -

72.14% 63.70% 70.90% -

74.40% 91.73% 76.80% -

74.70% 91.78%

65.30% 86.52%

Accuracy Diﬀ

Top-1 Top-5

-0.03% -0.76% -4.64% -3.33%

0.00% -0.59% -1.86% -1.84%

0.12% -0.45% -0.56% -4.91% -11.30%

-0.16% -0.19% -0.53% -1.47% -7.78%

-0.80%

-

-0.50% -0.10%

-0.52% -1.71% -5.20% -0.90%

-0.49% -

0.24%

-

-8.20%

-

-1.00%

-

0.01% -0.05%

-0.20%

-

-0.08% -0.04%

-0.19% -0.04%

ResNet-50 SqueezeNet
VGG-19

NCNN [28] TensorRT [173]
NCNN [28] Ristretto [90] Ristretto [90] Ristretto [90]
TensorRT [173]

PTQ, w/o oﬀset PTQ, w/o oﬀset
PTQ, w/o oﬀset Dynamic FP Miniﬂoat Pow-of-two
PTQ, w/o oﬀset

71.80% 73.23%
57.78% 57.68% 57.68% 57.68%
68.41%

89.90% 91.18%
79.88% 80.37% 80.37% 80.37%
88.78%

71.76% 73.10%
57.82% 57.21% 54.80% 41.60%
68.38%

90.06% 91.06%
79.84% 79.99% 78.28% 67.37%
88.70%

-0.04% -0.13%
0.04% -0.47% -2.88% -16.08%
-0.03%

0.16% -0.12%
-0.04% -0.38% -2.09% -13.00%
-0.08%

FPGA from 2.5 to 5 frames per second with 1.3% accuracy

loss.

TNN [6] is deployed on an FPGA with specialized com-

putation units optimized for ternary value multiplication. A

speciﬁc FPGA structure (dimensions) is determined during

synthesis to improve hardware eﬃciency. On the Sakura-X

FPGA board they achieved 255k MNIST image classiﬁca-

tions per second with an accuracy of 98.14%. A scalable de-

sign implemented on a Xilinx Virtex-7 VC709 board dramat-

ically reduced hardware resources and power consumption

but at a signiﬁcantly reduced throughput of 27k CIFAR-10

images per second [197]. Power consumption for CIFAR-10

was 6.8 Watts.

Reducing hardware costs is a key objective of logarithmic

hardware.

Xu

[249]

adopted

√ 2

based

logarithmic

quanti-

zation with 5-bits of resolution. This showed 50.8% top-1

accuracy and dissipated a quarter of the power while using

half the chip area. Half precision inference has a top-1 accu-

racy of 53.8%.

General Hardware: In addition to specialized hardware, INT8 quantization has been widely adopted in many general purpose processor architectures. In this section we provide a high-level overview. A detailed survey on hardware eﬃciency for processing DNNs can be found in [202].
CNN acceleration on ARM CPUs was originally implemented by ARM advanced SIMD extensions known as NEON. The ARM 8.2 ISA extension added NEON support for 8-bit integer matrix operations [8]. These were implemented in the CPU IP cores Cortex-A75 and A55 [9] as well as the Mali-G76 GPU IP core [10]. These cores have been integrated into the Kirin SoC by Huawei, Qualcomm Snapdragon SoC, MediaTek Helio SoC, and Samsung Exynos [116]. For example on Exynos 9825 Octa, 8-bit integer quantized MobileNet v2 can process an image in 19ms (52 images per second) using the Mali-G76 [116].
Intel improved the integer performance about 33% with Intel Advanced Vector Extension 512 (AVX-512) ISA [204]. This 512-bit SIMD ISA extension included a Fused Multiply-

T Liang et al.: Preprint submitted to Elsevier

Page 25 of 41

Neural Network Processing Performance
Survey on pruning and quantization

Peak GOps/Second

WaveSystem

Data Center Chips &

DGX-2

Cards

Arria GX1150 Baidu WaveDPU

Turing

Cambricon

Nervana2 Cambricon

V100 TPU3

DGX-1
DGX-Station
Data Center Systems

ArriaGX1155

TPU2

Goya Nervana

GraphCoreNode AMD-MI60

Very Low Power

Mobile TPU1

P100

A111ISS00hTto0iPTeDruemriGaDaMrnaiiOIagNTOnTaapENropOsuyaDes/eopWiNr/aiWsons/rsNWthaoZyMnoqvT-i0ZPd2yUi0unRSEsqo8dX-c30gk52MeM-c0G7ahaZ6ClilPiypiAe-n7U1Slq5l28s-04DZ25Ca0DSUAtiJ1arrAer0anJitr2taNsreixGiotaa-snXoGVoTX1XAnX1aF1Tr5v1r1XPZ0ii5ae2yG0GrnXqAA-10Tsr1r6r5iu0a0eGNXAoP1rM1htXh5iD7i0Sl-2iMyn9sx0I6CF lPushtiKe7Gr28r1a00pFh2CxSorkeyCL2akeSP

Peak Power (W)
Presentation Name - 26 of
Figure Author Initials MM/DD/YY 17: HaSrdlidweacroeurptelasytfoofrAmlbserftoRrenutehuerr,aMl InTeLtiwncoorlknsLaebﬃorcaiteonrycSyudpeerpcloomyp, uatidngopCteendterfrom [202].

Legend
Computation Precision Int1 Int2 Int8 Int8 -> Int16 Int12 -> Int16 Int16 Int32 Float16 Float16 -> Float32 Float32 Float64
Form Factor Chip Card System
Computation Type Inference Training

Add (FMA) instruction. Low precision computation on nVidia GPUs was enabled
since the Pascal series of GPUs [184]. The Turing GPU architecture [188] introduced specialized units to processes INT4 and INT8. This provides real-time integer performance on AI algorithms used in games. For embedded platforms, nVidia developed Jetson platforms [187]. They use CUDA Maxwell cores [183] that can process half-precision types. For the data center, nVidia developed the extremely high performance DGX system [185]. It contains multiple high-end GPUs interconnected using nVidia’s proprietary bus nVLINK. A DGX system can perform 4-bit integer to 32-bit ﬂoating point operations.
4.3.4. DNN Compilers Heterogeneous neural networks hardware accelerators
are accelerating deep learning algorithm deployment [202]. Often exchange formats can be used to import/export models. Further, compilers have been developed to optimize models and generate code for speciﬁc processors. However several challenges remain:
• Network Parsing: Developers design neural network models on diﬀerent platforms using various frameworks and programming languages. However, they have common parts, such as convolution, activation, pooling, etc. Parsing tools analyze the model compositions and transfer them into the uniﬁed representation.
• Structure Optimization: The model may contain operations used in training that aren’t required for inference. Tool-kits and compilers should optimize these structures (e.g. BN folding as discussed in Section 2.5).
• Intermediate Representation (IR): An optimized model

should be properly stored for further deployment. Since the inference engine is uncertain, the stored IR should include the model architecture and the trained weights. A compiler can then read the model and optimize it for a speciﬁc inference engine.
• Compression: Compilers and optimizers should optionally be able to automatically compress arbitrary network structures using pruning and quantization.
• Deployment: The ﬁnal optimized model should be mapped to the target engine(s) which may be heterogeneous.
Open Neural Network Exchange (ONNX) [190] is an open-source tool to parse AI models written for a variety diverse frameworks. It imports and exports models using an open-source format facilitating the translation of neural network models between frameworks. It is thus capable of network parsing provided low-level operations are deﬁned in all target frameworks.
TVM [36], Glow [205], OpenVINO [118], and MLIR [134] are deep learning compilers. They diﬀer from frameworks such as Caﬀe in that they store intermediate representations and optimize those to map models onto speciﬁc hardware engines. They typically integrate both quantizationaware training and calibration-based post-training quantization. We summarize key features below. They perform all the operations noted in our list. A detailed survey can be found in [149].
TVM [36] leverages the eﬃciency of quantization by enabling deployment of quantized models from PyTorch and TF-Lite. As a compiler, TVM has the ability to map the model on general hardware such as Intel’s AVX and nVidia’s CUDA.

T Liang et al.: Preprint submitted to Elsevier

Page 26 of 41

Survey on pruning and quantization

Glow [205] enables quantization with zero points and converts the data into 8-bit signed integers using a calibrationbased method. Neither Glow or TVM currently support quantization-aware training although they both announced future support for it [205].
MLIR [134] and OpenVINO [118] have sophisticated quantization support including quantization-aware training. OpenVINO integrates it in TensorFlow and PyTorch while MLIR natively supports quantization-aware training. This allows users to ﬁne-tune an optimized model when it doesn’t satisfy accuracy criteria.
4.4. Quantization Reduces Over-ﬁtting In addition to accelerating neural networks, quantization
has also been found in some cases to result in higher accuracy. As examples: 1) 3-bit weights VGG-16 outperforms its full precision counterpart by 1.1% top-1 [144], 2) AlexNet reduces 1.0% top-1 error of the reference with 2-bit weights and 8-bit activations [66], 3) ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% [174], 4) Zhou showed a quantized model reduced the classiﬁcation error by 0.15%, 2.28%, 0.13%, 0.71%, and 1.59% on AlexNet, VGG-16, GoogLeNet, ResNet-18 and ResNet-50, respectively [269], and 5) Xu showed reduced bit quantized networks help to reduce over-ﬁtting on Fully Connected Networks (FCNs). By taking advantage of strict constraints in biomedical image segmentation they improved segmentation accuracy by 1% combined with a 6.4× memory usage reduction [251].
5. Summary
In this section we summarize the results of Pruning and Quantization.
5.1. Pruning Section 3 shows pruning is an important technique for
compressing neural networks. In this paper, we discussed pruning techniques categorized as 1) static pruning and 2) dynamic pruning. Previously, static pruning was the dominant area of research. Recently, dynamic pruning has become a focus because it can further improve performance even if static pruning has ﬁrst been performed.
Pruning can be performed in multiple ways. Elementwise pruning improves weight compression and storage. Channel-wise and shape-wise pruning can be accelerated with specialized hardware and computation libraries. Filterwise and layer-wise pruning can dramatically reduce computational complexity.
Though pruning sometimes introduces incremental improvement in accuracy by escaping a local minima [12], accuracy improvements are better realized by switching to a better network architecture [24]. For example, a separable block may provide better accuracy with reduced computational complexity [105]. Considering the evolution of network structures, performance may also be bottlenecked by the structure itself. From this point of view, Network Architecture Search and Knowledge Distillation can be options for

further compression. Network pruning can be viewed as a subset of NAS but with a smaller searching space. This is especially true when the pruned architecture no longer needs to use weights from the unpruned network (see Section 3.3). In addition, some NAS techniques can also be applied to the pruning approach including borrowing trained coeﬃcients and reinforcement learning search.
Typically, compression is evaluated on large data-sets such as the ILSVRC-2012 dataset with one thousand object categories. In practice, resource constraints in embedded devices don’t allow a large capacity of optimized networks. Compressing a model to best ﬁt a constrained environment should consider but not be limited to the deployment environment, target device, speed/compression trade-oﬀs, and accuracy requirements [29].
Based on the reviewed pruning techniques, we recommend the following for eﬀective pruning:
• Uniform pruning introduces accuracy loss therefore setting the pruning ratio to vary by layers is better [159].
• Dynamic pruning may result in higher accuracy and maintain higher network capacity [246].
• Structurally pruning a network may beneﬁt from maturing libraries especially when pruning at a high level [241].
• Training a pruned model from scratch sometimes, but not always (see Section 3.3), is more eﬃcient than tunning from the unpruned weights [160].
• Penalty-based pruning typically reduces accuracy loss compared with magnitude-based pruning [255]. However, recent eﬀorts are narrowing the gap [72].
5.2. Quantization Section 4 discusses quantization techniques. It describes
binarized quantized neural networks, and reduced precision networks, along with their training methods. We described low-bit dataset validation techniques and results. We also list the accuracy of popular quantization frameworks and described hardware implementations in Section 4.3.
Quantization usually results in a loss of accuracy due to information lost during the quantization process. This is particularly evident on compact networks. Most of the early low bit quantization approaches only compare performance on small datasets (e.g., MNIST, and CIFAR-10) [58, 94, 156, 200, 235, 269]. However, observations showed that some quantized networks could outperform the original network (see: Section 4.4). Additionally, non-uniform distribution data may lead to further deterioration in quantization performance [275]. Sometimes this can be ameliorated by normalization in ﬁne-tuning [172] or by non-linear quantization (e.g., log representation) [175].
Advanced quantization techniques have improved accuracy. Asymmetric quantization [120] maintains higher dynamic range by using a zero point in addition to a regular

T Liang et al.: Preprint submitted to Elsevier

Page 27 of 41

Survey on pruning and quantization

scale parameter. Overheads introduced by the zero point were minimized by pipelining the processing unit. Calibration based quantization [173] removed zero points and replaced them with precise scales obtained from a calibrating dataset. Quantization-aware training was shown to further improve quantization accuracy.
8-bit quantization is widely applied in practice as a good trade-oﬀ between accuracy and compression. It can easily be deployed on current processors and custom hardware. Minimal accuracy loss is experienced especially when quantizationaware training is enabled. Binarized networks have also achieved reasonable accuracy with specialized hardware designs.
Though BN has advantages to help training and pruning, an issue with BN is that it may require a large dynamic range across a single layer kernel or between diﬀerent channels. This may make layer-wise quantization more diﬃcult. Because of this per channel quantization is recommended [131].
To achieve better accuracy following quantization, we recommend:
• Use asymmetrical quantization. It preserves ﬂexibility over the quantization range even though it has computational overheads [120].
• Quantize the weights rather than the activations. Activation is more sensitive to numerical precision [75].
• Do not quantize biases. They do not require signiﬁcant storage. High precision biases in all layers [114], and ﬁrst/last layers [200, 272], maintain higher network accuracy.
• Quantize kernels channel-wise instead of layer-wise to signiﬁcantly improve accuracy [131].
• Fine-tune the quantized model. It reduces the accuracy gap between the quantized model and the real-valued model [244].
• Initially train using a 32-bit ﬂoating point model. Lowbit quantized model can be diﬃcult to train from scratch - especially compact models on large-scaled data-sets [272].
• The sensitivity of quantization is ordered as gradients, activations, and then weights [272].
• Stochastic quantization of gradients is necessary when training quantized models [89, 272].
6. Future Work
Although punning and quantization algorithms help reduce the computation cost and bandwidth burden, there are still areas for improvement. In this section we highlight future work to further improvement quantization and prunning.
Automatic Compression. Low bit width quantization can cause signiﬁcant accuracy loss, especially when the quantized bit-width is very narrow and the dataset is large [272,

155]. Automatic quantization is a technique to automatically search quantization encoding to evaluate accuracy loss verses compression ratio. Similarly, automatic prunning is a technique to automatically search diﬀerent prunning approaches to evaluate the sparsity ratio versus accuracy. Similar to hyperparameter tuning [257], this can be performed without human intervention using any number of search techniques (e.g. random search, genetic search, etc.).
Compression on Other Types of Neural Networks. Current compression research is primarily focused on CNNs. More speciﬁcally, research is primarily directed towards CNN classiﬁcation tasks. Future work should also consider other types of applications such as object detection, speech recognition, language translation, etc. Network compression verses accuracy for diﬀerent applications is an interesting area of research.
Hardware Adaptation. Hardware implementations may limit the eﬀectiveness of pruning algorithms. For example, element-wise pruning only slightly reduces computations or bandwidth when using im2col-gemm on GPU [264]. Similarly, shape-wise pruning is not typically able to be implemented on dedicated CNN accelerators. Hardware-software co-design of compression techniques for hardware accelerators should be considered to achieve the best system eﬃciency.
Global Methods. Network optimizations are typically applied separately without information from one optimization informing any other optimization. Recently, approaches that consider optimization eﬀectiveness at multiple layers have been proposed. [150] discusses pruning combined with tensor factorization that results in better overall compression. Similar techniques can be considered using diﬀerent types and levels of compression and factorization.
7. Conclusions
Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the ﬁeld of computer vision. However, complex network architectures challenge eﬃcient real-time deployment and require signiﬁcant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve.
Pruning can be categorized as static (Section 3.1) if it is performed oﬄine or dynamic (Section 3.2) if it is performed at run-time. The criteria applied to removing redundant computations if often just a simple magnitude of weights with values near zero being pruned. More complicated methods include checking the -norm. Techniques such as LASSO and Ridge are built around 1 and 2 norms. Pruning can be performed element-wise, channel-wise, shape-wise, ﬁlterwise, layer-wise and even network-wise. Each has trade-oﬀs in compression, accuracy, and speedup.
Quantization reduces computations by reducing the precision of the datatype. Most networks are trained using 32-bit ﬂoating point. Weights, biases, and activations may then be

T Liang et al.: Preprint submitted to Elsevier

Page 28 of 41

Survey on pruning and quantization
quantized typically to 8-bit integers. Lower bit width quantizations have been performed with single bit being termed a binary neural network. It is diﬃcult to (re)train very low bit width neural networks. A single bit is not diﬀerentiable thereby prohibiting back propagation. Lower bit widths cause diﬃculties for computing gradients. The advantage of quantization is signiﬁcantly improved performance (usually 2-3x) and dramatically reduced storage requirements. In addition to describing how quantization is performed we also included an overview of popular libraries and frameworks that support quantization. We further provided a comparison of accuracy for a number of networks using diﬀerent frameworks Table 2.
In this paper, we summarized pruning and quantization techniques. Pruning removes redundant computations that have limited contribution to a result. Quantization reduces computations by reducing the precision of the datatype. Both can be used independently or in combination to reduce storage requirements and accelerate inference.

T Liang et al.: Preprint submitted to Elsevier

Page 29 of 41

Survey on pruning and quantization

8. Quantization Performance Results
Table 4: Quantization Network Performance on ILSVRC2012 for various bit-widths of the weights W and activation A (aka. feature)

Model AlexNet
GoogLeNet

Deployment
QuantNet BWNH SYQ TSQ INQ PACT QIL Mixed-Precision PACT QIL QuantNet ELNN DoReFa-Net TensorRT PACT PACT DoReFa-Net QuantNet DoReFa-Net WRPN DFP16 PACT PACT SYQ QIL FP8 BalancedQ ELNN SYQ QuantNet FFN DoReFa-Net Uniﬁed INT8 DeepShift-PS WEQ LQ-NETs SYQ LQ-NETs BalancedQ WRPN-2x DoReFa-Net DeepShift-Q WRPN-2x WEQ WRPN-2x WRPN-2x SYQ ELNN WRPN-2x WRPN-2x
Mixed-Precision DeepShift-PS DFP16 AngleEye AngleEye ShiftCNN DeepShift-Q LogQuant ShiftCNN TensorRT LogQuant INQ ELNN ELNN LogQuant QNN QNN ELNN BWN AngleEye TWN ELNN BWN

Bit-width WA

1

32

1

32

28

22

5 32

4

3

4

4

16 16

32 5

55

3(±4) 32

3(±4) 32

32 3

88

22

32 2

32 5

3(±2) 32

32 4

2 32

16 16

32

4

2

1

8

33

88

32 2

3(±2) 32

1

4

2 32

2 32

32 2

88

6 32

4

4

2 32

22

1

2

22

88

1

4

6 32

32 8

34

84

4

8

1

2

2

32

4

4

32 4

16 16

6

32

16 16

16 16

16 16

3

4

6

32

32 6

2

4

8

8

6

32

5

32

3(±4) 32

3(±2) 32

6

6

4

4

6

6

2

32

1

32

8

8

2

32

1

32

2

32

Acc. Drop Top-1 Top-5

-1.70% -1.40% -1.00% -0.90% -0.87% -0.60% -0.20% -0.16% -0.10% -0.10% -0.10% 0.00% 0.00% 0.03% 0.10% 0.20% 0.20% 0.30% 0.30% 0.40% 0.49% 0.50% 0.50% 0.50% 0.50% 0.50% 0.60% 0.80% 0.90% 0.90% 1.00% 1.00% 1.00% 1.19% 1.20% 1.30% 1.30% 1.40% 1.40% 1.50% 1.50% 1.55% 1.60% 1.60% 1.70% 1.70% 1.70% 1.80% 1.90% 1.90%
-0.10% -0.09% -0.08% 0.05% 0.05% 0.05% 0.27% 0.36% 0.39% 0.45% 0.64% 0.76% 2.40% 2.80% 3.43% 5.10% 5.20% 5.60% 5.80% 6.00% 7.50% 8.40% 9.70%

-1.50% -0.70% -0.60% -0.30% -1.39% -1.00% -0.20% -0.10% 0.20% -0.90% 0.00% -0.70% -0.20% -0.50% 0.00% -0.50% 0.59% -0.10% -0.10% 0.80% -2.00% 0.60% 0.80% 0.30% 0.30% 0.10% 0.67% 1.00% 0.80% 1.00% 1.40% -1.00% 0.81% 1.10% 1.60% 1.80% -
-0.09% 0.00% 0.45% 0.45% 0.09% 0.29% 0.28% 0.29% 0.19% 0.67% 0.25% 1.40% 1.60% 0.78% 7.80% 8.10% 3.50% 4.80% 3.20% 4.80% 5.70% 6.50%

Ref.
[253] [107] [66] [239] [269] [44] [127] [172] [44] [127] [253] [144] [272] [173] [44] [44] [272] [253] [272] [174] [54] [44] [44] [66] [127] [237] [273] [144] [66] [253] [238] [272] [275] [62] [192] [262] [66] [262] [273] [174] [272] [62] [174] [192] [174] [174] [66] [144] [174] [174]
[172] [62] [54] [85] [85] [84] [62] [30] [84] [173] [30] [269] [144] [144] [30] [113] [113] [144] [200] [85] [146] [144] [200]

Model MobileNet V1 MobileNet V2 ResNet-18
ResNet-34

Deployment
ShiftCNN LogQuant LogQuant LogQuant LogQuant BNN AngleEye
HAQ-Cloud HAQ-Edge MelinusNet59 HAQ-Edge PACT PACT HAQ-Cloud HAQ-Edge PACT PACT HAQ-Cloud PACT
HAQ-Edge HAQ-Cloud Uniﬁed INT8 PACT HAQ-Edge HAQ-Cloud PACT HAQ-Cloud HAQ-Edge PACT
RangeBN LBM QuantNet QIL QuantNet ShiftCNN LQ-NETs QIL LPBN QuantNet PACT SeerNet ShiftCNN PACT INQ Uniﬁed INT8 QIL LQ-NETs QIL DeepShift-Q ELNN PACT PACT QuantNet ELNN DeepShift-PS ABC-Net ELNN DoReFa-Net SYQ DoReFa-Net LQ-NETs DoReFa-Net ELNN QIL DoReFa-Net QIL LQ-NETs GroupNet-8 PACT DoReFa-Net TTN TTQ AddNN ELNN LPBN PACT DoReFa-Net QuantNet INQ
WRPN-2x WRPN-2x

WA

1

4

32 3

33

4

4

32 4

1

1

66

66

66

1

1

55

66

66

55

4

4

55

55

4

4

4

4

66

66

88

66

55

55

55

4

4

4

4

4

4

88

88

5 32

55

3(±4) 32

34

4

32

3 32

32 5

3(±2) 32

32 4

4

1

24

55

4

32

88

55

3(±4) 32

33

6 32

3(±2) 32

32 3

4

4

2 32

3(±4) 32

6

32

5

32

3(±2) 32

32 5

2

8

32 4

3

3

5

5

2

32

2

32

32 3

4

4

2

32

1

1

3

3

4

4

2

32

2

32

32 32

2

32

32 4

32 2

3

3

1

32

2

32

4

4

4

8

Top-1
11.26% 13.50% 18.07% 18.57% 18.57% 24.20% 52.10%
-0.38% -0.38% -0.10% 0.24% 0.36% 0.36% 0.85% 3.42% 3.82% 3.82% 5.49% 8.38%
-0.08% -0.04% 0.00% 0.56% 0.91% 2.36% 2.97% 4.80% 4.82% 10.42%
-0.60% -0.60% -0.30% -0.20% -0.10% 0.03% 0.20% 0.30% 0.30% 0.40% 0.40% 0.42% 0.54% 0.60% 0.62% 0.63% 0.80% 0.90% 1.00% 1.09% 1.10% 1.20% 1.20% 1.20% 1.30% 1.44% 1.46% 1.60% 1.70% 1.90% 1.90% 2.00% 2.00% 2.10% 2.10% 2.10% 2.20% 2.20% 2.20% 2.30% 2.30% 2.50% 2.70% 2.80% 2.80% 2.90% 2.90% 2.90% 3.10% 3.10%
-0.93% -0.89%

Top-5
7.36% 8.93% 12.85% 13.21% 13.21% 20.90% 57.35%
-0.23% -0.34% 0.08% 0.26% 0.26% 0.48% 1.95% 2.20% 2.20% 3.25% 5.66%
-0.11% 0.01% 0.25% 0.34% 1.31% 1.67% 2.79% 2.92% 6.53%
-0.10% -0.10% 0.12% 0.50% 0.30% 0.40% 0.20% 0.30% 0.18% 0.34% 0.30% 0.10% 0.80% 0.47% 0.70% 0.70% 0.60% 0.60% 0.60% 0.67% 1.18% 1.10% 1.00% 1.40% 1.10% 1.60% 1.30% 1.50% 1.30% 1.40% 1.60% 1.40% 1.40% 1.50% 1.80% 2.00% 1.50% 1.50% 1.70% 2.00% 2.00% 1.90% 1.90%
-

Ref.
[84] [30] [30] [30] [30] [53] [85]
[236] [236] [21] [236] [44] [44] [236] [236] [44] [44] [236] [44]
[236] [236] [275] [44] [236] [236] [44] [236] [236] [44]
[15] [268] [253] [127] [253] [84] [262] [127] [31] [253] [44] [32] [84] [44] [269] [275] [127] [262] [127] [62] [144] [44] [44] [253] [144] [62] [155] [144] [272] [66] [272] [262] [272] [144] [127] [272] [127] [262] [276] [44] [272] [274] [277] [35] [144] [31] [44] [272] [253] [269]
[174] [174]

T Liang et al.: Preprint submitted to Elsevier

Page 30 of 41

Model ResNet-50

Survey on pruning and quantization

Deployment
QIL QIL WRPN-2x WRPN-2x WRPN-2x SeerNet Uniﬁed INT8 LCCL QIL WRPN-3x WRPN-3x GroupNet-8 dLAC LQ-NETs GroupNet**-5 IR-Net QIL WRPN-2x WRPN-2x LQ-NETs GroupNet-5 ABC-Net HWGQ WAGEUBN ABC-Net LQ-NETs LQ-NETs BCGD HWGQ IR-Net CI-BCNN (add) Bi-Real WRPN-1x WRPN CI-BCNN DoReFa-Net DoReFa-Net ABC-Net BNN
Mixed-Precision DFP16 QuantNet LQ-NETs FGQ TensorRT PACT QuantNet Uniﬁed INT8 ShiftCNN ShiftCNN PACT LPBN ShiftCNN DeepShift-Q DeepShift-PS PACT QuantNet PACT dLAC QuantNet AddNN LQ-NETs LQ-NETs INQ PACT IAO PACT HAQ HAQ LQ-NETs LPBN Deep Comp. PACT ShiftCNN FFN UNIQ QuantNet SYQ FGQ-TWN PACT LQ-NETs

W A Top-1

4

4

55

4

2

24

22

4

1

88

33

1

1

1

1

1

1

2 16

33

1

1

1

32

22

1

1

1

1

22

1

1

55

1

32

88

33

1

2

4

4

1

4

1

2

1

1

1

1

1

1

1

1

1

1

1

1

1

4

1

2

1

1

1

1

16 16

16 16

5 32

4

32

32 32

88

55

3(±4) 32

88

34

34

4

4

32 5

24

6 32

6 32

5

32

3(±2) 32

4

32

2

16

2

32

32 32

4

4

2

32

5

32

3

32

8

8

3

3

2MP 4MP

MP MP

3

3

32 4

3

MP

4

2

2

4

2

32

4

8

1

32

2

8

2

8

2

2

2

2

0.00% 0.00% 0.01% 0.09% 0.27% 0.35% 0.39% 0.43% 0.60% 0.90% 1.21% 1.40% 1.67% 1.90% 2.70% 2.90% 3.10% 3.40% 3.74% 4.00% 4.70% 4.90% 5.10% 5.18% 6.60% 6.70% 6.70% 7.60% 9.00% 9.50% 11.07% 11.10% 12.80% 13.05% 13.59% 14.60% 20.40% 20.90% 29.10%
-0.12% -0.07% 0.00% 0.00% 0.00% 0.13% 0.20% 0.20% 0.26% 0.29% 0.31% 0.40% 0.40% 0.67% 0.81% 0.84% 0.90% 0.90% 1.00% 1.20% 1.20% 1.30% 1.30% 1.30% 1.32% 1.40% 1.50% 1.60% 1.91% 2.09% 2.20% 2.20% 2.29% 2.40% 2.49% 2.50% 2.60% 3.20% 3.70% 4.29% 4.70% 4.90%

Top-5
0.17% 0.17% 1.00% 0.89% 1.20% 2.10% 1.80% 2.30% 3.40% 3.10% 3.40% 3.90% 4.40% 4.40% 4.70% 5.60% 6.20% 6.39% 7.40% 8.65% 14.80% 24.20%
-0.06% 0.00% 0.10% 0.12% -0.20% 0.00% 0.15% 0.16% -0.10% 0.40% 0.41% 0.21% 0.31% 0.20% 0.40% 0.20% 0.60% 1.20% 0.80% 0.90% 0.41% 0.50% 0.50% 1.60% 1.20% 1.20% 1.64% 1.30% 1.70% 2.10% 2.60% 2.90%

Ref.
[127] [127] [174] [174] [174] [32] [275] [59] [127] [174] [174] [276] [235] [262] [276] [199] [127] [174] [174] [262] [276] [155] [31] [254] [155] [262] [262] [256] [31] [199] [240] [252] [174] [174] [240] [272] [272] [155] [272]
[172] [54] [253] [262] [170] [173] [44] [253] [275] [84] [84] [44] [81] [84] [62] [62] [44] [253] [44] [235] [253] [35] [262] [262] [269] [44] [120] [44] [236] [236] [262] [81] [92] [44] [84] [238] [18] [253] [66] [170] [44] [262]

Model
ResNet-100 ResNet-101 ResNet-150 ResNet-152 SqueezeNet
VGG-16

Deployment
SYQ DoReFa-Net DoReFa-Net FGQ ABC-Net FGQ-TWN HWGQ
IAO TensorRT FGQ-TWN FGQ-TWN IAO TensorRT dLAC
AngleEye ShiftCNN ShiftCNN AngleEye AngleEye ShiftCNN
ELNN ELNN AngleEye DFP16 AngleEye SeerNet DeepShift-Q FFN DeepShift-PS DeepShift-Q INQ TWN ELNN TSQ AngleEye BWN AngleEye ELNN AngleEye AngleEye LogQuant LogQuant LogQuant LogQuant LogQuant LogQuant LogQuant LDR LogNN

WA

1

8

4

4

55

28

55

24

1

2

88 88 28 24 88 88 2 16

16 16

34

24

88

66

1

4

3(±4) 32

3(±2) 32

16 16

16 16

88

4

1

6 32

2 32

6 32

6 32

5 32

2 32

2 32

22

16 16

2 32

88

1

32

66

66

33

4

4

66

32 3

32 4

32 6

6 32

54

54

Top-1
5.40% 5.50% 5.50% 5.60% 6.30% 6.67% 6.90%
1.40% -0.01% 3.65% 6.81% 2.10% 0.08% 1.20%
0.00% 0.01% 1.01% 1.42% 28.13% 35.39%
-1.10% -0.60% 0.09% 0.11% 0.21% 0.28% 0.29% 0.30% 0.47% 0.72% 0.77% 1.10% 2.00% 2.00% 2.15% 2.20% 2.35% 3.30% 9.07% 22.38% -

Top-5
3.40% 3.30% -0.20% 3.50% 4.60%
0.05% 0.04% 0.64%
0.01% 0.01% 0.71% 1.05% 27.43% 35.09%
-1.00% -0.80% -0.05% 0.29% 0.08% 0.10% 0.11% -0.20% 0.30% 0.29% 0.08% 0.30% 0.90% 0.70% 1.49% 1.20% 1.76% 1.80% 6.58% 17.75% 0.99% 0.51% 0.83% 0.82% 0.36% 0.31% 0.76% 0.90% 1.38%

Ref.
[66] [272] [272] [170] [155] [170] [31]
[120] [173] [170] [170] [120] [173] [235]
[85] [84] [84] [85] [85] [84]
[144] [144] [85] [54] [85] [32] [62] [238] [62] [62] [62] [146] [144] [239] [85] [200] [85] [144] [85] [85] [30] [30] [30] [30] [30] [30] [30] [175] [175]

T Liang et al.: Preprint submitted to Elsevier

Page 31 of 41

Survey on pruning and quantization

References

[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro,

C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S.,

Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz,

R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R.,

Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner,

B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,

V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M.,

Yu, Y., Zheng, X., 2016. TensorFlow: Large-Scale Machine Learn-

ing on Heterogeneous Distributed Systems. arXiv preprint arXiv:

1603.04467 URL: https://arxiv.org/abs/1603.04467.

[2] Abdel-Hamid, O., Mohamed, A.r., Jiang, H., Deng, L., Penn, G.,

Yu, D., 2014. Convolutional Neural Networks for Speech Recogni-

tion. IEEE/ACM Transactions on Audio, Speech, and Language Pro-

cessing 22, 1533–1545. URL: http://ieeexplore.ieee.org/document/

6857341/, doi:10.1109/TASLP.2014.2339736.

[3] Abdelouahab, K., Pelcat, M., Serot, J., Berry, F., 2018. Accelerating

CNN inference on FPGAs: A Survey. ArXiv preprint URL: http:

//arxiv.org/abs/1806.01683.

[4] Achronix Semiconductor Corporation, 2020. FPGAs Enable the Next

Generation of Communication and Networking Solutions. White

Paper WP021, 1–15.

[5] Albanie, 2020. convnet-burden. URL: https://github.com/albanie/

convnet- burden.

[6] Alemdar, H., Leroy, V., Prost-Boucle, A., Petrot, F., 2017. Ternary

neural networks for resource-eﬃcient AI applications, in: 2017 Inter-

national Joint Conference on Neural Networks (IJCNN), IEEE. pp.

2547–2554. URL: https://ieeexplore.ieee.org/abstract/document/

7966166/, doi:10.1109/IJCNN.2017.7966166.

[7] AMD, . Radeon Instinct™ MI25 Accelerator. URL: https://www.

amd.com/en/products/professional- graphics/instinct- mi25.

[8] Arm, 2015.

ARM Architecture Reference Man-

ual ARMv8, for ARMv8-A architecture proﬁle.

https://developer.arm.com/documentation/ddi0487/latest. URL:

https://developer.arm.com/documentation/ddi0487/latest.

[9] Arm, 2020. Arm Cortex-M Processor Comparison Table. URL:

https://developer.arm.com/ip- products/processors/cortex- a.

[10] Arm, Graphics, C., 2020. MALI-G76 High-Performance

GPU for Complex Graphics Features and Bene ts High Perfor-

mance for Mixed Realities. URL: https://www.arm.com/products/

silicon- ip- multimedia/gpu/mali- g76.

[11] ARM, Reddy, V.G., 2008. Neon technology introduction. ARM Cor-

poration , 1–34URL: http://caxapa.ru/thumbs/301908/AT_-_NEON_

for_Multimedia_Applications.pdf.

[12] Augasta, M.G., Kathirvalavakumar, T., 2013. Pruning algorithms of

neural networks - A comparative study. Open Computer Science 3,

105–115. doi:10.2478/s13537-013-0109-x.

[13] Baidu, 2019. PArallel Distributed Deep LEarning: Machine Learn-

ing Framework from Industrial Practice. URL: https://github.com/

PaddlePaddle/Paddle.

[14] Balzer, W., Takahashi, M., Ohta, J., Kyuma, K., 1991. Weight

quantization in Boltzmann machines. Neural Networks 4, 405–409.

doi:10.1016/0893- 6080(91)90077- I.

[15] Banner, R., Hubara, I., Hoﬀer, E., Soudry, D., 2018.

Scalable methods for 8-bit training of neural networks,

in: Advances in Neural Information Processing Systems

(NIPS), pp. 5145–5153. URL: http://papers.nips.cc/paper/

7761- scalable- methods- for- 8- bit- training- of- neural- networks.

[16] Banner, R., Nahshan, Y., Soudry, D., 2019. Post training 4-bit quanti-

zation of convolutional networks for rapid-deployment, in: Advances

in Neural Information Processing Systems (NIPS), pp. 7950–7958.

[17] Baoyuan Liu, Min Wang, Foroosh, H., Tappen, M., Penksy, M., 2015.

Sparse Convolutional Neural Networks, in: 2015 IEEE Conference on

Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 806–

814. URL: http://ieeexplore.ieee.org/document/7298681/, doi:10.

1109/CVPR.2015.7298681.

[18] Baskin, C., Schwartz, E., Zheltonozhskii, E., Liss, N., Giryes, R.,

Bronstein, A.M., Mendelson, A., 2018. UNIQ: Uniform Noise In-

jection for Non-Uniform Quantization of Neural Networks. arXiv preprint arXiv:1804.10969 URL: http://arxiv.org/abs/1804.10969. [19] Bengio, E., Bacon, P.L., Pineau, J., Precup, D., 2015. Conditional Computation in Neural Networks for faster models. ArXiv preprint URL: http://arxiv.org/abs/1511.06297. [20] Bengio, Y., 2013. Estimating or Propagating Gradients Through Stochastic Neurons. ArXiv preprint URL: http://arxiv.org/abs/ 1305.2982. [21] Bethge, J., Bartz, C., Yang, H., Chen, Y., Meinel, C., 2020. MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy? ArXiv preprint URL: http://arxiv.org/abs/2001.05936. [22] Bethge, J., Yang, H., Bornstein, M., Meinel, C., 2019. BinaryDenseNet: Developing an architecture for binary neural networks. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019 , 1951–1960doi:10.1109/ICCVW.2019.00244. [23] Bianco, S., Cadene, R., Celona, L., Napoletano, P., 2018. Benchmark analysis of representative deep neural network architectures. IEEE Access 6, 64270–64277. doi:10.1109/ACCESS.2018.2877890. [24] Blalock, D., Ortiz, J.J.G., Frankle, J., Guttag, J., 2020. What is the State of Neural Network Pruning? ArXiv preprint URL: http: //arxiv.org/abs/2003.03033. [25] Bolukbasi, T., Wang, J., Dekel, O., Saligrama, V., 2017. Adaptive Neural Networks for Eﬃcient Inference. Thirty-fourth International Conference on Machine Learning URL: https://arxiv.org/abs/1702. 07811http://arxiv.org/abs/1702.07811. [26] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D., 2020. Language Models are Few-Shot Learners. ArXiv preprint URL: http://arxiv.org/abs/ 2005.14165. [27] Buciluaˇ, C., Caruana, R., Niculescu-Mizil, A., 2006. Model compression, in: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’06, ACM Press, New York, New York, USA. p. 535. URL: https://dl.acm. org/doi/abs/10.1145/1150402.1150464, doi:10.1145/1150402.1150464. [28] BUG1989, 2019. BUG1989/caﬀe-int8-convert-tools: Generate a quantization parameter ﬁle for ncnn framework int8 inference. URL: https://github.com/BUG1989/caffe- INT8- convert- tools. [29] Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S., 2019. Once-for-All: Train One Network and Specialize it for Eﬃcient Deployment. ArXiv preprint , 1–15URL: http://arxiv.org/abs/1908.09791. [30] Cai, J., Takemoto, M., Nakajo, H., 2018. A Deep Look into Logarithmic Quantization of Model Parameters in Neural Networks, in: Proceedings of the 10th International Conference on Advances in Information Technology - IAIT 2018, ACM Press, New York, New York, USA. pp. 1–8. URL: http://dl.acm.org/citation.cfm?doid= 3291280.3291800, doi:10.1145/3291280.3291800. [31] Cai, Z., He, X., Sun, J., Vasconcelos, N., 2017. Deep Learning with Low Precision by Half-Wave Gaussian Quantization, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 5406–5414. URL: http://ieeexplore.ieee.org/document/ 8100057/, doi:10.1109/CVPR.2017.574. [32] Cao, S., Ma, L., Xiao, W., Zhang, C., Liu, Y., Zhang, L., Nie, L., Yang, Z., 2019. SeerNet : Predicting Convolutional Neural Network Feature-Map Sparsity through LowBit Quantization. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) URL:
http://openaccess.thecvf.com/content_CVPR_2019/papers/Cao_
SeerNet_Predicting_Convolutional_Neural_Network_Feature- Map_
Sparsity_Through_Low- Bit_Quantization_CVPR_2019_paper.pdf. [33] Carreira-Perpinan, M.A., Idelbayev, Y., 2018. "Learning-
Compression" Algorithms for Neural Net Pruning, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 8532–8541. URL: https://ieeexplore.ieee.org/document/ 8578988/, doi:10.1109/CVPR.2018.00890.

T Liang et al.: Preprint submitted to Elsevier

Page 32 of 41

Survey on pruning and quantization

[34] Chellapilla, K., Puri, S., Simard, P., 2006. High Performance Convolutional Neural Networks for Document Processing, in: Tenth International Workshop on Frontiers in Handwriting Recognition. URL: https://hal.inria.fr/inria-00112631/, doi:10.1.1.137.482.
[35] Chen, H., Wang, Y., Xu, C., Shi, B., Xu, C., Tian, Q., Xu, C., 2020. AdderNet: Do We Really Need Multiplications in Deep Learning?, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1468–1477. URL: http://arxiv. org/abs/1912.13200.
[36] Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Cowan, M., Shen, H., Wang, L., Hu, Y., Ceze, L., Guestrin, C., Krishnamurthy, A., 2018. TVM: An automated end-to-end optimizing compiler for deep learning, in: Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018, pp. 579–594. URL: http://arxiv.org/abs/1802.04799.
[37] Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y., 2015. Compressing neural networks with the hashing trick., in: In International Conference on Machine Learning, pp. 2285–2294. URL: http://arxiv.org/abs/1504.04788.
[38] Chen, Y., Chen, T., Xu, Z., Sun, N., Temam, O., 2016. DianNao family: Energy-Eﬃcient Hardware Accelerators for Machine Learning. Communications of the ACM 59, 105–112. URL:
10.1145/2594446%5Cnhttps://ejwl.idm.oclc.org/login?url=http:
//search.ebscohost.com/login.aspx?direct=true&db=bth&AN=
95797996&site=ehost- livehttp://dl.acm.org/citation.cfm?doid=
3013530.2996864, doi:10.1145/2996864. [39] Cheng, J., Wang, P.s., Li, G., Hu, Q.h., Lu, H.q., 2018. Recent ad-
vances in eﬃcient computation of deep convolutional neural networks. Frontiers of Information Technology & Electronic Engineering 19, 64–77. URL: http://link.springer.com/10.1631/FITEE.1700789, doi:10.1631/FITEE.1700789. [40] Cheng, Y., Wang, D., Zhou, P., Zhang, T., 2017. A Survey of Model Compression and Acceleration for Deep Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1710.09282. [41] Cheng, Z., Soudry, D., Mao, Z., Lan, Z., 2015. Training Binary Multilayer Neural Networks for Image Classiﬁcation using Expectation Backpropagation. ArXiv preprint URL: http://cn.arxiv.org/pdf/ 1503.03562.pdfhttp://arxiv.org/abs/1503.03562. [42] Chiliang, Z., Tao, H., Yingda, G., Zuochang, Y., 2019. Accelerating Convolutional Neural Networks with Dynamic Channel Pruning, in: 2019 Data Compression Conference (DCC), IEEE. pp. 563–563. URL: https://ieeexplore.ieee.org/document/8712710/, doi:10.1109/ DCC.2019.00075. [43] Choi, B., Lee, J.H., Kim, D.H., 2008. Solving local minima problem with large number of hidden nodes on two-layered feedforward artiﬁcial neural networks. Neurocomputing 71, 3640–3643. doi:10.1016/j.neucom.2008.04.004. [44] Choi, J., Wang, Z., Venkataramani, S., Chuang, P.I.j., Srinivasan, V., Gopalakrishnan, K., 2018. PACT: Parameterized Clipping Activation for Quantized Neural Networks. ArXiv preprint , 1–15URL: http: //arxiv.org/abs/1805.06085. [45] Choi, Y., El-Khamy, M., Lee, J., 2017a. Towards the Limit of Network Quantization, in: International Conference on Learning Representations(ICLR), IEEE. URL: https://arxiv.org/abs/1612.01543http: //arxiv.org/abs/1612.01543. [46] Choi, Y., Member, S.S., Bae, D., Sim, J., Member, S.S., Choi, S., Kim, M., Member, S.S., Kim, L.s.S., Member, S.S., 2017b. EnergyEﬃcient Design of Processing Element for Convolutional Neural Network. IEEE Transactions on Circuits and Systems II: Express Briefs 64, 1332–1336. URL: http://ieeexplore.ieee.org/document/ 7893765/, doi:10.1109/TCSII.2017.2691771. [47] Chollet, F., Google, C., 2017. Xception : Deep Learning with Depthwise Separable Convolutions, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 1251– 1258. URL: http://ieeexplore.ieee.org/document/8099678/, doi:10. 1109/CVPR.2017.195. [48] Choudhary, T., Mishra, V., Goswami, A., Sarangapani, J., 2020. A comprehensive survey on model compression and acceleration.

Artiﬁcial Intelligence Review 53, 5113–5155. URL: https://doi. org/10.1007/s10462-020-09816-7, doi:10.1007/s10462-020-09816-7. [49] Cornea, M., 2015. Intel ® AVX-512 Instructions and Their Use in the Implementation of Math Functions. Intel Corporation . [50] Cotofana, S., Vassiliadis, S., Logic, T., Addition, B., Addition, S.,

1997. Low Weight and Fan-In Neural Networks for Basic Arithmetic Operations, in: 15th IMACS World Congress, pp. 227–232. doi:10. 1.1.50.4450. [51] Courbariaux, M., Bengio, Y., David, J.P., 2014. Training deep neural networks with low precision multiplications, in: International

Conference on Learning Representations(ICLR), pp. 1–10. URL: http://arxiv.org/abs/1412.7024, doi:arXiv:1412.7024. [52] Courbariaux, M., Bengio, Y., David, J.P., 2015. BinaryConnect: Training Deep Neural Networks with binary weights during propagations, in: Advances in Neural Information Processing Systems (NIPS), pp. 1–9. URL: http://arxiv.org/abs/1511.00363, doi:10. 5555/2969442.2969588. [53] Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., Bengio, Y., 2016. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. ArXiv preprint URL: https://github.com/MatthieuCourbariaux/http: //arxiv.org/abs/1602.02830. [54] Das, D., Mellempudi, N., Mudigere, D., Kalamkar, D., Avancha, S., Banerjee, K., Sridharan, S., Vaidyanathan, K., Kaul, B., Georganas, E., Heinecke, A., Dubey, P., Corbal, J., Shustrov, N., Dubtsov, R., Fomenko, E., Pirogov, V., 2018. Mixed Precision Training of Convolutional Neural Networks using Integer Operations, in: In-

ternational Conference on Learning Representations(ICLR),

pp. 1–11.

URL: https://www.anandtech.com/show/11741/

hot- chips- intel- knights- mill- live- blog- 445pm- pt- 1145pm- utchttp:
//arxiv.org/abs/1802.00930. [55] Dash, M., Liu, H., 1997. Feature selection for classiﬁcation. Intelli-
gent Data Analysis 1, 131–156. doi:10.3233/IDA-1997-1302. [56] Davis, A., Arel, I., 2013. Low-Rank Approximations for Conditional
Feedforward Computation in Deep Neural Networks, in: International Conference on Learning Representations Workshops (ICLRW), pp. 1–10. URL: http://arxiv.org/abs/1312.4461. [57] Deng, W., Yin, W., Zhang, Y., 2013. Group sparse optimization by alternating direction method, in: Van De Ville, D.,

Goyal, V.K., Papadakis, M. (Eds.), Wavelets and Sparsity XV, p. 88580R. URL: http://proceedings.spiedigitallibrary.org/ proceeding.aspx?doi=10.1117/12.2024410, doi:10.1117/12.2024410. [58] Dettmers, T., 2015. 8-Bit Approximations for Parallelism in Deep Learning, in: International Conference on Learn-

ing Representations(ICLR). URL: https://github.com/soumith/ convnet- benchmarkshttp://arxiv.org/abs/1511.04561. [59] Dong, X., Huang, J., Yang, Y., Yan, S., 2017. More is less: A more complicated network with less inference complexity. Proceedings 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017 2017-Janua, 1895–1903. URL: http://arxiv.org/abs/ 1703.08651, doi:10.1109/CVPR.2017.205. [60] Dongarra, J.J., Du Croz, J., Hammarling, S., Duﬀ, I.S., 1990. A set of level 3 basic linear algebra subprograms. ACM Transactions on Mathematical Software (TOMS) 16, 1–17. doi:10.1145/77626.79170. [61] Dukhan, M., Yiming, W., Hao, L., Lu, H., 2019. QNNPACK: Open source library for optimized mobile deep learning - Facebook

Engineering. URL: https://engineering.fb.com/ml-applications/ qnnpack/. [62] Elhoushi, M., Chen, Z., Shaﬁq, F., Tian, Y.H., Li, J.Y., 2019. DeepShift: Towards Multiplication-Less Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1905.13298. [63] Elsken, T., Metzen, J.H., Hutter, F., 2019. Neural Architecture Search. Journal of Machine Learning Research 20, 63– 77. URL: http://link.springer.com/10.1007/978-3-030-05318-5_3, doi:10.1007/978- 3- 030- 05318- 5{\_}3. [64] Engelbrecht, A.P., 2001. A new pruning heuristic based on variance analysis of sensitivity information. IEEE Transactions on Neural Networks 12, 1386–1389. doi:10.1109/72.963775.

T Liang et al.: Preprint submitted to Elsevier

Page 33 of 41

Survey on pruning and quantization

[65] Esser, S.K., Merolla, P.A., Arthur, J.V., Cassidy, A.S., Appuswamy, R., Andreopoulos, A., Berg, D.J., McKinstry, J.L., Melano, T., Barch, D.R., di Nolfo, C., Datta, P., Amir, A., Taba, B., Flickner, M.D., Modha, D.S., 2016. Convolutional networks for fast, energy-eﬃcient neuromorphic computing. Proceedings of the National Academy of Sciences 113, 11441–11446. URL: http://www.pnas.org/lookup/ doi/10.1073/pnas.1604850113, doi:10.1073/pnas.1604850113.
[66] Faraone, J., Fraser, N., Blott, M., Leong, P.H., 2018. SYQ: Learning Symmetric Quantization for Eﬃcient Deep Neural Networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
[67] Fiesler, E., Choudry, A., Caulﬁeld, H.J., 1990. Weight discretization paradigm for optical neural networks. Optical Interconnections and Networks 1281, 164. doi:10.1117/12.20700.
[68] Figurnov, M., Collins, M.D., Zhu, Y., Zhang, L., Huang, J., Vetrov, D., Salakhutdinov, R., 2017. Spatially Adaptive Computation Time for Residual Networks, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 1790–1799. URL: http://ieeexplore.ieee.org/document/8099677/, doi:10.1109/ CVPR.2017.194.
[69] FPGA, I., . Intel® FPGA Development Tools - Intel FPGA. URL: https://www.intel.com/content/www/us/en/software/ programmable/overview.html.
[70] Frankle, J., Carbin, M., 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks, in: International Conference on Learning Representations(ICLR). URL: http://arxiv.org/abs/1803. 03635.
[71] Fukushima, K., 1988. Neocognitron: A hierarchical neural network capable of visual pattern recognition. Neural Networks 1, 119–130. doi:10.1016/0893- 6080(88)90014- 7.
[72] Gale, T., Elsen, E., Hooker, S., 2019. The State of Sparsity in Deep Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1902. 09574.
[73] Gao, X., Zhao, Y., Dudziak, L., Mullins, R., Xu, C.Z., Dudziak, L., Mullins, R., Xu, C.Z., 2019. Dynamic Channel Pruning: Feature Boosting and Suppression, in: International Conference on Learning Representations (ICLR), pp. 1–14. URL: http://arxiv.org/abs/1810. 05331.
[74] Glossner, J., Blinzer, P., Takala, J., 2016. HSA-enabled DSPs and accelerators. 2015 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2015 , 1407–1411doi:10.1109/GlobalSIP. 2015.7418430.
[75] Gong, R., Liu, X., Jiang, S., Li, T., Hu, P., Lin, J., Yu, F., Yan, J., 2019. Diﬀerentiable soft quantization: Bridging full-precision and low-bit neural networks, in: Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 4851–4860. doi:10. 1109/ICCV.2019.00495.
[76] Gong, Y., Liu, L., Yang, M., Bourdev, L., 2014. Compressing Deep Convolutional Networks using Vector Quantization, in: International Conference on Learning Representations(ICLR). URL: http://arxiv. org/abs/1412.6115.
[77] Google, . Hosted models | TensorFlow Lite. URL: https://www. tensorflow.org/lite/guide/hosted_models.
[78] Google, 2018. google/gemmlowp: Low-precision matrix multiplication. https://github.com/google/gemmlowp. URL: https: //github.com/google/gemmlowp.
[79] Gordon, A., Eban, E., Nachum, O., Chen, B., Wu, H., Yang, T.J., Choi, E., 2018. MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 1586–1595. URL: https://ieeexplore.ieee.org/document/ 8578269/, doi:10.1109/CVPR.2018.00171.
[80] Gou, J., Yu, B., Maybank, S.J., Tao, D., 2020. Knowledge Distillation: A Survey. ArXiv preprint URL: http://arxiv.org/abs/2006.05525.
[81] Graham, B., 2017. Low-Precision Batch-Normalized Activations. ArXiv preprint , 1–16URL: http://arxiv.org/abs/1702.08231.
[82] Graves, A., 2016. Adaptive Computation Time for Recurrent Neural Networks. ArXiv preprint , 1–19URL: http://arxiv.org/abs/1603.

08983. [83] Greﬀ, K., Srivastava, R.K., Schmidhuber, J., 2016. Highway and
Residual Networks learn Unrolled Iterative Estimation, in: International Conference on Learning Representations(ICLR), pp. 1–14. URL: http://arxiv.org/abs/1612.07771. [84] Gudovskiy, D.A., Rigazio, L., 2017. ShiftCNN: Generalized LowPrecision Architecture for Inference of Convolutional Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1706.02393. [85] Guo, K., Sui, L., Qiu, J., Yu, J., Wang, J., Yao, S., Han, S., Wang, Y., Yang, H., 2018. Angel-Eye: A complete design ﬂow for mapping CNN onto embedded FPGA. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 37, 35–47. URL: https:// ieeexplore.ieee.org/abstract/document/7930521/, doi:10.1109/TCAD. 2017.2705069. [86] Guo, K., Zeng, S., Yu, J., Wang, Y., Yang, H., 2017. A Survey of FPGA-Based Neural Network Accelerator. ACM Transactions on Reconﬁgurable Technology and Systems 9. URL: http://arxiv.org/ abs/1712.08934https://arxiv.org/abs/1712.08934. [87] Guo, Y., 2018. A Survey on Methods and Theories of Quantized Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1808. 04752. [88] Guo, Y., Yao, A., Chen, Y., 2016. Dynamic Network Surgery for Eﬃcient DNNs, in: Advances in Neural Information Processing Systems (NIPS), pp. 1379–1387. URL: http://papers.nips.cc/paper/ 6165- dynamic- network- surgery- for- efficient- dnns. [89] Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P., 2015. Deep learning with limited numerical precision, in: International Conference on Machine Learning (ICML), pp. 1737–1746. [90] Gysel, P., Pimentel, J., Motamedi, M., Ghiasi, S., 2018. Ristretto: A Framework for Empirical Study of Resource-Eﬃcient Inference in Convolutional Neural Networks. IEEE Transactions on Neural Networks and Learning Systems 29, 1–6. URL: https://ieeexplore.ieee. org/abstract/document/8318896/, doi:10.1109/TNNLS.2018.2808319. [91] Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.A., Dally, W.J., 2016a. EIE: Eﬃcient Inference Engine on Compressed Deep Neural Network, in: 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), IEEE. pp. 243–254. URL: http://ieeexplore.ieee.org/document/7551397/http: //arxiv.org/abs/1602.01528, doi:10.1109/ISCA.2016.30. [92] Han, S., Mao, H., Dally, W.J., 2016b. Deep compression: Compressing deep neural networks with pruning, trained quantization and Huﬀman coding, in: International Conference on Learning Representations(ICLR), pp. 199–203. URL: http://arxiv.org/abs/1510.00149. [93] Han, S., Pool, J., Narang, S., Mao, H., Gong, E., Tang, S., Elsen, E., Vajda, P., Paluri, M., Tran, J., Catanzaro, B., Dally, W.J., 2016c. DSD: Dense-Sparse-Dense Training for Deep Neural Networks, in: International Conference on Learning Representations(ICLR). URL: http://arxiv.org/abs/1607.04381. [94] Han, S., Pool, J., Tran, J., Dally, W.J., 2015. Learning both Weights and Connections for Eﬃcient Neural Networks, in: Advances in Neural Information Processing Systems (NIPS), pp. 1135–1143. URL: http://arxiv.org/abs/1506.02626, doi:10.1016/S0140-6736(95) 92525- 2. [95] Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A., Ng, A.Y., 2014. Deep Speech: Scaling up end-to-end speech recognition. ArXiv preprint , 1–12URL: http://arxiv.org/abs/1412.5567. [96] HANSON, S., 1989. Comparing biases for minimal network construction with back-propagation, in: Advances in Neural Information Processing Systems (NIPS), pp. 177–185. [97] Hassibi, B., Stork, D.G., Wolﬀ, G.J., 1993. Optimal brain surgeon and general network pruning. doi:10.1109/icnn.1993.298572. [98] He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep Residual Learning for Image Recognition, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 171–180. URL: http://arxiv.org/abs/1512.03385http://ieeexplore.ieee.org/ document/7780459/, doi:10.3389/fpsyg.2013.00124. [99] He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y., 2018. Soft Filter

T Liang et al.: Preprint submitted to Elsevier

Page 34 of 41

Survey on pruning and quantization

Pruning for Accelerating Deep Convolutional Neural Networks, in: Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence (IJCAI-18), International Joint Conferences on Artiﬁcial Intelligence Organization, California. pp. 2234–2240. URL: http://arxiv.org/abs/1808.06866, doi:10.24963/ijcai.2018/309. [100] He, Y., Liu, P., Wang, Z., Hu, Z., Yang, Y., 2019. Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) URL: http://arxiv.org/abs/1811.00250. [101] He, Y., Zhang, X., Sun, J., 2017. Channel Pruning for Accelerating Very Deep Neural Networks, in: IEEE International Conference on Computer Vision (ICCV), IEEE. pp. 1398–1406. URL: http://openaccess.thecvf.com/content_ICCV_2017/papers/He_
Channel_Pruning_for_ICCV_2017_paper.pdfhttp://ieeexplore.ieee.
org/document/8237417/, doi:10.1109/ICCV.2017.155. [102] Hinton, G., 2012. Neural networks for machine learning. Technical
Report. Coursera. [103] Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhut-
dinov, R.R., 2012. Improving neural networks by preventing coadaptation of feature detectors. ArXiv preprint , 1–18URL: http: //arxiv.org/abs/1207.0580. [104] Hou, L., Yao, Q., Kwok, J.T., 2017. Loss-aware Binarization of Deep Networks, in: International Conference on Learning Representations(ICLR). URL: http://arxiv.org/abs/1611.01600. [105] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H., 2017. MobileNets: Eﬃcient Convolutional Neural Networks for Mobile Vision Applications. ArXiv preprint URL: http://arxiv.org/abs/1704.04861. [106] Hu, H., Peng, R., Tai, Y.W., Tang, C.K., 2016. Network Trimming: A Data-Driven Neuron Pruning Approach towards Eﬃcient Deep Architectures. ArXiv preprint URL: http://arxiv.org/abs/1607.03250. [107] Hu, Q., Wang, P., Cheng, J., 2018. From hashing to CNNs: Training binary weight networks via hashing, in: AAAI Conference on Artiﬁcial Intelligence, pp. 3247–3254. [108] Huang, G., Chen, D., Li, T., Wu, F., Van Der Maaten, L., Weinberger, K., 2018. Multi-scale dense networks for resource eﬃcient image classiﬁcation, in: International Conference on Learning Representations(ICLR). URL: http://image-net.org/challenges/talks/. [109] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely Connected Convolutional Networks, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 2261–2269. URL: https://ieeexplore.ieee.org/document/8099726/, doi:10.1109/CVPR.2017.243. [110] Huang, G.B., Learned-miller, E., 2014. Labeled faces in the wild: Updates and new reporting procedures. Dept. Comput. Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, Tech. Rep 14, 1–5. [111] Huang, Z., Wang, N., 2018. Data-Driven Sparse Structure Selection for Deep Neural Networks, in: Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics). volume 11220 LNCS, pp. 317– 334. URL: http://link.springer.com/10.1007/978-3-030-01270-0_ 19, doi:10.1007/978-3-030-01270-0{\_}19. [112] Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y., 2016a. Binarized Neural Networks, in: Advances in Neural Information Processing Systems (NIPS), pp. 4114–4122. URL: http://papers.nips.cc/paper/6573- binarized- neural- networks. [113] Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., Bengio, Y., 2016b. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. Journal of Machine Learning Research 18 18, 187–1. URL: http://arxiv.org/abs/1609.07061. [114] Hwang, K., Sung, W., 2014. Fixed-point feedforward deep neural network design using weights +1, 0, and -1, in: 2014 IEEE Workshop on Signal Processing Systems (SiPS), IEEE. pp. 1–6. URL: https:// ieeexplore.ieee.org/abstract/document/6986082/, doi:10.1109/SiPS. 2014.6986082. [115] Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K., 2016. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size, in: ArXiv e-prints.

URL: https://arxiv.org/abs/1602.07360http://arxiv.org/abs/1602.

07360, doi:10.1007/978-3-319-24553-9.

[116] Ignatov, A., Timofte, R., Kulik, A., Yang, S., Wang, K., Baum, F.,

Wu, M., Xu, L., Van Gool, L., 2019. AI benchmark: All about

deep learning on smartphones in 2019. Proceedings - 2019 In-

ternational Conference on Computer Vision Workshop, ICCVW

2019 , 3617–3635URL: https://developer.arm.com/documentation/

ddi0487/latest, doi:10.1109/ICCVW.2019.00447.

[117] Imagination, . PowerVR - embedded graphics processors

powering iconic products.

URL: https://www.imgtec.com/

graphics- processors/.

[118] Intel, . OpenVINO™ Toolkit. URL: https://docs.openvinotoolkit.

org/latest/index.html.

[119] Ioﬀe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep

network training by reducing internal covariate shift, in: International

Conference on Machine Learning (ICML), pp. 448–456. URL: http:

//arxiv.org/abs/1502.03167.

[120] Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.,

Adam, H., Kalenichenko, D., 2018. Quantization and Training of

Neural Networks for Eﬃcient Integer-Arithmetic-Only Inference, in:

IEEE/CVF Conference on Computer Vision and Pattern Recognition

(CVPR), IEEE. pp. 2704–2713. URL: https://ieeexplore.ieee.org/

document/8578384/, doi:10.1109/CVPR.2018.00286.

[121] Jia, Z., Tillman, B., Maggioni, M., Scarpazza, D.P., 2019. Dissect-

ing the graphcore IPU architecture via microbenchmarking. ArXiv

preprint .

[122] Jia Deng, Wei Dong, Socher, R., Li-Jia Li, Kai Li, Li Fei-Fei, 2009.

ImageNet: A large-scale hierarchical image database. IEEE/CVF

Conference on Computer Vision and Pattern Recognition (CVPR) ,

248–255doi:10.1109/cvprw.2009.5206848.

[123] Jianchang Mao, Mohiuddin, K., Jain, A., 1994. Parsimonious network

design and feature selection through node pruning, in: Proceedings

of the 12th IAPR International Conference on Pattern Recognition,

Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5),

IEEE Comput. Soc. Press. pp. 622–624. URL: http://ieeexplore.

ieee.org/document/577060/, doi:10.1109/icpr.1994.577060.

[124] Jiao, Y., Han, L., Long, X., 2020. Hanguang 800 NPU – The Ultimate

AI Inference Solution for Data Centers, in: 2020 IEEE Hot Chips 32

Symposium (HCS), IEEE. pp. 1–29. URL: https://ieeexplore.ieee.

org/document/9220619/, doi:10.1109/HCS49909.2020.9220619.

[125] Jouppi, N.P., Borchers, A., Boyle, R., Cantin, P.l., Chao, C., Clark,

C., Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Young, C.,

Ghaemmaghami, T.V., Gottipati, R., Gulland, W., Hagmann, R., Ho,

C.R., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Patil, N.,

Jaﬀey, A., Jaworski, A., Kaplan, A., Khaitan, H., Killebrew, D., Koch,

A., Kumar, N., Lacy, S., Laudon, J., Law, J., Patterson, D., Le, D.,

Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean, G., Maggiore, A.,

Mahony, M., Miller, K., Nagarajan, R., Agrawal, G., Narayanaswami,

R., Ni, R., Nix, K., Norrie, T., Omernick, M., Penukonda, N., Phelps,

A., Ross, J., Ross, M., Salek, A., Bajwa, R., Samadiani, E., Severn, C.,

Sizikov, G., Snelham, M., Souter, J., Steinberg, D., Swing, A., Tan,

M., Thorson, G., Tian, B., Bates, S., Toma, H., Tuttle, E., Vasudevan,

V., Walter, R., Wang, W., Wilcox, E., Yoon, D.H., Bhatia, S., Boden,

N., 2017. In-Datacenter Performance Analysis of a Tensor Processing

Unit. ACM SIGARCH Computer Architecture News 45, 1–12. URL:

http://dl.acm.org/citation.cfm?doid=3140659.3080246, doi:10.1145/

3140659.3080246.

[126] Judd, P., Delmas, A., Sharify, S., Moshovos, A., 2017. Cnvlutin2:

Ineﬀectual-Activation-and-Weight-Free Deep Neural Network Com-

puting. ArXiv preprint , 1–6URL: https://arxiv.org/abs/1705.

00125.

[127] Jung, S., Son, C., Lee, S., Son, J., Kwak, Y., Han, J.J., Hwang, S.J.,

Choi, C., 2018. Learning to Quantize Deep Networks by Optimizing

Quantization Intervals with Task Loss. Revue Internationale de la

Croix-Rouge et Bulletin international des Sociétés de la Croix-Rouge

URL: http://arxiv.org/abs/1808.05779, doi:arXiv:1808.05779v2.

[128] Kathail, V., 2020. Xilinx Vitis Uniﬁed Software Platform, in: Pro-

ceedings of the 2020 ACM/SIGDA International Symposium on

T Liang et al.: Preprint submitted to Elsevier

Page 35 of 41

Survey on pruning and quantization

Field-Programmable Gate Arrays, ACM, New York, NY, USA. pp. 173–174. URL: https://dl.acm.org/doi/10.1145/3373087.3375887, doi:10.1145/3373087.3375887. [129] Keil, 2018. CMSIS NN Software Library. URL: https:// arm- software.github.io/CMSIS_5/NN/html/index.html. [130] Köster, U., Webb, T.J., Wang, X., Nassar, M., Bansal, A.K., Constable, W.H., Elibol, O.H., Gray, S., Hall, S., Hornof, L., Khosrowshahi, A., Kloss, C., Pai, R.J., Rao, N., 2017. Flexpoint: An Adaptive Numerical Format for Eﬃcient Training of Deep Neural Networks. ArXiv preprint URL: http://arxiv.org/abs/1711.02213. [131] Krishnamoorthi, R., 2018. Quantizing deep convolutional networks for eﬃcient inference: A whitepaper. ArXiv preprint 8, 667– 668. URL: http://cn.arxiv.org/pdf/1806.08342.pdfhttp://arxiv. org/abs/1806.08342, doi:arXiv:1806.08342v1. [132] Krizhevsky, A., 2009. Learning Multiple Layers of Features from Tiny Images. Science Department, University of Toronto, Tech. doi:10.1.1.222.9220. [133] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet Classiﬁcation with Deep Convolutional Neural Networks, in: Advances in Neural Information Processing Systems (NIPS), pp. 1–9. URL: http://code.google.com/p/cuda-convnet/, doi:http://dx.doi.org/10. 1016/j.protcy.2014.09.007. [134] Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar, J., Riddle, R., Shpeisman, T., Vasilache, N., Zinenko, O., 2020. MLIR: A Compiler Infrastructure for the End of Moore’s Law. ArXiv preprint URL: http://arxiv.org/abs/2002.11054. [135] Lavin, A., Gray, S., 2016. Fast Algorithms for Convolutional Neural Networks, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 4013– 4021. URL: http://ieeexplore.ieee.org/document/7780804/http:// arxiv.org/abs/1312.5851, doi:10.1109/CVPR.2016.435. [136] Lebedev, V., Lempitsky, V., 2016. Fast ConvNets Using Group-Wise Brain Damage, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 2554–2564. URL:
http://openaccess.thecvf.com/content_cvpr_2016/html/Lebedev_
Fast_ConvNets_Using_CVPR_2016_paper.htmlhttp://ieeexplore.ieee.
org/document/7780649/, doi:10.1109/CVPR.2016.280. [137] Lebedev, V., Lempitsky, V., 2018. Speeding-up convolutional
neural networks: A survey. BULLETIN OF THE POLISH ACADEMY OF SCIENCES TECHNICAL SCIENCES 66, 2018. URL: http://www.czasopisma.pan.pl/Content/109869/PDF/
05_799- 810_00925_Bpast.No.66- 6_31.12.18_K2.pdf?handler=pdfhttp:
//www.czasopisma.pan.pl/Content/109869/PDF/05_799- 810_00925_
Bpast.No.66-6_31.12.18_K2.pdf, doi:10.24425/bpas.2018.125927. [138] Lecun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521,
436–444. doi:10.1038/nature14539. [139] LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P., 1998. Gradient-
based learning applied to document recognition. Proceedings of the IEEE 86, 2278–2323. URL: http://ieeexplore.ieee.org/document/ 726791/, doi:10.1109/5.726791. [140] LeCun, Y., Denker, J.S., Solla, S.A., 1990. Optimal Brain Damage, in: Advances in Neural Information Processing Systems (NIPS), p. 598–605. doi:10.5555/109230.109298. [141] Lee, N., Ajanthan, T., Torr, P.H., 2019. SnIP: Single-shot network pruning based on connection sensitivity, in: International Conference on Learning Representations(ICLR). [142] Lei, J., Gao, X., Song, J., Wang, X.L., Song, M.L., 2018. Survey of Deep Neural Network Model Compression. Ruan Jian Xue Bao/Journal of Software 29, 251–266. URL: https://www.scopus.com/
inward/record.uri?eid=2- s2.0- 85049464636&doi=10.13328%2Fj.cnki.
jos.005428&partnerID=40&md5=5a79dfdff4a05f188c5d553fb3b3123a, doi:10.13328/j.cnki.jos.005428. [143] Lei, W., Chen, H., Wu, Y., 2017. Compressing Deep Convolutional Networks Using K-means Based on Weights Distribution, in: Proceedings of the 2nd International Conference on Intelligent Information Processing - IIP’17, ACM Press, New York, New York, USA. pp. 1–6. URL: http://dl.acm.org/citation.cfm?doid=3144789.3144803, doi:10.1145/3144789.3144803.

[144] Leng, C., Li, H., Zhu, S., Jin, R., 2018. Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM. The Thirty-Second AAAI Conference on Artiﬁcial Intelligence (AAAI-18) URL: http: //arxiv.org/abs/1707.09870.
[145] Leroux, S., Bohez, S., De Coninck, E., Verbelen, T., Vankeirsbilck, B., Simoens, P., Dhoedt, B., 2017. The cascading neural network: building the Internet of Smart Things. Knowledge and Information Systems 52, 791–814. URL: http://link.springer.com/10.1007/ s10115-017-1029-1, doi:10.1007/s10115-017-1029-1.
[146] Li, F., Zhang, B., Liu, B., 2016. Ternary Weight Networks, in: Advances in Neural Information Processing Systems (NIPS). URL: http://arxiv.org/abs/1605.04711.
[147] Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P., 2017a. Pruning Filters for Eﬃcient ConvNets, in: International Conference on Learning Representations (ICLR). URL: http://arxiv.org/abs/ 1608.08710, doi:10.1029/2009GL038531.
[148] Li, H., Zhang, H., Qi, X., Ruigang, Y., Huang, G., 2019. Improved Techniques for Training Adaptive Deep Networks, in: 2019 IEEE/CVF International Conference on Computer Vision (ICCV), IEEE. pp. 1891–1900. URL: https://ieeexplore.ieee.org/document/ 9010043/, doi:10.1109/ICCV.2019.00198.
[149] Li, M., Liu, Y.I., Liu, X., Sun, Q., You, X.I.N., Yang, H., Luan, Z., Gan, L., Yang, G., Qian, D., 2020a. The Deep Learning Compiler: A Comprehensive Survey. ArXiv preprint 1, 1–36. URL: http: //arxiv.org/abs/2002.03794.
[150] Li, Y., Gu, S., Mayer, C., Van Gool, L., Timofte, R., 2020b. Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression, in: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 8015–8024. URL: https://ieeexplore.ieee.org/document/9157445/, doi:10.1109/CVPR42600.2020.00804.
[151] Li, Z., Wang, Y., Zhi, T., Chen, T., 2017b. A survey of neural network accelerators. Frontiers of Computer Science 11, 746–761. URL: http://link.springer.com/10.1007/s11704-016-6159-1, doi:10. 1007/s11704- 016- 6159- 1.
[152] Li, Z., Zhang, Y., Wang, J., Lai, J., 2020c. A survey of FPGA design for AI era. Journal of Semiconductors 41. doi:10.1088/1674-4926/ 41/2/021402.
[153] Lin, J., Rao, Y., Lu, J., Zhou, J., 2017a. Runtime Neural Pruning, in: Advances in Neural Information Processing Systems (NIPS), pp. 2178–2188. URL: https://papers.nips.cc/paper/ 6813- runtime- neural- pruning.pdf.
[154] Lin, M., Chen, Q., Yan, S., 2014. Network in network, in: International Conference on Learning Representations(ICLR), pp. 1–10.
[155] Lin, X., Zhao, C., Pan, W., 2017b. Towards accurate binary convolutional neural network, in: Advances in Neural Information Processing Systems (NIPS), pp. 345–353.
[156] Lin, Z., Courbariaux, M., Memisevic, R., Bengio, Y., 2016. Neural Networks with Few Multiplications, in: International Conference on Learning Representations(ICLR). URL:
https://github.com/hantek/http://arxiv.org/abs/1510.03009https:
//arxiv.org/abs/1510.03009. [157] Liu, J., Musialski, P., Wonka, P., Ye, J., 2013. Tensor Completion for
Estimating Missing Values in Visual Data. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, 208–220. URL: http:// ieeexplore.ieee.org/document/6138863/, doi:10.1109/TPAMI.2012.39. [158] Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C., 2017. Learning Eﬃcient Convolutional Networks through Network Slimming, in: IEEE International Conference on Computer Vision (ICCV), IEEE. pp. 2755–2763. URL: http://ieeexplore.ieee.org/document/ 8237560/, doi:10.1109/ICCV.2017.298. [159] Liu, Z., Mu, H., Zhang, X., Guo, Z., Yang, X., Cheng, T.K.T., Sun, J., 2019a. MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning, in: IEEE International Conference on Computer Vision. URL: http://arxiv.org/abs/1903.10258. [160] Liu, Z., Sun, M., Zhou, T., Huang, G., Darrell, T., 2019b. Rethinking the Value of Network Pruning, in: International Conference on Learning Representations (ICLR), pp. 1–11. URL: http:

T Liang et al.: Preprint submitted to Elsevier

Page 36 of 41

Survey on pruning and quantization

//arxiv.org/abs/1810.05270. [161] Liu, Z., Wu, B., Luo, W., Yang, X., Liu, W., Cheng, K.T., 2018. Bi-
Real Net: Enhancing the performance of 1-bit CNNs with improved representational capability and advanced training algorithm. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) 11219 LNCS, 747–763. doi:10.1007/978-3-030-01267-0{\_}44. [162] Liu, Z.G., Mattina, M., 2019. Learning low-precision neural networks without Straight-Through Estimator (STE), in: IJCAI International Joint Conference on Artiﬁcial Intelligence, International Joint Conferences on Artiﬁcial Intelligence Organization, California. pp. 3066–3072. URL: https://www.ijcai.org/proceedings/2019/425, doi:10.24963/ijcai.2019/425. [163] Luo, J.H., Wu, J., 2020. AutoPruner: An end-to-end trainable ﬁlter pruning method for eﬃcient deep model inference. Pattern Recognition 107, 107461. URL: https://linkinghub.elsevier.com/retrieve/ pii/S0031320320302648, doi:10.1016/j.patcog.2020.107461. [164] Luo, J.H.H., Wu, J., Lin, W., 2017. ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression. Proceedings of the IEEE International Conference on Computer Vision (ICCV) 2017Octob, 5068–5076. URL: http://ieeexplore.ieee.org/document/ 8237803/, doi:10.1109/ICCV.2017.541. [165] Ma, Y., Suda, N., Cao, Y., Seo, J.S., Vrudhula, S., 2016. Scalable and modularized RTL compilation of Convolutional Neural Networks onto FPGA. FPL 2016 - 26th International Conference on Field-Programmable Logic and Applications doi:10.1109/FPL.2016. 7577356. [166] Macchi, O., 1975. Coincidence Approach To Stochastic Point Process. Advances in Applied Probability 7, 83–122. doi:10.1017/ s0001867800040313. [167] Mariet, Z., Sra, S., 2016. Diversity Networks: Neural Network Compression Using Determinantal Point Processes, in: International Conference on Learning Representations(ICLR), pp. 1–13. URL: http://arxiv.org/abs/1511.05077. [168] Mathieu, M., Henaﬀ, M., LeCun, Y., 2013. Fast Training of Convolutional Networks through FFTs. ArXiv preprint URL: http: //arxiv.org/abs/1312.5851. [169] Medina, E., 2019. Habana Labs presentation. 2019 IEEE Hot Chips 31 Symposium, HCS 2019 doi:10.1109/HOTCHIPS.2019.8875670. [170] Mellempudi, N., Kundu, A., Mudigere, D., Das, D., Kaul, B., Dubey, P., 2017. Ternary Neural Networks with Fine-Grained Quantization. ArXiv preprint URL: http://arxiv.org/abs/1705.01462. [171] Merolla, P., Appuswamy, R., Arthur, J., Esser, S.K., Modha, D., 2016. Deep neural networks are robust to weight binarization and other nonlinear distortions. ArXiv preprint URL: https://arxiv.org/abs/1606. 01981http://arxiv.org/abs/1606.01981. [172] Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., Wu, H., 2017. Mixed Precision Training, in: International Conference on Learning Representations(ICLR). URL: http://arxiv.org/abs/1710. 03740. [173] Migacz, S., 2017. 8-bit inference with TensorRT. GPU Technology Conference 2, 7. URL: https://on-demand.gputechconf.com/gtc/ 2017/presentation/s7310- 8- bit- inference- with- tensorrt.pdf. [174] Mishra, A., Nurvitadhi, E., Cook, J.J., Marr, D., 2018. WRPN: Wide reduced-precision networks, in: International Conference on Learning Representations(ICLR), pp. 1–11. [175] Miyashita, D., Lee, E.H., Murmann, B., 2016. Convolutional Neural Networks using Logarithmic Data Representation. ArXiv preprint URL: http://cn.arxiv.org/pdf/1603.01025.pdfhttp: //arxiv.org/abs/1603.01025. [176] Molchanov, D., Ashukha, A., Vetrov, D., 2017. Variational dropout sparsiﬁes deep neural networks, in: International Conference on Machine Learning (ICML), pp. 3854–3863. URL: https://dl.acm. org/citation.cfm?id=3305939. [177] Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J., 2016. Pruning Convolutional Neural Networks for Resource Eﬃcient Inference, in: International Conference on Learning Representations (ICLR), pp.

1–17. URL: http://arxiv.org/abs/1611.06440. [178] Moss, D.J.M., Nurvitadhi, E., Sim, J., Mishra, A., Marr, D., Sub-
haschandra, S., Leong, P.H.W., 2017. High performance binary neural networks on the Xeon+FPGA™ platform, in: 2017 27th International Conference on Field Programmable Logic and Applications

(FPL), IEEE. pp. 1–4. URL: https://ieeexplore.ieee.org/abstract/ document/8056823/, doi:10.23919/FPL.2017.8056823. [179] Moudgill, M., Glossner, J., Huang, W., Tian, C., Xu, C., Yang, N., Wang, L., Liang, T., Shi, S., Zhang, X., Iancu, D., Nacer, G., Li, K., 2020. Heterogeneous Edge CNN Hardware Accelerator, in: The 12th

International Conference on Wireless Communications and Signal Processing, pp. 6–11. [180] Muller, L.K., Indiveri, G., 2015. Rounding Methods for Neural Networks with Low Resolution Synaptic Weights. ArXiv preprint URL: http://arxiv.org/abs/1504.05767. [181] Muthukrishnan, R., Rohini, R., 2016. LASSO: A feature selection technique in predictive modeling for machine learning, in: 2016 IEEE International Conference on Advances in Computer Applications (ICACA), IEEE. pp. 18–20. URL: http://ieeexplore.ieee.org/ document/7887916/, doi:10.1109/ICACA.2016.7887916. [182] Neill, J.O., 2020. An Overview of Neural Network Compression. ArXiv preprint , 1–73URL: http://arxiv.org/abs/2006.03669. [183] NVIDIA Corporation, 2014. NVIDIA GeForce GTX 980 Featuring Maxwell, The Most Advanced GPU Ever Made. White Paper , 1–32URL: http://international.download.nvidia.com/geforce-com/ international/pdfs/GeForce_GTX_980_Whitepaper_FINAL.PDF. [184] NVIDIA Corporation, 2015. NVIDIA Tesla P100. White Paper URL:

https://www.nvidia.com/en- us/data- center/tesla- p100/.

[185] NVIDIA Corporation, 2017a. NVIDIA DGX-1 With Tesla V100

System Architecture. White Paper URL: http://images.nvidia.com/

content/pdf/dgx1- v100- system- architecture- whitepaper.pdf.

[186] NVIDIA Corporation, 2017b.

NVIDIA Tesla V100

GPU Volta Architecture.

White Paper , 53URL:

http://images.nvidia.com/content/volta- architecture/pdf/

volta- architecture- whitepaper.pdf%0Ahttp://www.nvidia.com/
content/gated- pdfs/Volta- Architecture- Whitepaper- v1.1.pdf. [187] NVIDIA Corporation, 2018a. NVIDIA A100 Tensor Core GPU.
White Paper , 20–21. [188] NVIDIA Corporation, 2018b. NVIDIA Turing GPU Architecture.

White Paper URL: https://gpltech.com/wp-content/uploads/2018/ 11/NVIDIA- Turing- Architecture- Whitepaper.pdf. [189] Odena, A., Lawson, D., Olah, C., 2017. Changing Model Behavior at Test-Time Using Reinforcement Learning, in: International Conference on Learning Representations Workshops (ICLRW), In-

ternational Conference on Learning Representations, ICLR. URL: http://arxiv.org/abs/1702.07780. [190] ONNX, . onnx/onnx: Open standard for machine learning interoperability. URL: https://github.com/onnx/onnx. [191] Ouyang, J., Noh, M., Wang, Y., Qi, W., Ma, Y., Gu, C., Kim, S., Hong, K.i., Bae, W.K., Zhao, Z., Wang, J., Wu, P., Gong, X., Shi, J., Zhu, H., Du, X., 2020. Baidu Kunlun An AI processor for diversiﬁed workloads, in: 2020 IEEE Hot Chips 32 Symposium (HCS), IEEE. pp. 1–18. URL: https://ieeexplore.ieee.org/document/9220641/, doi:10. 1109/HCS49909.2020.9220641. [192] Park, E., Ahn, J., Yoo, S., 2017. Weighted-Entropy-Based Quantization for Deep Neural Networks, in: IEEE/CVF Conference

on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 7197–7205. URL: http://ieeexplore.ieee.org/document/8100244/, doi:10.1109/CVPR.2017.761. [193] Paszke, A., Gross, S., Bradbury, J., Lin, Z., Devito, Z., Massa, F., Steiner, B., Killeen, T., Yang, E., 2019. PyTorch : An Imperative

Style , High-Performance Deep Learning Library. ArXiv preprint . [194] Pilipović, R., Bulić, P., Risojević, V., 2018. Compression of convolu-
tional neural networks: A short survey, in: 2018 17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018 - Proceedings, IEEE. pp. 1–6. URL: https://ieeexplore.ieee.org/document/ 8345545/, doi:10.1109/INFOTEH.2018.8345545. [195] Polyak, A., Wolf, L., 2015. Channel-level acceleration of deep

T Liang et al.: Preprint submitted to Elsevier

Page 37 of 41

Survey on pruning and quantization

face representations. IEEE Access 3, 2163–2175. URL: http: //ieeexplore.ieee.org/document/7303876/, doi:10.1109/ACCESS.2015. 2494536. [196] Preuser, T.B., Gambardella, G., Fraser, N., Blott, M., 2018. Inference of quantized neural networks on heterogeneous all-programmable devices, in: 2018 Design, Automation & Test in Europe Conference & Exhibition (DATE), IEEE. pp. 833–838. URL: http://ieeexplore. ieee.org/document/8342121/, doi:10.23919/DATE.2018.8342121. [197] Prost-Boucle, A., Bourge, A., Petrot, F., Alemdar, H., Caldwell, N., Leroy, V., 2017. Scalable high-performance architecture for convolutional ternary neural networks on FPGA, in: 2017 27th International Conference on Field Programmable Logic and Applications (FPL), IEEE. pp. 1–7. URL: https://hal.archives-ouvertes.fr/ hal-01563763http://ieeexplore.ieee.org/document/8056850/, doi:10. 23919/FPL.2017.8056850. [198] Qin, H., Gong, R., Liu, X., Bai, X., Song, J., Sebe, N., 2020a. Binary neural networks: A survey. Pattern Recognition 105, 107281. URL: https://linkinghub.elsevier.com/retrieve/pii/ S0031320320300856, doi:10.1016/j.patcog.2020.107281. [199] Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., Song, J., 2020b. Forward and Backward Information Retention for Accurate Binary Neural Networks, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 2247–2256. URL: https://ieeexplore.ieee.org/document/9157443/, doi:10.1109/ CVPR42600.2020.00232. [200] Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A., 2016. XNOR-Net: ImageNet Classiﬁcation Using Binary Convolutional Neural Networks, in: European Conference on Computer Vision, Springer. pp. 525–542. URL: http://arxiv.org/abs/1603. 05279http://link.springer.com/10.1007/978- 3- 319- 46493- 0_32, doi:10.1007/978- 3- 319- 46493- 0{\_}32. [201] Reed, R., 1993. Pruning Algorithms - A Survey. IEEE Transactions on Neural Networks 4, 740–747. URL: http://ieeexplore.ieee.org/ document/248452/, doi:10.1109/72.248452. [202] Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., Kepner, J., 2019. Survey and Benchmarking of Machine Learning Accelerators, in: 2019 IEEE High Performance Extreme Computing Conference (HPEC), IEEE. pp. 1–9. URL: https://ieeexplore.ieee. org/document/8916327/, doi:10.1109/HPEC.2019.8916327. [203] Richard Chuang, Oliyide, O., Garrett, B., 2020. Introducing the Intel® Vision Accelerator Design with Intel® Arria® 10 FPGA. White Paper . [204] Rodriguez, A., Segal, E., Meiri, E., Fomenko, E., Kim, Y.J., Shen, H., 2018. Lower Numerical Precision Deep Learning Inference and Training. Intel White Paper , 1– 19URL: https://software.intel.com/sites/default/files/managed/ db/92/Lower- Numerical- Precision- Deep- Learning- Jan2018.pdf. [205] Rotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng, S., Dzhabarov, R., Gibson, N., Hegeman, J., Lele, M., Levenstein, R., Montgomery, J., Maher, B., Nadathur, S., Olesen, J., Park, J., Rakhov, A., Smelyanskiy, M., Wang, M., 2018. Glow: Graph lowering compiler techniques for neural networks. ArXiv preprint . [206] Ruﬀy, F., Chahal, K., 2019. The State of Knowledge Distillation for Classiﬁcation. ArXiv preprint URL: http://arxiv.org/abs/1912. 10850. [207] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L., 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision 115, 211– 252. URL: http://link.springer.com/10.1007/s11263-015-0816-y, doi:10.1007/s11263- 015- 0816- y. [208] Saad, D., Marom, E., 1990. Training Feed Forward Nets with Binary Weights Via a Modiﬁed CHIR Algorithm. Complex Systems 4, 573– 586. URL: https://www.complex-systems.com/pdf/04-5-5.pdf. [209] Sabour, S., Frosst, N., Hinton, G.E., 2017. Dynamic routing between capsules, in: Advances in Neural Information Processing Systems (NIPS), pp. 3857–3867. [210] Santurkar, S., Tsipras, D., Ilyas, A., Madry, A., 2018. How does

batch normalization help optimization?, in: Advances in Neural Information Processing Systems (NIPS), pp. 2483–2493. [211] Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., 2013. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, in: International Conference on Learning Representations(ICLR). URL: http://arxiv.org/abs/1312. 6229. [212] Settle, S.O., Bollavaram, M., D’Alberto, P., Delaye, E., Fernandez, O., Fraser, N., Ng, A., Sirasao, A., Wu, M., 2018. Quantizing Convolutional Neural Networks for Low-Power High-Throughput Inference Engines. ArXiv preprint URL: http://arxiv.org/abs/1805.07941. [213] Shen, M., Han, K., Xu, C., Wang, Y., 2019. Searching for accurate binary neural architectures. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019 , 2041– 2044doi:10.1109/ICCVW.2019.00256. [214] Shen, X., Yi, B., Zhang, Z., Shu, J., Liu, H., 2016. Automatic Recommendation Technology for Learning Resources with Convolutional Neural Network, in: Proceedings - 2016 International Symposium on Educational Technology, ISET 2016, pp. 30–34. doi:10.1109/ISET.2016.12. [215] Sheng, T., Feng, C., Zhuo, S., Zhang, X., Shen, L., Aleksic, M., 2018. A Quantization-Friendly Separable Convolution for MobileNets. 2018 1st Workshop on Energy Eﬃcient Machine Learning and Cognitive Computing for Embedded Applications (EMC2) , 14–18URL: https://ieeexplore.ieee.org/document/8524017/, doi:10. 1109/EMC2.2018.00011. [216] Simons, T., Lee, D.J., 2019. A review of binarized neural networks. Electronics (Switzerland) 8. doi:10.3390/electronics8060661. [217] Simonyan, K., Zisserman, A., 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition, in: International Conference on Learning Representations(ICLR), pp. 1–14. URL: http://arxiv.org/abs/1409.1556. [218] Singh, P., Kumar Verma, V., Rai, P., Namboodiri, V.P., 2019. Play and Prune: Adaptive Filter Pruning for Deep Model Compression, in: Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, International Joint Conferences on Artiﬁcial Intelligence Organization, California. pp. 3460–3466. URL: https:// www.ijcai.org/proceedings/2019/480, doi:10.24963/ijcai.2019/480. [219] Society, I.C., Committee, M.S., 2008. IEEE Standard for FloatingPoint Arithmetic. IEEE Std 754-2008 2008, 1–70. doi:10.1109/ IEEESTD.2008.4610935. [220] Soudry, D., Hubara, I., Meir, R., 2014. Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights, in: Advances in Neural Information Processing Systems (NIPS), pp. 963–971. URL: https://dl.acm.org/doi/abs/ 10.5555/2968826.2968934. [221] Srinivas, S., Babu, R.V., 2015. Data-free parameter pruning for Deep Neural Networks, in: Procedings of the British Machine Vision Conference 2015, British Machine Vision Association. pp. 1–31. URL: http://www.bmva.org/bmvc/2015/papers/paper031/index. htmlhttp://arxiv.org/abs/1507.06149, doi:10.5244/C.29.31. [222] Srivastava, N., Hinton, G., . . . , A.K.T.j.o.m., 2014, U., 2014. Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine learning research 15, 1929–1958. URL:
http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.
pdf?utm_content=buffer79b43&utm_medium=social&utm_source=
twitter.com&utm_campaign=buffer, doi:10.5555/2627435.2670313. [223] Sun, J., Luo, X., Gao, H., Wang, W., Gao, Y., Yang, X., 2020. Cate-
gorizing Malware via A Word2Vec-based Temporal Convolutional Network Scheme. Journal of Cloud Computing 9. doi:10.1186/ s13677- 020- 00200- y. [224] Sun, M., Song, Z., Jiang, X., Pan, J., Pang, Y., 2017. Learning Pooling for Convolutional Neural Network. Neurocomputing 224, 96– 104. URL: http://dx.doi.org/10.1016/j.neucom.2016.10.049, doi:10. 1016/j.neucom.2016.10.049. [225] Sze, V., Chen, Y.H.H., Yang, T.J.J., Emer, J.S., 2017. Eﬃcient Processing of Deep Neural Networks: A Tutorial and Survey. Proceedings of the IEEE 105, 2295–2329. URL: http://ieeexplore.

T Liang et al.: Preprint submitted to Elsevier

Page 38 of 41

Survey on pruning and quantization

ieee.org/document/8114708/, doi:10.1109/JPROC.2017.2761740. [226] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D.,
Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE. pp. 1–9. URL: http://ieeexplore.ieee.org/document/7298594/, doi:10. 1109/CVPR.2015.7298594. [227] TansorFlow, . Fixed Point Quantization. URL: https://www. tensorflow.org/lite/guide. [228] Technologies, Q., 2019. Snapdragon Neural Processing Engine SDK. URL: https://developer.qualcomm.com/docs/snpe/index.html. [229] Tencent, 2019. NCNN is a high-performance neural network inference framework optimized for the mobile platform. URL: https: //github.com/Tencent/ncnn. [230] Tishbirani, R., 1996. Regression shrinkage and selection via the Lasso. URL: https://statweb.stanford.edu/~tibs/lasso/lasso.pdf. [231] Umuroglu, Y., Fraser, N.J., Gambardella, G., Blott, M., Leong, P., Jahre, M., Vissers, K., 2016. FINN: A Framework for Fast, Scalable Binarized Neural Network Inference. Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays - FPGA ’17 , 65–74URL: http://dl.acm.org/citation.cfm? doid=3020078.3021744, doi:10.1145/3020078.3021744. [232] Vanholder, H., 2016. Eﬃcient Inference with TensorRT. Technical Report. [233] Vanhoucke, V., Senior, A., Mao, M.Z., 2011. Improving the speed of neural networks on CPUs URL: https://research.google/pubs/ pub37631/. [234] Venieris, S.I., Kouris, A., Bouganis, C.S., 2018. Toolﬂows for Mapping Convolutional Neural Networks on FPGAs. ACM Computing Surveys 51, 1–39. URL: http://dl.acm.org/citation.cfm?doid= 3212709.3186332, doi:10.1145/3186332. [235] Venkatesh, G., Nurvitadhi, E., Marr, D., 2017. Accelerating Deep Convolutional Networks using low-precision and sparsity, in: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE. pp. 2861–2865. URL:
https://arxiv.org/pdf/1610.00324.pdfhttp://ieeexplore.ieee.org/
document/7952679/, doi:10.1109/ICASSP.2017.7952679. [236] Wang, K., Liu, Z., Lin, Y., Lin, J., Han, S., 2019a. HAQ: Hardware-
Aware Automated Quantization With Mixed Precision, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 8604–8612. URL: http://arxiv.org/abs/1811. 08886https://ieeexplore.ieee.org/document/8954415/, doi:10.1109/ CVPR.2019.00881. [237] Wang, N., Choi, J., Brand, D., Chen, C.Y., Gopalakrishnan, K., 2018a. Training deep neural networks with 8-bit ﬂoating point numbers, in: Advances in Neural Information Processing Systems (NIPS), pp. 7675–7684. [238] Wang, P., Cheng, J., 2017. Fixed-Point Factorized Networks, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 3966–3974. URL: http://ieeexplore.ieee.org/ document/8099905/, doi:10.1109/CVPR.2017.422. [239] Wang, P., Hu, Q., Zhang, Y., Zhang, C., Liu, Y., Cheng, J., 2018b. Two-Step Quantization for Low-bit Neural Networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4376–4384. doi:10.1109/CVPR.2018.00460. [240] Wang, Z., Lu, J., Tao, C., Zhou, J., Tian, Q., 2019b. Learning channel-wise interactions for binary convolutional neural networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 568–577. doi:10.1109/CVPR. 2019.00066. [241] Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H., 2016. Learning Structured Sparsity in Deep Neural Networks, in: Advances in Neural Information Processing Systems (NIPS), IEEE. pp. 2074– 2082. URL: https://dl.acm.org/doi/abs/10.5555/3157096.3157329, doi:10.1016/j.ccr.2008.06.009. [242] Wu, H., Judd, P., Zhang, X., Isaev, M., Micikevicius, P., 2020. Integer quantization for deep learning inference: Principles and empirical evaluation. ArXiv preprint , 1–20.

[243] Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J., 2016. Quantized Convolutional Neural Networks for Mobile Devices, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 4820–4828. URL: http://arxiv.org/abs/1512.06473http:// ieeexplore.ieee.org/document/7780890/, doi:10.1109/CVPR.2016.521.
[244] Wu, S., Li, G., Chen, F., Shi, L., 2018a. Training and Inference with Integers in Deep Neural Networks, in: International Conference on Learning Representations (ICLR). URL: http://arxiv.org/abs/1802. 04680.
[245] Wu, S., Li, G., Deng, L., Liu, L., Wu, D., Xie, Y., Shi, L., 2019. L1-Norm Batch Normalization for Eﬃcient Training of Deep Neural Networks. IEEE Transactions on Neural Networks and Learning Systems 30, 2043–2051. URL: https://ieeexplore.ieee.org/abstract/ document/8528524/https://ieeexplore.ieee.org/document/8528524/, doi:10.1109/TNNLS.2018.2876179.
[246] Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L.S., Grauman, K., Feris, R., 2018b. BlockDrop: Dynamic Inference Paths in Residual Networks, in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 8817–8826. URL: https://ieeexplore.ieee.org/document/8579017/, doi:10.1109/ CVPR.2018.00919.
[247] Xiaomi, 2019. MACE is a deep learning inference framework optimized for mobile heterogeneous computing platforms. URL: https://github.com/XiaoMi/mace/.
[248] Xilinx, Inc, 2018. Accelerating DNNs with Xilinx Alveo Accelerator Cards (WP504). White Paper 504, 1–11. URL: www.xilinx.com1.
[249] Xu, J., Huan, Y., Zheng, L.R., Zou, Z., 2019. A Low-Power Arithmetic Element for Multi-Base Logarithmic Computation on Deep Neural Networks, in: International System on Chip Conference, IEEE. pp. 260–265. URL: https://ieeexplore.ieee.org/document/8618560/, doi:10.1109/SOCC.2018.8618560.
[250] Xu, S., Huang, A., Chen, L., Zhang, B., 2020. Convolutional Neural Network Pruning: A Survey, in: 2020 39th Chinese Control Conference (CCC), IEEE. pp. 7458–7463. URL: https://ieeexplore.ieee. org/document/9189610/, doi:10.23919/CCC50068.2020.9189610.
[251] Xu, X., Lu, Q., Yang, L., Hu, S., Chen, D., Hu, Y., Shi, Y., 2018a. Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8300–8308. doi:10.1109/CVPR.2018.00866.
[252] Xu, Z., Hsu, Y.C., Huang, J., 2018b. Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks, in: International Conference on Learning Representations (ICLR) - Workshop.
[253] Yang, J., Shen, X., Xing, J., Tian, X., Li, H., Deng, B., Huang, J., Hua, X.s., 2019. Quantization Networks, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. pp. 7300–7308. URL: https://ieeexplore.ieee.org/document/8953531/, doi:10.1109/CVPR.2019.00748.
[254] Yang, Y., Deng, L., Wu, S., Yan, T., Xie, Y., Li, G., 2020. Training high-performance and large-scale deep neural networks with full 8-bit integers. Neural Networks 125, 70–82. doi:10.1016/j.neunet.2019. 12.027.
[255] Ye, J., Lu, X., Lin, Z., Wang, J.Z., 2018. Rethinking the SmallerNorm-Less-Informative Assumption in Channel Pruning of Convolution Layers. ArXiv preprint URL: http://arxiv.org/abs/1802.00124.
[256] Yin, P., Zhang, S., Lyu, J., Osher, S., Qi, Y., Xin, J., 2019. Blended coarse gradient descent for full quantization of deep neural networks. Research in Mathematical Sciences 6. doi:10.1007/ s40687- 018- 0177- 6.
[257] Yogatama, D., Mann, G., 2014. Eﬃcient Transfer Learning Method for Automatic Hyperparameter Tuning, in: Kaski, S., Corander, J. (Eds.), Proceedings of the Seventeenth International Conference on Artiﬁcial Intelligence and Statistics, PMLR, Reykjavik, Iceland. pp. 1077–1085. URL: http://proceedings.mlr.press/v33/yogatama14. html.
[258] Yu, J., Lukefahr, A., Palframan, D., Dasika, G., Das, R., Mahlke, S., 2017. Scalpel: Customizing DNN pruning to the underlying hardware

T Liang et al.: Preprint submitted to Elsevier

Page 39 of 41

Survey on pruning and quantization

parallelism. ACM SIGARCH Computer Architecture News 45, 548– 560. URL: http://dl.acm.org/citation.cfm?doid=3140659.3080215, doi:10.1145/3140659.3080215. [259] Yu, J., Yang, L., Xu, N., Yang, J., Huang, T., 2018. Slimmable Neural Networks, in: International Conference on Learning Representa-

tions(ICLR), International Conference on Learning Representations, ICLR. pp. 1–12. URL: http://arxiv.org/abs/1812.08928. [260] Yuan, M., Lin, Y., 2006. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68, 49–

67. URL: http://doi.wiley.com/10.1111/j.1467-9868.2005.00532.x, doi:10.1111/j.1467- 9868.2005.00532.x. [261] Yuan, Z., Hu, J., Wu, D., Ban, X., 2020. A dual-attention recurrent neural network method for deep cone thickener underﬂow concentration prediction. Sensors (Switzerland) 20, 1–18. doi:10.3390/ s20051260. [262] Zhang, D., Yang, J., Ye, D., Hua, G., 2018. LQ-Nets: Learned quantization for highly accurate and compact deep neural networks, in: Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), pp. 373–390. doi:10.1007/978-3-030-01237-3{\_}23. [263] Zhang, Q., Zhang, M., Chen, T., Sun, Z., Ma, Y., Yu, B., 2019a.

Recent Advances in Convolutional Neural Network Acceleration. Neurocomputing 323, 37–51. URL: https://linkinghub.elsevier. com/retrieve/pii/S0925231218311007, doi:10.1016/j.neucom.2018.09. 038. [264] Zhang, S., Du, Z., Zhang, L., Lan, H., Liu, S., Li, L., Guo, Q.,

Chen, T., Chen, Y., 2016a. Cambricon-X: An accelerator for sparse neural networks, in: 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), IEEE. pp. 1–12. URL: http://ieeexplore.ieee.org/document/7783723/, doi:10.1109/ MICRO.2016.7783723. [265] Zhang, S., Wu, Y., Che, T., Lin, Z., Memisevic, R., Salakhutdinov, R., Bengio, Y., 2016b. Architectural complexity measures of recurrent neural networks, in: Advances in Neural Information Processing Systems (NIPS), pp. 1830–1838. [266] Zhang, Y., Zhao, C., Ni, B., Zhang, J., Deng, H., 2019b. Exploiting Channel Similarity for Accelerating Deep Convolutional Neural Networks. ArXiv preprint , 1–14URL: http://arxiv.org/abs/1908.02620. [267] Zhao, R., Song, W., Zhang, W., Xing, T., Lin, J.H., Srivastava, M., Gupta, R., Zhang, Z., 2017. Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs, in: Proceedings of the 2017 ACM/SIGDA International Symposium on FieldProgrammable Gate Arrays - FPGA ’17, ACM Press, New York, New

York, USA. pp. 15–24. URL: http://dl.acm.org/citation.cfm?doid=

3020078.3021741, doi:10.1145/3020078.3021741.

[268] Zhong, K., Zhao, T., Ning, X., Zeng, S., Guo, K., Wang, Y., Yang, H.,

2020. Towards Lower Bit Multiplication for Convolutional Neural

Network Training. ArXiv preprint URL: http://arxiv.org/abs/2006.

02804.

[269] Zhou, A., Yao, A., Guo, Y., Xu, L., Chen, Y., 2017a.

Incremental Network Quantization: Towards Lossless

CNNs with Low-Precision Weights, in: International

Conference on Learning Representations(ICLR).

URL:

https://github.com/Zhouaojun/Incremental- http://arxiv.org/
abs/1702.03044http://cn.arxiv.org/pdf/1702.03044.pdf. [270] Zhou, H., Alvarez, J.M., Porikli, F., 2016a. Less Is More: To-
wards Compact CNNs, in: European Conference on Computer Vision, pp. 662–677. URL: https://link.springer.com/chapter/

10.1007/978- 3- 319- 46493- 0_40http://link.springer.com/10.1007/
978-3-319-46493-0_40, doi:10.1007/978-3-319-46493-0{\_}40. [271] Zhou, S., Kannan, R., Prasanna, V.K., 2018. Accelerating low rank
matrix completion on FPGA, in: 2017 International Conference on Reconﬁgurable Computing and FPGAs, ReConFig 2017, IEEE. pp. 1–7. URL: http://ieeexplore.ieee.org/document/8279771/, doi:10. 1109/RECONFIG.2017.8279771. [272] Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., Zou, Y., 2016b. DoReFaNet: Training Low Bitwidth Convolutional Neural Networks with

Low Bitwidth Gradients. ArXiv preprint abs/1606.0, 1–13. URL: https://arxiv.org/abs/1606.06160. [273] Zhou, S.C., Wang, Y.Z., Wen, H., He, Q.Y., Zou, Y.H., 2017b. Balanced Quantization: An Eﬀective and Eﬃcient Approach to Quantized Neural Networks. Journal of Computer Science and Technology 32, 667–682. doi:10.1007/s11390-017-1750-y. [274] Zhu, C., Han, S., Mao, H., Dally, W.J., 2017. Trained Ternary Quantization, in: International Conference on Learning Representations (ICLR), pp. 1–10. URL: http://arxiv.org/abs/1612.01064. [275] Zhu, F., Gong, R., Yu, F., Liu, X., Wang, Y., Li, Z., Yang, X., Yan, J., . Towards Uniﬁed INT8 Training for Convolutional Neural Network, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). URL: http://arxiv.org/abs/1912. 12607. [276] Zhuang, B., Shen, C., Tan, M., Liu, L., Reid, I., 2019. Structured binary neural networks for accurate image classiﬁcation and semantic segmentation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2019-June, 413–422. doi:10.1109/CVPR.2019.00050. [277] Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V., 2017. Learning Transferable Architectures for Scalable Image Recognition. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 8697–8710URL: https://ieeexplore. ieee.org/abstract/document/8579005/.

T Liang et al.: Preprint submitted to Elsevier

Page 40 of 41

Survey on pruning and quantization
Tailin Liang received the B.E. degree in Computer Science and B.B.A from the University of Science and Technology Beijing in 2017. He is currently working toward a Ph.D. degree in Computer Science at the School of Computer and Communication Engineering, University of Science and Technology Beijing. His current research interests include deep learning domain-speciﬁc processors and codesigned optimization algorithms.
John Glossner received the Ph.D. degree in Electrical Engineering from TU Delft in 2001. He is the Director of the Computer Architecture, Heterogeneous Computing, and AI Lab at the University of Science and Technology Beijing. He is also the CEO of Optimum Semiconductor Technologies and President of both the Heterogeneous System Architecture Foundation and Wireless Innovation Forum. John’s research interests include the design of heterogeneous computing systems, computer architecture, embedded systems, digital signal processors, software deﬁned radios, artiﬁcial intelligence algorithms, and machine learning systems.
Lei Wang received the B.E. and Ph.D. degrees in 2006 and 2012 from the University of Science and Technology Beijing. He then served as an assistant researcher at the Institute of Automation of the Chinese Academy of Sciences during 2012-2015. He was a joint Ph.D. of Electronic Engineering at The University of Texas at Dallas during 2009-2011. Currently, he is an adjunct professor at the School of Computer and Communication Engineering, University of Science and Technology Beijing.
Shaobo Shi received the B.E. and Ph.D. degrees in 2008 and 2014 from the University of Science and Technology Beijing. He then served as an assistant researcher at the Institute of Automation of the Chinese Academy of Sciences during 2014-2017. Currently, he is a deep learning domain-speciﬁc processor engineer at Huaxia General Processor Technology. As well serve as an adjunct professor at the School of Computer and Communication Engineering, University of Science and Technology Beijing.
Xiaotong Zhang received the M.E. and Ph.D. degrees from the University of Science and Technology Beijing in 1997 and 2000, respectively, where he was a professor of Computer Science and Technology. His research interest includes the quality of wireless channels and networks, wireless sensor networks, networks management, cross-layer design and resource allocation of broadband and wireless networks, and the signal processing of communication and computer architecture.
T Liang et al.: Preprint submitted to Elsevier

Page 41 of 41

