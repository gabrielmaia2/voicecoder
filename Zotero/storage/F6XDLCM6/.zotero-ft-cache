
Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > eess > arXiv:1804.02812

Help | Advanced Search
Search
Electrical Engineering and Systems Science > Audio and Speech Processing
(eess)
[Submitted on 9 Apr 2018 ( v1 ), last revised 24 Jun 2018 (this version, v2)]
Title: Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations
Authors: Ju-chieh Chou , Cheng-chieh Yeh , Hung-yi Lee , Lin-shan Lee
Download a PDF of the paper titled Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations, by Ju-chieh Chou and 3 other authors
Download PDF

    Abstract: Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker. In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals. An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation. The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance. The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator. A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained. Conventional voice conversion metrics are reported. We also show that the speaker information has been properly reduced from the latent representations. 

Comments: 	Accepted to Interspeech 2018
Subjects: 	Audio and Speech Processing (eess.AS) ; Computation and Language (cs.CL); Sound (cs.SD)
Cite as: 	arXiv:1804.02812 [eess.AS]
  	(or arXiv:1804.02812v2 [eess.AS] for this version)
  	https://doi.org/10.48550/arXiv.1804.02812
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Ju-Chieh Chou [ view email ]
[v1] Mon, 9 Apr 2018 04:31:43 UTC (1,839 KB)
[v2] Sun, 24 Jun 2018 18:11:02 UTC (884 KB)
Full-text links:
Access Paper:

    Download a PDF of the paper titled Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations, by Ju-chieh Chou and 3 other authors
    Download PDF
    PostScript
    Other Formats 

( view license )
Current browse context:
eess.AS
< prev   |   next >
new | recent | 1804
Change to browse by:
cs
cs.CL
cs.SD
eess
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

