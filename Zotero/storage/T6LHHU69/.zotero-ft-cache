Information Science and Statistics
Series Editors: M. Jordan J. Kleinberg B. Scho¨lkopf

Information Science and Statistics
Akaike and Kitagawa: The Practice of Time Series Analysis. Bishop: Pattern Recognition and Machine Learning. Cowell, Dawid, Lauritzen, and Spiegelhalter: Probabilistic Networks and
Expert Systems. Doucet, de Freitas, and Gordon: Sequential Monte Carlo Methods in Practice. Fine: Feedforward Neural Network Methodology. Hawkins and Olwell: Cumulative Sum Charts and Charting for Quality Improvement. Jensen: Bayesian Networks and Decision Graphs. Marchette: Computer Intrusion Detection and Network Monitoring:
A Statistical Viewpoint. Rubinstein and Kroese: The Cross-Entropy Method: A Unified Approach to
Combinatorial Optimization, Monte Carlo Simulation, and Machine Learning. Studený: Probabilistic Conditional Independence Structures. Vapnik: The Nature of Statistical Learning Theory, Second Edition. Wallace: Statistical and Inductive Inference by Minimum Massage Length.

Christopher M. Bishop
Pattern Recognition and Machine Learning

Christopher M. Bishop F.R.Eng. Assistant Director Microsoft Research Ltd Cambridge CB3 0FB, U.K. cmbishop@microsoft.com http://research.microsoft.com/ϳcmbishop

Series Editors Michael Jordan Department of Computer
Science and Department of Statistics University of California, Berkeley Berkeley, CA 94720 USA

Professor Jon Kleinberg Department of Computer
Science Cornell University Ithaca, NY 14853 USA

Bernhard Scho¨lkopf Max Planck Institute for
Biological Cybernetics Spemannstrasse 38 72076 Tu¨bingen Germany

Library of Congress Control Number: 2006922522
ISBN-10: 0-387-31073-8 ISBN-13: 978-0387-31073-2
Printed on acid-free paper.
© 2006 Springer Science+Business Media, LLC All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden. The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.
Printed in Singapore. (KYO)
987654321
springer.com

This book is dedicated to my family: Jenna, Mark, and Hugh
Total eclipse of the sun, Antalya, Turkey, 29 March 2006.

Preface
Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same ﬁeld, and together they have undergone substantial development over the past ten years. In particular, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic models. Also, the practical applicability of Bayesian methods has been greatly enhanced through the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation. Similarly, new models based on kernels have had signiﬁcant impact on both algorithms and applications.
This new textbook reﬂects these recent developments while providing a comprehensive introduction to the ﬁelds of pattern recognition and machine learning. It is aimed at advanced undergraduates or ﬁrst year PhD students, as well as researchers and practitioners, and assumes no previous knowledge of pattern recognition or machine learning concepts. Knowledge of multivariate calculus and basic linear algebra is required, and some familiarity with probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.
Because this book has broad scope, it is impossible to provide a complete list of references, and in particular no attempt has been made to provide accurate historical attribution of ideas. Instead, the aim has been to give references that offer greater detail than is possible here and that hopefully provide entry points into what, in some cases, is a very extensive literature. For this reason, the references are often to more recent textbooks and review articles rather than to original sources.
The book is supported by a great deal of additional material, including lecture slides as well as the complete set of ﬁgures used in the book, and the reader is encouraged to visit the book web site for the latest information:
http://research.microsoft.com/∼cmbishop/PRML
vii

viii

PREFACE

Exercises
The exercises that appear at the end of every chapter form an important component of the book. Each exercise has been carefully chosen to reinforce concepts explained in the text or to develop and generalize them in signiﬁcant ways, and each is graded according to difﬁculty ranging from ( ), which denotes a simple exercise taking a few minutes to complete, through to ( ), which denotes a signiﬁcantly more complex exercise.
It has been difﬁcult to know to what extent these solutions should be made widely available. Those engaged in self study will ﬁnd worked solutions very beneﬁcial, whereas many course tutors request that solutions be available only via the publisher so that the exercises may be used in class. In order to try to meet these conﬂicting requirements, those exercises that help amplify key points in the text, or that ﬁll in important details, have solutions that are available as a PDF ﬁle from the book web site. Such exercises are denoted by www . Solutions for the remaining exercises are available to course tutors by contacting the publisher (contact details are given on the book web site). Readers are strongly encouraged to work through the exercises unaided, and to turn to the solutions only as required.
Although this book focuses on concepts and principles, in a taught course the students should ideally have the opportunity to experiment with some of the key algorithms using appropriate data sets. A companion volume (Bishop and Nabney, 2008) will deal with practical aspects of pattern recognition and machine learning, and will be accompanied by Matlab software implementing most of the algorithms discussed in this book.
Acknowledgements
First of all I would like to express my sincere thanks to Markus Svense´n who has provided immense help with preparation of ﬁgures and with the typesetting of the book in LATEX. His assistance has been invaluable.
I am very grateful to Microsoft Research for providing a highly stimulating research environment and for giving me the freedom to write this book (the views and opinions expressed in this book, however, are my own and are therefore not necessarily the same as those of Microsoft or its afﬁliates).
Springer has provided excellent support throughout the ﬁnal stages of preparation of this book, and I would like to thank my commissioning editor John Kimmel for his support and professionalism, as well as Joseph Piliero for his help in designing the cover and the text format and MaryAnn Brickner for her numerous contributions during the production phase. The inspiration for the cover design came from a discussion with Antonio Criminisi.
I also wish to thank Oxford University Press for permission to reproduce excerpts from an earlier textbook, Neural Networks for Pattern Recognition (Bishop, 1995a). The images of the Mark 1 perceptron and of Frank Rosenblatt are reproduced with the permission of Arvin Calspan Advanced Technology Center. I would also like to thank Asela Gunawardana for plotting the spectrogram in Figure 13.1, and Bernhard Scho¨lkopf for permission to use his kernel PCA code to plot Figure 12.17.

PREFACE

ix

Many people have helped by proofreading draft material and providing comments and suggestions, including Shivani Agarwal, Ce´dric Archambeau, Arik Azran, Andrew Blake, Hakan Cevikalp, Michael Fourman, Brendan Frey, Zoubin Ghahramani, Thore Graepel, Katherine Heller, Ralf Herbrich, Geoffrey Hinton, Adam Johansen, Matthew Johnson, Michael Jordan, Eva Kalyvianaki, Anitha Kannan, Julia Lasserre, David Liu, Tom Minka, Ian Nabney, Tonatiuh Pena, Yuan Qi, Sam Roweis, Balaji Sanjiya, Toby Sharp, Ana Costa e Silva, David Spiegelhalter, Jay Stokes, Tara Symeonides, Martin Szummer, Marshall Tappen, Ilkay Ulusoy, Chris Williams, John Winn, and Andrew Zisserman.
Finally, I would like to thank my wife Jenna who has been hugely supportive throughout the several years it has taken to write this book.
Chris Bishop Cambridge February 2006

Mathematical notation
I have tried to keep the mathematical content of the book to the minimum necessary to achieve a proper understanding of the ﬁeld. However, this minimum level is nonzero, and it should be emphasized that a good grasp of calculus, linear algebra, and probability theory is essential for a clear understanding of modern pattern recognition and machine learning techniques. Nevertheless, the emphasis in this book is on conveying the underlying concepts rather than on mathematical rigour.
I have tried to use a consistent notation throughout the book, although at times this means departing from some of the conventions used in the corresponding research literature. Vectors are denoted by lower case bold Roman letters such as x, and all vectors are assumed to be column vectors. A superscript T denotes the transpose of a matrix or vector, so that xT will be a row vector. Uppercase bold roman letters, such as M, denote matrices. The notation (w1, . . . , wM ) denotes a row vector with M elements, while the corresponding column vector is written as w = (w1, . . . , wM )T.
The notation [a, b] is used to denote the closed interval from a to b, that is the interval including the values a and b themselves, while (a, b) denotes the corresponding open interval, that is the interval excluding a and b. Similarly, [a, b) denotes an interval that includes a but excludes b. For the most part, however, there will be little need to dwell on such reﬁnements as whether the end points of an interval are included or not.
The M × M identity matrix (also known as the unit matrix) is denoted IM , which will be abbreviated to I where there is no ambiguity about it dimensionality. It has elements Iij that equal 1 if i = j and 0 if i = j.
A functional is denoted f [y] where y(x) is some function. The concept of a functional is discussed in Appendix D.
The notation g(x) = O(f (x)) denotes that |f (x)/g(x)| is bounded as x → ∞. For instance if g(x) = 3x2 + 2, then g(x) = O(x2).
The expectation of a function f (x, y) with respect to a random variable x is denoted by Ex[f (x, y)]. In situations where there is no ambiguity as to which variable is being averaged over, this will be simpliﬁed by omitting the sufﬁx, for instance
xi

xii

MATHEMATICAL NOTATION

E[x]. If the distribution of x is conditioned on another variable z, then the corresponding conditional expectation will be written Ex[f (x)|z]. Similarly, the variance is denoted var[f (x)], and for vector variables the covariance is written cov[x, y]. We
shall also use cov[x] as a shorthand notation for cov[x, x]. The concepts of expecta-
tions and covariances are introduced in Section 1.2.2. If we have N values x1, . . . , xN of a D-dimensional vector x = (x1, . . . , xD)T,
we can combine the observations into a data matrix X in which the nth row of X corresponds to the row vector xTn. Thus the n, i element of X corresponds to the ith element of the nth observation xn. For the case of one-dimensional variables we shall denote such a matrix by x, which is a column vector whose nth element is xn. Note that x (which has dimensionality N ) uses a different typeface to distinguish it
from x (which has dimensionality D).

Contents

Preface

vii

Mathematical notation

xi

Contents

xiii

1 Introduction

1

1.1 Example: Polynomial Curve Fitting . . . . . . . . . . . . . . . . . 4

1.2 Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 12

1.2.1 Probability densities . . . . . . . . . . . . . . . . . . . . . 17

1.2.2 Expectations and covariances . . . . . . . . . . . . . . . . 19

1.2.3 Bayesian probabilities . . . . . . . . . . . . . . . . . . . . 21

1.2.4 The Gaussian distribution . . . . . . . . . . . . . . . . . . 24

1.2.5 Curve ﬁtting re-visited . . . . . . . . . . . . . . . . . . . . 28

1.2.6 Bayesian curve ﬁtting . . . . . . . . . . . . . . . . . . . . 30

1.3 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

1.4 The Curse of Dimensionality . . . . . . . . . . . . . . . . . . . . . 33

1.5 Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

1.5.1 Minimizing the misclassiﬁcation rate . . . . . . . . . . . . 39

1.5.2 Minimizing the expected loss . . . . . . . . . . . . . . . . 41

1.5.3 The reject option . . . . . . . . . . . . . . . . . . . . . . . 42

1.5.4 Inference and decision . . . . . . . . . . . . . . . . . . . . 42

1.5.5 Loss functions for regression . . . . . . . . . . . . . . . . . 46

1.6 Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 48

1.6.1 Relative entropy and mutual information . . . . . . . . . . 55

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

xiii

xiv

CONTENTS

2 Probability Distributions

67

2.1 Binary Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

2.1.1 The beta distribution . . . . . . . . . . . . . . . . . . . . . 71

2.2 Multinomial Variables . . . . . . . . . . . . . . . . . . . . . . . . 74

2.2.1 The Dirichlet distribution . . . . . . . . . . . . . . . . . . . 76

2.3 The Gaussian Distribution . . . . . . . . . . . . . . . . . . . . . . 78

2.3.1 Conditional Gaussian distributions . . . . . . . . . . . . . . 85

2.3.2 Marginal Gaussian distributions . . . . . . . . . . . . . . . 88

2.3.3 Bayes’ theorem for Gaussian variables . . . . . . . . . . . . 90

2.3.4 Maximum likelihood for the Gaussian . . . . . . . . . . . . 93

2.3.5 Sequential estimation . . . . . . . . . . . . . . . . . . . . . 94

2.3.6 Bayesian inference for the Gaussian . . . . . . . . . . . . . 97

2.3.7 Student’s t-distribution . . . . . . . . . . . . . . . . . . . . 102

2.3.8 Periodic variables . . . . . . . . . . . . . . . . . . . . . . . 105

2.3.9 Mixtures of Gaussians . . . . . . . . . . . . . . . . . . . . 110

2.4 The Exponential Family . . . . . . . . . . . . . . . . . . . . . . . 113

2.4.1 Maximum likelihood and sufﬁcient statistics . . . . . . . . 116

2.4.2 Conjugate priors . . . . . . . . . . . . . . . . . . . . . . . 117

2.4.3 Noninformative priors . . . . . . . . . . . . . . . . . . . . 117

2.5 Nonparametric Methods . . . . . . . . . . . . . . . . . . . . . . . 120

2.5.1 Kernel density estimators . . . . . . . . . . . . . . . . . . . 122

2.5.2 Nearest-neighbour methods . . . . . . . . . . . . . . . . . 124

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127

3 Linear Models for Regression

137

3.1 Linear Basis Function Models . . . . . . . . . . . . . . . . . . . . 138

3.1.1 Maximum likelihood and least squares . . . . . . . . . . . . 140

3.1.2 Geometry of least squares . . . . . . . . . . . . . . . . . . 143

3.1.3 Sequential learning . . . . . . . . . . . . . . . . . . . . . . 143

3.1.4 Regularized least squares . . . . . . . . . . . . . . . . . . . 144

3.1.5 Multiple outputs . . . . . . . . . . . . . . . . . . . . . . . 146

3.2 The Bias-Variance Decomposition . . . . . . . . . . . . . . . . . . 147

3.3 Bayesian Linear Regression . . . . . . . . . . . . . . . . . . . . . 152

3.3.1 Parameter distribution . . . . . . . . . . . . . . . . . . . . 152

3.3.2 Predictive distribution . . . . . . . . . . . . . . . . . . . . 156

3.3.3 Equivalent kernel . . . . . . . . . . . . . . . . . . . . . . . 159

3.4 Bayesian Model Comparison . . . . . . . . . . . . . . . . . . . . . 161

3.5 The Evidence Approximation . . . . . . . . . . . . . . . . . . . . 165

3.5.1 Evaluation of the evidence function . . . . . . . . . . . . . 166

3.5.2 Maximizing the evidence function . . . . . . . . . . . . . . 168

3.5.3 Effective number of parameters . . . . . . . . . . . . . . . 170

3.6 Limitations of Fixed Basis Functions . . . . . . . . . . . . . . . . 172

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

CONTENTS

xv

4 Linear Models for Classiﬁcation

179

4.1 Discriminant Functions . . . . . . . . . . . . . . . . . . . . . . . . 181

4.1.1 Two classes . . . . . . . . . . . . . . . . . . . . . . . . . . 181

4.1.2 Multiple classes . . . . . . . . . . . . . . . . . . . . . . . . 182

4.1.3 Least squares for classiﬁcation . . . . . . . . . . . . . . . . 184

4.1.4 Fisher’s linear discriminant . . . . . . . . . . . . . . . . . . 186

4.1.5 Relation to least squares . . . . . . . . . . . . . . . . . . . 189

4.1.6 Fisher’s discriminant for multiple classes . . . . . . . . . . 191

4.1.7 The perceptron algorithm . . . . . . . . . . . . . . . . . . . 192

4.2 Probabilistic Generative Models . . . . . . . . . . . . . . . . . . . 196

4.2.1 Continuous inputs . . . . . . . . . . . . . . . . . . . . . . 198

4.2.2 Maximum likelihood solution . . . . . . . . . . . . . . . . 200

4.2.3 Discrete features . . . . . . . . . . . . . . . . . . . . . . . 202

4.2.4 Exponential family . . . . . . . . . . . . . . . . . . . . . . 202

4.3 Probabilistic Discriminative Models . . . . . . . . . . . . . . . . . 203

4.3.1 Fixed basis functions . . . . . . . . . . . . . . . . . . . . . 204

4.3.2 Logistic regression . . . . . . . . . . . . . . . . . . . . . . 205

4.3.3 Iterative reweighted least squares . . . . . . . . . . . . . . 207

4.3.4 Multiclass logistic regression . . . . . . . . . . . . . . . . . 209

4.3.5 Probit regression . . . . . . . . . . . . . . . . . . . . . . . 210

4.3.6 Canonical link functions . . . . . . . . . . . . . . . . . . . 212

4.4 The Laplace Approximation . . . . . . . . . . . . . . . . . . . . . 213

4.4.1 Model comparison and BIC . . . . . . . . . . . . . . . . . 216

4.5 Bayesian Logistic Regression . . . . . . . . . . . . . . . . . . . . 217

4.5.1 Laplace approximation . . . . . . . . . . . . . . . . . . . . 217

4.5.2 Predictive distribution . . . . . . . . . . . . . . . . . . . . 218

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220

5 Neural Networks

225

5.1 Feed-forward Network Functions . . . . . . . . . . . . . . . . . . 227

5.1.1 Weight-space symmetries . . . . . . . . . . . . . . . . . . 231

5.2 Network Training . . . . . . . . . . . . . . . . . . . . . . . . . . . 232

5.2.1 Parameter optimization . . . . . . . . . . . . . . . . . . . . 236

5.2.2 Local quadratic approximation . . . . . . . . . . . . . . . . 237

5.2.3 Use of gradient information . . . . . . . . . . . . . . . . . 239

5.2.4 Gradient descent optimization . . . . . . . . . . . . . . . . 240

5.3 Error Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . 241

5.3.1 Evaluation of error-function derivatives . . . . . . . . . . . 242

5.3.2 A simple example . . . . . . . . . . . . . . . . . . . . . . 245

5.3.3 Efﬁciency of backpropagation . . . . . . . . . . . . . . . . 246

5.3.4 The Jacobian matrix . . . . . . . . . . . . . . . . . . . . . 247

5.4 The Hessian Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 249

5.4.1 Diagonal approximation . . . . . . . . . . . . . . . . . . . 250

5.4.2 Outer product approximation . . . . . . . . . . . . . . . . . 251

5.4.3 Inverse Hessian . . . . . . . . . . . . . . . . . . . . . . . . 252

xvi

CONTENTS

5.4.4 Finite differences . . . . . . . . . . . . . . . . . . . . . . . 252 5.4.5 Exact evaluation of the Hessian . . . . . . . . . . . . . . . 253 5.4.6 Fast multiplication by the Hessian . . . . . . . . . . . . . . 254 5.5 Regularization in Neural Networks . . . . . . . . . . . . . . . . . 256 5.5.1 Consistent Gaussian priors . . . . . . . . . . . . . . . . . . 257 5.5.2 Early stopping . . . . . . . . . . . . . . . . . . . . . . . . 259 5.5.3 Invariances . . . . . . . . . . . . . . . . . . . . . . . . . . 261 5.5.4 Tangent propagation . . . . . . . . . . . . . . . . . . . . . 263 5.5.5 Training with transformed data . . . . . . . . . . . . . . . . 265 5.5.6 Convolutional networks . . . . . . . . . . . . . . . . . . . 267 5.5.7 Soft weight sharing . . . . . . . . . . . . . . . . . . . . . . 269 5.6 Mixture Density Networks . . . . . . . . . . . . . . . . . . . . . . 272 5.7 Bayesian Neural Networks . . . . . . . . . . . . . . . . . . . . . . 277 5.7.1 Posterior parameter distribution . . . . . . . . . . . . . . . 278 5.7.2 Hyperparameter optimization . . . . . . . . . . . . . . . . 280 5.7.3 Bayesian neural networks for classiﬁcation . . . . . . . . . 281 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284

6 Kernel Methods

291

6.1 Dual Representations . . . . . . . . . . . . . . . . . . . . . . . . . 293

6.2 Constructing Kernels . . . . . . . . . . . . . . . . . . . . . . . . . 294

6.3 Radial Basis Function Networks . . . . . . . . . . . . . . . . . . . 299

6.3.1 Nadaraya-Watson model . . . . . . . . . . . . . . . . . . . 301

6.4 Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . . . 303

6.4.1 Linear regression revisited . . . . . . . . . . . . . . . . . . 304

6.4.2 Gaussian processes for regression . . . . . . . . . . . . . . 306

6.4.3 Learning the hyperparameters . . . . . . . . . . . . . . . . 311

6.4.4 Automatic relevance determination . . . . . . . . . . . . . 312

6.4.5 Gaussian processes for classiﬁcation . . . . . . . . . . . . . 313

6.4.6 Laplace approximation . . . . . . . . . . . . . . . . . . . . 315

6.4.7 Connection to neural networks . . . . . . . . . . . . . . . . 319

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320

7 Sparse Kernel Machines

325

7.1 Maximum Margin Classiﬁers . . . . . . . . . . . . . . . . . . . . 326

7.1.1 Overlapping class distributions . . . . . . . . . . . . . . . . 331

7.1.2 Relation to logistic regression . . . . . . . . . . . . . . . . 336

7.1.3 Multiclass SVMs . . . . . . . . . . . . . . . . . . . . . . . 338

7.1.4 SVMs for regression . . . . . . . . . . . . . . . . . . . . . 339

7.1.5 Computational learning theory . . . . . . . . . . . . . . . . 344

7.2 Relevance Vector Machines . . . . . . . . . . . . . . . . . . . . . 345

7.2.1 RVM for regression . . . . . . . . . . . . . . . . . . . . . . 345

7.2.2 Analysis of sparsity . . . . . . . . . . . . . . . . . . . . . . 349

7.2.3 RVM for classiﬁcation . . . . . . . . . . . . . . . . . . . . 353

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357

CONTENTS

xvii

8 Graphical Models

359

8.1 Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 360

8.1.1 Example: Polynomial regression . . . . . . . . . . . . . . . 362

8.1.2 Generative models . . . . . . . . . . . . . . . . . . . . . . 365

8.1.3 Discrete variables . . . . . . . . . . . . . . . . . . . . . . . 366

8.1.4 Linear-Gaussian models . . . . . . . . . . . . . . . . . . . 370

8.2 Conditional Independence . . . . . . . . . . . . . . . . . . . . . . 372

8.2.1 Three example graphs . . . . . . . . . . . . . . . . . . . . 373

8.2.2 D-separation . . . . . . . . . . . . . . . . . . . . . . . . . 378

8.3 Markov Random Fields . . . . . . . . . . . . . . . . . . . . . . . 383

8.3.1 Conditional independence properties . . . . . . . . . . . . . 383

8.3.2 Factorization properties . . . . . . . . . . . . . . . . . . . 384

8.3.3 Illustration: Image de-noising . . . . . . . . . . . . . . . . 387

8.3.4 Relation to directed graphs . . . . . . . . . . . . . . . . . . 390

8.4 Inference in Graphical Models . . . . . . . . . . . . . . . . . . . . 393

8.4.1 Inference on a chain . . . . . . . . . . . . . . . . . . . . . 394

8.4.2 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398

8.4.3 Factor graphs . . . . . . . . . . . . . . . . . . . . . . . . . 399

8.4.4 The sum-product algorithm . . . . . . . . . . . . . . . . . . 402

8.4.5 The max-sum algorithm . . . . . . . . . . . . . . . . . . . 411

8.4.6 Exact inference in general graphs . . . . . . . . . . . . . . 416

8.4.7 Loopy belief propagation . . . . . . . . . . . . . . . . . . . 417

8.4.8 Learning the graph structure . . . . . . . . . . . . . . . . . 418

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418

9 Mixture Models and EM

423

9.1 K-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . 424

9.1.1 Image segmentation and compression . . . . . . . . . . . . 428

9.2 Mixtures of Gaussians . . . . . . . . . . . . . . . . . . . . . . . . 430

9.2.1 Maximum likelihood . . . . . . . . . . . . . . . . . . . . . 432

9.2.2 EM for Gaussian mixtures . . . . . . . . . . . . . . . . . . 435

9.3 An Alternative View of EM . . . . . . . . . . . . . . . . . . . . . 439

9.3.1 Gaussian mixtures revisited . . . . . . . . . . . . . . . . . 441

9.3.2 Relation to K-means . . . . . . . . . . . . . . . . . . . . . 443

9.3.3 Mixtures of Bernoulli distributions . . . . . . . . . . . . . . 444

9.3.4 EM for Bayesian linear regression . . . . . . . . . . . . . . 448

9.4 The EM Algorithm in General . . . . . . . . . . . . . . . . . . . . 450

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

10 Approximate Inference

461

10.1 Variational Inference . . . . . . . . . . . . . . . . . . . . . . . . . 462

10.1.1 Factorized distributions . . . . . . . . . . . . . . . . . . . . 464

10.1.2 Properties of factorized approximations . . . . . . . . . . . 466

10.1.3 Example: The univariate Gaussian . . . . . . . . . . . . . . 470

10.1.4 Model comparison . . . . . . . . . . . . . . . . . . . . . . 473

10.2 Illustration: Variational Mixture of Gaussians . . . . . . . . . . . . 474

xviii

CONTENTS

10.2.1 Variational distribution . . . . . . . . . . . . . . . . . . . . 475 10.2.2 Variational lower bound . . . . . . . . . . . . . . . . . . . 481 10.2.3 Predictive density . . . . . . . . . . . . . . . . . . . . . . . 482 10.2.4 Determining the number of components . . . . . . . . . . . 483 10.2.5 Induced factorizations . . . . . . . . . . . . . . . . . . . . 485 10.3 Variational Linear Regression . . . . . . . . . . . . . . . . . . . . 486 10.3.1 Variational distribution . . . . . . . . . . . . . . . . . . . . 486 10.3.2 Predictive distribution . . . . . . . . . . . . . . . . . . . . 488 10.3.3 Lower bound . . . . . . . . . . . . . . . . . . . . . . . . . 489 10.4 Exponential Family Distributions . . . . . . . . . . . . . . . . . . 490 10.4.1 Variational message passing . . . . . . . . . . . . . . . . . 491 10.5 Local Variational Methods . . . . . . . . . . . . . . . . . . . . . . 493 10.6 Variational Logistic Regression . . . . . . . . . . . . . . . . . . . 498 10.6.1 Variational posterior distribution . . . . . . . . . . . . . . . 498 10.6.2 Optimizing the variational parameters . . . . . . . . . . . . 500 10.6.3 Inference of hyperparameters . . . . . . . . . . . . . . . . 502 10.7 Expectation Propagation . . . . . . . . . . . . . . . . . . . . . . . 505 10.7.1 Example: The clutter problem . . . . . . . . . . . . . . . . 511 10.7.2 Expectation propagation on graphs . . . . . . . . . . . . . . 513 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517

11 Sampling Methods

523

11.1 Basic Sampling Algorithms . . . . . . . . . . . . . . . . . . . . . 526

11.1.1 Standard distributions . . . . . . . . . . . . . . . . . . . . 526

11.1.2 Rejection sampling . . . . . . . . . . . . . . . . . . . . . . 528

11.1.3 Adaptive rejection sampling . . . . . . . . . . . . . . . . . 530

11.1.4 Importance sampling . . . . . . . . . . . . . . . . . . . . . 532

11.1.5 Sampling-importance-resampling . . . . . . . . . . . . . . 534

11.1.6 Sampling and the EM algorithm . . . . . . . . . . . . . . . 536

11.2 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . 537

11.2.1 Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 539

11.2.2 The Metropolis-Hastings algorithm . . . . . . . . . . . . . 541

11.3 Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 542

11.4 Slice Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546

11.5 The Hybrid Monte Carlo Algorithm . . . . . . . . . . . . . . . . . 548

11.5.1 Dynamical systems . . . . . . . . . . . . . . . . . . . . . . 548

11.5.2 Hybrid Monte Carlo . . . . . . . . . . . . . . . . . . . . . 552

11.6 Estimating the Partition Function . . . . . . . . . . . . . . . . . . 554

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556

12 Continuous Latent Variables

559

12.1 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . 561

12.1.1 Maximum variance formulation . . . . . . . . . . . . . . . 561

12.1.2 Minimum-error formulation . . . . . . . . . . . . . . . . . 563

12.1.3 Applications of PCA . . . . . . . . . . . . . . . . . . . . . 565

12.1.4 PCA for high-dimensional data . . . . . . . . . . . . . . . 569

CONTENTS

xix

12.2 Probabilistic PCA . . . . . . . . . . . . . . . . . . . . . . . . . . 570 12.2.1 Maximum likelihood PCA . . . . . . . . . . . . . . . . . . 574 12.2.2 EM algorithm for PCA . . . . . . . . . . . . . . . . . . . . 577 12.2.3 Bayesian PCA . . . . . . . . . . . . . . . . . . . . . . . . 580 12.2.4 Factor analysis . . . . . . . . . . . . . . . . . . . . . . . . 583
12.3 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586 12.4 Nonlinear Latent Variable Models . . . . . . . . . . . . . . . . . . 591
12.4.1 Independent component analysis . . . . . . . . . . . . . . . 591 12.4.2 Autoassociative neural networks . . . . . . . . . . . . . . . 592 12.4.3 Modelling nonlinear manifolds . . . . . . . . . . . . . . . . 595 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599

13 Sequential Data

605

13.1 Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607

13.2 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . 610

13.2.1 Maximum likelihood for the HMM . . . . . . . . . . . . . 615

13.2.2 The forward-backward algorithm . . . . . . . . . . . . . . 618

13.2.3 The sum-product algorithm for the HMM . . . . . . . . . . 625

13.2.4 Scaling factors . . . . . . . . . . . . . . . . . . . . . . . . 627

13.2.5 The Viterbi algorithm . . . . . . . . . . . . . . . . . . . . . 629

13.2.6 Extensions of the hidden Markov model . . . . . . . . . . . 631

13.3 Linear Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . 635

13.3.1 Inference in LDS . . . . . . . . . . . . . . . . . . . . . . . 638

13.3.2 Learning in LDS . . . . . . . . . . . . . . . . . . . . . . . 642

13.3.3 Extensions of LDS . . . . . . . . . . . . . . . . . . . . . . 644

13.3.4 Particle ﬁlters . . . . . . . . . . . . . . . . . . . . . . . . . 645

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646

14 Combining Models

653

14.1 Bayesian Model Averaging . . . . . . . . . . . . . . . . . . . . . . 654

14.2 Committees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655

14.3 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657

14.3.1 Minimizing exponential error . . . . . . . . . . . . . . . . 659

14.3.2 Error functions for boosting . . . . . . . . . . . . . . . . . 661

14.4 Tree-based Models . . . . . . . . . . . . . . . . . . . . . . . . . . 663

14.5 Conditional Mixture Models . . . . . . . . . . . . . . . . . . . . . 666

14.5.1 Mixtures of linear regression models . . . . . . . . . . . . . 667

14.5.2 Mixtures of logistic models . . . . . . . . . . . . . . . . . 670

14.5.3 Mixtures of experts . . . . . . . . . . . . . . . . . . . . . . 672

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 674

Appendix A Data Sets

677

Appendix B Probability Distributions

685

Appendix C Properties of Matrices

695

xx

CONTENTS

Appendix D Calculus of Variations

703

Appendix E Lagrange Multipliers

707

References

711

Index

729

1
Introduction
The problem of searching for patterns in data is a fundamental one and has a long and successful history. For instance, the extensive astronomical observations of Tycho Brahe in the 16th century allowed Johannes Kepler to discover the empirical laws of planetary motion, which in turn provided a springboard for the development of classical mechanics. Similarly, the discovery of regularities in atomic spectra played a key role in the development and veriﬁcation of quantum physics in the early twentieth century. The ﬁeld of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.
Consider the example of recognizing handwritten digits, illustrated in Figure 1.1. Each digit corresponds to a 28×28 pixel image and so can be represented by a vector x comprising 784 real numbers. The goal is to build a machine that will take such a vector x as input and that will produce the identity of the digit 0, . . . , 9 as the output. This is a nontrivial problem due to the wide variability of handwriting. It could be
1

2

1. INTRODUCTION

Figure 1.1 Examples of hand-written digits taken from US zip codes.

tackled using handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes, but in practice such an approach leads to a proliferation of rules and of exceptions to the rules and so on, and invariably gives poor results.
Far better results can be obtained by adopting a machine learning approach in which a large set of N digits {x1, . . . , xN } called a training set is used to tune the parameters of an adaptive model. The categories of the digits in the training set are known in advance, typically by inspecting them individually and hand-labelling them. We can express the category of a digit using target vector t, which represents the identity of the corresponding digit. Suitable techniques for representing categories in terms of vectors will be discussed later. Note that there is one such target vector t for each digit image x.
The result of running the machine learning algorithm can be expressed as a function y(x) which takes a new digit image x as input and that generates an output vector y, encoded in the same way as the target vectors. The precise form of the function y(x) is determined during the training phase, also known as the learning phase, on the basis of the training data. Once the model is trained it can then determine the identity of new digit images, which are said to comprise a test set. The ability to categorize correctly new examples that differ from those used for training is known as generalization. In practical applications, the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors, and so generalization is a central goal in pattern recognition.
For most practical applications, the original input variables are typically preprocessed to transform them into some new space of variables where, it is hoped, the pattern recognition problem will be easier to solve. For instance, in the digit recognition problem, the images of the digits are typically translated and scaled so that each digit is contained within a box of a ﬁxed size. This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes. This pre-processing stage is sometimes also called feature extraction. Note that new test data must be pre-processed using the same steps as the training data.
Pre-processing might also be performed in order to speed up computation. For example, if the goal is real-time face detection in a high-resolution video stream, the computer must handle huge numbers of pixels per second, and presenting these directly to a complex pattern recognition algorithm may be computationally infeasible. Instead, the aim is to ﬁnd useful features that are fast to compute, and yet that

1. INTRODUCTION

3

also preserve useful discriminatory information enabling faces to be distinguished from non-faces. These features are then used as the inputs to the pattern recognition algorithm. For instance, the average value of the image intensity over a rectangular subregion can be evaluated extremely efﬁciently (Viola and Jones, 2004), and a set of such features can prove very effective in fast face detection. Because the number of such features is smaller than the number of pixels, this kind of pre-processing represents a form of dimensionality reduction. Care must be taken during pre-processing because often information is discarded, and if this information is important to the solution of the problem then the overall accuracy of the system can suffer.
Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. Cases such as the digit recognition example, in which the aim is to assign each input vector to one of a ﬁnite number of discrete categories, are called classiﬁcation problems. If the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations of reactants, the temperature, and the pressure.
In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.
Finally, the technique of reinforcement learning (Sutton and Barto, 1998) is concerned with the problem of ﬁnding suitable actions to take in a given situation in order to maximize a reward. Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them by a process of trial and error. Typically there is a sequence of states and actions in which the learning algorithm is interacting with its environment. In many cases, the current action not only affects the immediate reward but also has an impact on the reward at all subsequent time steps. For example, by using appropriate reinforcement learning techniques a neural network can learn to play the game of backgammon to a high standard (Tesauro, 1994). Here the network must learn to take a board position as input, along with the result of a dice throw, and produce a strong move as the output. This is done by having the network play against a copy of itself for perhaps a million games. A major challenge is that a game of backgammon can involve dozens of moves, and yet it is only at the end of the game that the reward, in the form of victory, is achieved. The reward must then be attributed appropriately to all of the moves that led to it, even though some moves will have been good ones and others less so. This is an example of a credit assignment problem. A general feature of reinforcement learning is the trade-off between exploration, in which the system tries out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward. Too strong a focus on either exploration or exploitation will yield poor results. Reinforcement learning continues to be an active area of machine learning research. However, a

4

1. INTRODUCTION

Figure 1.2 Plot of a training data set of N =

10 points, shown as blue circles,

each comprising an observation of the input variable x along with

1

the corresponding target variable t

t. The green curve shows the

function sin(2πx) used to generate the data. Our goal is to pre- 0

dict the value of t for some new

value of x, without knowledge of

the green curve.

−1

0

x1

detailed treatment lies beyond the scope of this book. Although each of these tasks needs its own tools and techniques, many of the
key ideas that underpin them are common to all such problems. One of the main goals of this chapter is to introduce, in a relatively informal way, several of the most important of these concepts and to illustrate them using simple examples. Later in the book we shall see these same ideas re-emerge in the context of more sophisticated models that are applicable to real-world pattern recognition applications. This chapter also provides a self-contained introduction to three important tools that will be used throughout the book, namely probability theory, decision theory, and information theory. Although these might sound like daunting topics, they are in fact straightforward, and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications.

1.1. Example: Polynomial Curve Fitting
We begin by introducing a simple regression problem, which we shall use as a running example throughout this chapter to motivate a number of key concepts. Suppose we observe a real-valued input variable x and we wish to use this observation to predict the value of a real-valued target variable t. For the present purposes, it is instructive to consider an artiﬁcial example using synthetically generated data because we then know the precise process that generated the data for comparison against any learned model. The data for this example is generated from the function sin(2πx) with random noise included in the target values, as described in detail in Appendix A.
Now suppose that we are given a training set comprising N observations of x, written x ≡ (x1, . . . , xN )T, together with corresponding observations of the values of t, denoted t ≡ (t1, . . . , tN )T. Figure 1.2 shows a plot of a training set comprising N = 10 data points. The input data set x in Figure 1.2 was generated by choosing values of xn, for n = 1, . . . , N , spaced uniformly in range [0, 1], and the target data set t was obtained by ﬁrst computing the corresponding values of the function

1.1. Example: Polynomial Curve Fitting

5

sin(2πx) and then adding a small level of random noise having a Gaussian distribution (the Gaussian distribution is discussed in Section 1.2.4) to each such point in order to obtain the corresponding value tn. By generating data in this way, we are capturing a property of many real data sets, namely that they possess an underlying regularity, which we wish to learn, but that individual observations are corrupted by random noise. This noise might arise from intrinsically stochastic (i.e. random) processes such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved.
Our goal is to exploit this training set in order to make predictions of the value t of the target variable for some new value x of the input variable. As we shall see later, this involves implicitly trying to discover the underlying function sin(2πx). This is intrinsically a difﬁcult problem as we have to generalize from a ﬁnite data set. Furthermore the observed data are corrupted with noise, and so for a given x there is uncertainty as to the appropriate value for t. Probability theory, discussed in Section 1.2, provides a framework for expressing such uncertainty in a precise and quantitative manner, and decision theory, discussed in Section 1.5, allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria.
For the moment, however, we shall proceed rather informally and consider a simple approach based on curve ﬁtting. In particular, we shall ﬁt the data using a polynomial function of the form

M
y(x, w) = w0 + w1x + w2x2 + . . . + wM xM = wjxj
j=0

(1.1)

where M is the order of the polynomial, and xj denotes x raised to the power of j. The polynomial coefﬁcients w0, . . . , wM are collectively denoted by the vector w. Note that, although the polynomial function y(x, w) is a nonlinear function of x, it is a linear function of the coefﬁcients w. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called linear models and will be discussed extensively in Chapters 3 and 4.
The values of the coefﬁcients will be determined by ﬁtting the polynomial to the training data. This can be done by minimizing an error function that measures the misﬁt between the function y(x, w), for any given value of w, and the training set data points. One simple choice of error function, which is widely used, is given by the sum of the squares of the errors between the predictions y(xn, w) for each data point xn and the corresponding target values tn, so that we minimize

1 E(w) =
2

N

{y(xn, w) − tn}2

n=1

(1.2)

where the factor of 1/2 is included for later convenience. We shall discuss the motivation for this choice of error function later in this chapter. For the moment we simply note that it is a nonnegative quantity that would be zero if, and only if, the

6

1. INTRODUCTION

Figure 1.3 The error function (1.2) corre-
sponds to (one half of) the sum of t the squares of the displacements
(shown by the vertical green bars)
of each data point from the function y(x, w).

tn y(xn, w)

Exercise 1.1

xn

x

function y(x, w) were to pass exactly through each training data point. The geometrical interpretation of the sum-of-squares error function is illustrated in Figure 1.3.
We can solve the curve ﬁtting problem by choosing the value of w for which E(w) is as small as possible. Because the error function is a quadratic function of the coefﬁcients w, its derivatives with respect to the coefﬁcients will be linear in the elements of w, and so the minimization of the error function has a unique solution, denoted by w , which can be found in closed form. The resulting polynomial is given by the function y(x, w ).
There remains the problem of choosing the order M of the polynomial, and as we shall see this will turn out to be an example of an important concept called model comparison or model selection. In Figure 1.4, we show four examples of the results of ﬁtting polynomials having orders M = 0, 1, 3, and 9 to the data set shown in Figure 1.2.
We notice that the constant (M = 0) and ﬁrst order (M = 1) polynomials give rather poor ﬁts to the data and consequently rather poor representations of the function sin(2πx). The third order (M = 3) polynomial seems to give the best ﬁt to the function sin(2πx) of the examples shown in Figure 1.4. When we go to a much higher order polynomial (M = 9), we obtain an excellent ﬁt to the training data. In fact, the polynomial passes exactly through each data point and E(w ) = 0. However, the ﬁtted curve oscillates wildly and gives a very poor representation of the function sin(2πx). This latter behaviour is known as over-ﬁtting.
As we have noted earlier, the goal is to achieve good generalization by making accurate predictions for new data. We can obtain some quantitative insight into the dependence of the generalization performance on M by considering a separate test set comprising 100 data points generated using exactly the same procedure used to generate the training set points but with new choices for the random noise values included in the target values. For each choice of M , we can then evaluate the residual value of E(w ) given by (1.2) for the training data, and we can also evaluate E(w ) for the test data set. It is sometimes more convenient to use the root-mean-square

1.1. Example: Polynomial Curve Fitting

7

1

M =0

1

M =1

t

t

0

0

−1 0

−1

x1

0

x1

1

M =3

1

M =9

t

t

0

0

−1

−1

0

x1

0

x1

Figure 1.4 Plots of polynomials having various orders M , shown as red curves, ﬁtted to the data set shown in Figure 1.2.

(RMS) error deﬁned by

ERMS = 2E(w )/N

(1.3)

in which the division by N allows us to compare different sizes of data sets on an equal footing, and the square root ensures that ERMS is measured on the same scale (and in the same units) as the target variable t. Graphs of the training and test set RMS errors are shown, for various values of M , in Figure 1.5. The test set error is a measure of how well we are doing in predicting the values of t for new data observations of x. We note from Figure 1.5 that small values of M give relatively large values of the test set error, and this can be attributed to the fact that the corresponding polynomials are rather inﬂexible and are incapable of capturing the oscillations in the function sin(2πx). Values of M in the range 3 M 8 give small values for the test set error, and these also give reasonable representations of the generating function sin(2πx), as can be seen, for the case of M = 3, from Figure 1.4.

8

1. INTRODUCTION

Figure 1.5 Graphs of the root-mean-square

error, deﬁned by (1.3), evaluated

1

on the training set and on an inde-

pendent test set for various values

of M .

0.5

Training Test

ERMS

0

0

3M 6

9

For M = 9, the training set error goes to zero, as we might expect because this polynomial contains 10 degrees of freedom corresponding to the 10 coefﬁcients w0, . . . , w9, and so can be tuned exactly to the 10 data points in the training set. However, the test set error has become very large and, as we saw in Figure 1.4, the corresponding function y(x, w ) exhibits wild oscillations.
This may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases. The M = 9 polynomial is therefore capable of generating results at least as good as the M = 3 polynomial. Furthermore, we might suppose that the best predictor of new data would be the function sin(2πx) from which the data was generated (and we shall see later that this is indeed the case). We know that a power series expansion of the function sin(2πx) contains terms of all orders, so we might expect that results should improve monotonically as we increase M .
We can gain some insight into the problem by examining the values of the coefﬁcients w obtained from polynomials of various order, as shown in Table 1.1. We see that, as M increases, the magnitude of the coefﬁcients typically gets larger. In particular for the M = 9 polynomial, the coefﬁcients have become ﬁnely tuned to the data by developing large positive and negative values so that the correspond-

Table 1.1 Table of the coefﬁcients w for

M =0 M =1 M =6

M =9

polynomials of various order. Observe how the typical magnitude of the coefﬁcients in-

w0 w1

creases dramatically as the or- w2

0.19 0.82 0.31 -1.27 7.99 -25.43

0.35 232.37 -5321.83

der of the polynomial increases. w3

17.37 48568.31

w4

-231639.30

w5

640042.26

w6

-1061800.52

w7

1042400.18

w8

-557682.99

w9

125201.43

1.1. Example: Polynomial Curve Fitting

9

1

N = 15

1

N = 100

t

t

0

0

−1

−1

0

x1

0

x1

Figure 1.6 Plots of the solutions obtained by minimizing the sum-of-squares error function using the M = 9 polynomial for N = 15 data points (left plot) and N = 100 data points (right plot). We see that increasing the size of the data set reduces the over-ﬁtting problem.

Section 3.4

ing polynomial function matches each of the data points exactly, but between data points (particularly near the ends of the range) the function exhibits the large oscillations observed in Figure 1.4. Intuitively, what is happening is that the more ﬂexible polynomials with larger values of M are becoming increasingly tuned to the random noise on the target values.
It is also interesting to examine the behaviour of a given model as the size of the data set is varied, as shown in Figure 1.6. We see that, for a given model complexity, the over-ﬁtting problem become less severe as the size of the data set increases. Another way to say this is that the larger the data set, the more complex (in other words more ﬂexible) the model that we can afford to ﬁt to the data. One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model. However, as we shall see in Chapter 3, the number of parameters is not necessarily the most appropriate measure of model complexity.
Also, there is something rather unsatisfying about having to limit the number of parameters in a model according to the size of the available training set. It would seem more reasonable to choose the complexity of the model according to the complexity of the problem being solved. We shall see that the least squares approach to ﬁnding the model parameters represents a speciﬁc case of maximum likelihood (discussed in Section 1.2.5), and that the over-ﬁtting problem can be understood as a general property of maximum likelihood. By adopting a Bayesian approach, the over-ﬁtting problem can be avoided. We shall see that there is no difﬁculty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points. Indeed, in a Bayesian model the effective number of parameters adapts automatically to the size of the data set.
For the moment, however, it is instructive to continue with the current approach and to consider how in practice we can apply it to data sets of limited size where we

10

1. INTRODUCTION

1

ln λ = −18

1

ln λ = 0

t

t

0

0

−1

−1

0

x1

0

x1

Figure 1.7 Plots of M = 9 polynomials ﬁtted to the data set shown in Figure 1.2 using the regularized error function (1.4) for two values of the regularization parameter λ corresponding to ln λ = −18 and ln λ = 0. The case of no regularizer, i.e., λ = 0, corresponding to ln λ = −∞, is shown at the bottom right of Figure 1.4.

Exercise 1.2

may wish to use relatively complex and ﬂexible models. One technique that is often

used to control the over-ﬁtting phenomenon in such cases is that of regularization,

which involves adding a penalty term to the error function (1.2) in order to discourage

the coefﬁcients from reaching large values. The simplest such penalty term takes the

form of a sum of squares of all of the coefﬁcients, leading to a modiﬁed error function

of the form

1 E(w) =
2

N

{y(xn,

w)

−

tn}2

+

λ 2

w

2

n=1

(1.4)

where w 2 ≡ wTw = w02 + w12 + . . . + wM 2 , and the coefﬁcient λ governs the relative importance of the regularization term compared with the sum-of-squares error term. Note that often the coefﬁcient w0 is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable (Hastie et al., 2001), or it may be included but with its own regularization coefﬁcient (we shall discuss this topic in more detail in Section 5.5.1). Again, the error function in (1.4) can be minimized exactly in closed form. Techniques such as this are known in the statistics literature as shrinkage methods because they reduce the value of the coefﬁcients. The particular case of a quadratic regularizer is called ridge regression (Hoerl and Kennard, 1970). In the context of neural networks, this approach is known as weight decay.
Figure 1.7 shows the results of ﬁtting the polynomial of order M = 9 to the same data set as before but now using the regularized error function given by (1.4). We see that, for a value of ln λ = −18, the over-ﬁtting has been suppressed and we now obtain a much closer representation of the underlying function sin(2πx). If, however, we use too large a value for λ then we again obtain a poor ﬁt, as shown in Figure 1.7 for ln λ = 0. The corresponding coefﬁcients from the ﬁtted polynomials are given in Table 1.2, showing that regularization has the desired effect of reducing

1.1. Example: Polynomial Curve Fitting

11

Table 1.2 Table of the coefﬁcients w for M =

ln λ = −∞ ln λ = −18 ln λ = 0

9 polynomials with various values for the regularization parameter λ. Note that ln λ = −∞ corresponds to a

w0 w1

0.35 232.37

0.35

0.13

4.74 -0.05

model with no regularization, i.e., to w2

-5321.83

-0.77 -0.06

the graph at the bottom right in Fig- w3

48568.31

-31.97 -0.05

ure 1.4. We see that, as the value of λ increases, the typical magnitude of the coefﬁcients gets smaller.

w4 w5 w6

-231639.30 640042.26 -1061800.52

-3.89 55.28 41.32

-0.03 -0.02 -0.01

w7 1042400.18

-45.95 -0.00

w8 -557682.99

-91.53

0.00

w9 125201.43

72.68

0.01

Section 1.3

the magnitude of the coefﬁcients. The impact of the regularization term on the generalization error can be seen by
plotting the value of the RMS error (1.3) for both training and test sets against ln λ, as shown in Figure 1.8. We see that in effect λ now controls the effective complexity of the model and hence determines the degree of over-ﬁtting.
The issue of model complexity is an important one and will be discussed at length in Section 1.3. Here we simply note that, if we were trying to solve a practical application using this approach of minimizing an error function, we would have to ﬁnd a way to determine a suitable value for the model complexity. The results above suggest a simple way of achieving this, namely by taking the available data and partitioning it into a training set, used to determine the coefﬁcients w, and a separate validation set, also called a hold-out set, used to optimize the model complexity (either M or λ). In many cases, however, this will prove to be too wasteful of valuable training data, and we have to seek more sophisticated approaches.
So far our discussion of polynomial curve ﬁtting has appealed largely to intuition. We now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory. As well as providing the foundation for nearly all of the subsequent developments in this book, it will also

Figure 1.8 Graph of the root-mean-square er-

ror (1.3) versus ln λ for the M = 9

1

polynomial.

Training Test

0.5

ERMS

0

−35

−30 ln λ −25

−20

12

1. INTRODUCTION

give us some important insights into the concepts we have introduced in the context of polynomial curve ﬁtting and will allow us to extend these to more complex situations.

1.2. Probability Theory
A key concept in the ﬁeld of pattern recognition is that of uncertainty. It arises both through noise on measurements, as well as through the ﬁnite size of data sets. Probability theory provides a consistent framework for the quantiﬁcation and manipulation of uncertainty and forms one of the central foundations for pattern recognition. When combined with decision theory, discussed in Section 1.5, it allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous.
We will introduce the basic concepts of probability theory by considering a simple example. Imagine we have two boxes, one red and one blue, and in the red box we have 2 apples and 6 oranges, and in the blue box we have 3 apples and 1 orange. This is illustrated in Figure 1.9. Now suppose we randomly pick one of the boxes and from that box we randomly select an item of fruit, and having observed which sort of fruit it is we replace it in the box from which it came. We could imagine repeating this process many times. Let us suppose that in so doing we pick the red box 40% of the time and we pick the blue box 60% of the time, and that when we remove an item of fruit from a box we are equally likely to select any of the pieces of fruit in the box.
In this example, the identity of the box that will be chosen is a random variable, which we shall denote by B. This random variable can take one of two possible values, namely r (corresponding to the red box) or b (corresponding to the blue box). Similarly, the identity of the fruit is also a random variable and will be denoted by F . It can take either of the values a (for apple) or o (for orange).
To begin with, we shall deﬁne the probability of an event to be the fraction of times that event occurs out of the total number of trials, in the limit that the total number of trials goes to inﬁnity. Thus the probability of selecting the red box is 4/10

Figure 1.9

We use a simple example of two coloured boxes each containing fruit (apples shown in green and oranges shown in orange) to introduce the basic ideas of probability.

1.2. Probability Theory

13

Figure 1.10 We can derive the sum and product rules of probability by
considering two random variables, X, which takes the values {xi} where i = 1, . . . , M , and Y , which takes the values {yj} where j = 1, . . . , L. In this illustration we have M = 5 and L = 3. If we consider a total number N of instances of these variables, then we denote the number of instances where X = xi and Y = yj by nij, which is the number of yj points in the corresponding cell of the array. The number of points in
column i, corresponding to X = xi, is denoted by ci, and the number of points in row j, corresponding to Y = yj, is denoted by rj.

}

ci

nij

} rj

xi

and the probability of selecting the blue box is 6/10. We write these probabilities as p(B = r) = 4/10 and p(B = b) = 6/10. Note that, by deﬁnition, probabilities must lie in the interval [0, 1]. Also, if the events are mutually exclusive and if they
include all possible outcomes (for instance, in this example the box must be either
red or blue), then we see that the probabilities for those events must sum to one.
We can now ask questions such as: “what is the overall probability that the se-
lection procedure will pick an apple?”, or “given that we have chosen an orange,
what is the probability that the box we chose was the blue one?”. We can answer
questions such as these, and indeed much more complex questions associated with
problems in pattern recognition, once we have equipped ourselves with the two el-
ementary rules of probability, known as the sum rule and the product rule. Having
obtained these rules, we shall then return to our boxes of fruit example.
In order to derive the rules of probability, consider the slightly more general example shown in Figure 1.10 involving two random variables X and Y (which could
for instance be the Box and Fruit variables considered above). We shall suppose that X can take any of the values xi where i = 1, . . . , M , and Y can take the values yj where j = 1, . . . , L. Consider a total of N trials in which we sample both of the variables X and Y , and let the number of such trials in which X = xi and Y = yj be nij. Also, let the number of trials in which X takes the value xi (irrespective of the value that Y takes) be denoted by ci, and similarly let the number of trials in which Y takes the value yj be denoted by rj.
The probability that X will take the value xi and Y will take the value yj is written p(X = xi, Y = yj) and is called the joint probability of X = xi and Y = yj. It is given by the number of points falling in the cell i,j as a fraction of the total number of points, and hence

p(X

= xi, Y

= yj) =

nij . N

(1.5)

Here we are implicitly considering the limit N → ∞. Similarly, the probability that
X takes the value xi irrespective of the value of Y is written as p(X = xi) and is given by the fraction of the total number of points that fall in column i, so that

p(X

=

xi)

=

ci N

.

(1.6)

Because the number of instances in column i in Figure 1.10 is just the sum of the number of instances in each cell of that column, we have ci = j nij and therefore,

14

1. INTRODUCTION

from (1.5) and (1.6), we have

L
p(X = xi) = p(X = xi, Y = yj)
j=1

(1.7)

which is the sum rule of probability. Note that p(X = xi) is sometimes called the marginal probability, because it is obtained by marginalizing, or summing out, the
other variables (in this case Y ).
If we consider only those instances for which X = xi, then the fraction of such instances for which Y = yj is written p(Y = yj|X = xi) and is called the conditional probability of Y = yj given X = xi. It is obtained by ﬁnding the fraction of those points in column i that fall in cell i,j and hence is given by

p(Y

= yj|X

= xi) =

nij . ci

(1.8)

From (1.5), (1.6), and (1.8), we can then derive the following relationship

p(X = xi, Y = yj)

=

nij = nij · ci N ci N

= p(Y = yj|X = xi)p(X = xi)

(1.9)

which is the product rule of probability. So far we have been quite careful to make a distinction between a random vari-
able, such as the box B in the fruit example, and the values that the random variable can take, for example r if the box were the red one. Thus the probability that B takes the value r is denoted p(B = r). Although this helps to avoid ambiguity, it leads to a rather cumbersome notation, and in many cases there will be no need for such pedantry. Instead, we may simply write p(B) to denote a distribution over the random variable B, or p(r) to denote the distribution evaluated for the particular value r, provided that the interpretation is clear from the context.
With this more compact notation, we can write the two fundamental rules of probability theory in the following form.

The Rules of Probability

sum rule product rule

p(X) = p(X, Y )
Y
p(X, Y ) = p(Y |X)p(X).

(1.10) (1.11)

Here p(X, Y ) is a joint probability and is verbalized as “the probability of X and Y ”. Similarly, the quantity p(Y |X) is a conditional probability and is verbalized as “the probability of Y given X”, whereas the quantity p(X) is a marginal probability

1.2. Probability Theory

15

and is simply “the probability of X”. These two simple rules form the basis for all of the probabilistic machinery that we use throughout this book.
From the product rule, together with the symmetry property p(X, Y ) = p(Y, X), we immediately obtain the following relationship between conditional probabilities

p(Y

|X )

=

p(X|Y )p(Y p(X )

)

(1.12)

which is called Bayes’ theorem and which plays a central role in pattern recognition and machine learning. Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator

p(X) = p(X|Y )p(Y ).
Y

(1.13)

We can view the denominator in Bayes’ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of (1.12) over all values of Y equals one.
In Figure 1.11, we show a simple example involving a joint distribution over two variables to illustrate the concept of marginal and conditional distributions. Here a ﬁnite sample of N = 60 data points has been drawn from the joint distribution and is shown in the top left. In the top right is a histogram of the fractions of data points having each of the two values of Y . From the deﬁnition of probability, these fractions would equal the corresponding probabilities p(Y ) in the limit N → ∞. We can view the histogram as a simple way to model a probability distribution given only a ﬁnite number of points drawn from that distribution. Modelling distributions from data lies at the heart of statistical pattern recognition and will be explored in great detail in this book. The remaining two plots in Figure 1.11 show the corresponding histogram estimates of p(X) and p(X|Y = 1).
Let us now return to our example involving boxes of fruit. For the moment, we shall once again be explicit about distinguishing between the random variables and their instantiations. We have seen that the probabilities of selecting either the red or the blue boxes are given by

p(B = r) = 4/10 p(B = b) = 6/10

(1.14) (1.15)

respectively. Note that these satisfy p(B = r) + p(B = b) = 1. Now suppose that we pick a box at random, and it turns out to be the blue box.
Then the probability of selecting an apple is just the fraction of apples in the blue box which is 3/4, and so p(F = a|B = b) = 3/4. In fact, we can write out all four conditional probabilities for the type of fruit, given the selected box

p(F = a|B = r) = 1/4 p(F = o|B = r) = 3/4 p(F = a|B = b) = 3/4 p(F = o|B = b) = 1/4.

(1.16) (1.17) (1.18) (1.19)

16

1. INTRODUCTION

p(X, Y )

Y =2

Y =1

X p(X )

p(Y ) p(X|Y = 1)

X

X

Figure 1.11 An illustration of a distribution over two variables, X, which takes 9 possible values, and Y , which takes two possible values. The top left ﬁgure shows a sample of 60 points drawn from a joint probability distribution over these variables. The remaining ﬁgures show histogram estimates of the marginal distributions p(X) and p(Y ), as well as the conditional distribution p(X|Y = 1) corresponding to the bottom row in the top left ﬁgure.

Again, note that these probabilities are normalized so that

p(F = a|B = r) + p(F = o|B = r) = 1

(1.20)

and similarly

p(F = a|B = b) + p(F = o|B = b) = 1.

(1.21)

We can now use the sum and product rules of probability to evaluate the overall probability of choosing an apple

p(F = a) = p(F = a|B = r)p(B = r) + p(F = a|B = b)p(B = b)

=

1×

4

+3×

6

11 =

4 10 4 10 20

(1.22)

from which it follows, using the sum rule, that p(F = o) = 1 − 11/20 = 9/20.

1.2. Probability Theory

17

Suppose instead we are told that a piece of fruit has been selected and it is an orange, and we would like to know which box it came from. This requires that we evaluate the probability distribution over boxes conditioned on the identity of the fruit, whereas the probabilities in (1.16)–(1.19) give the probability distribution over the fruit conditioned on the identity of the box. We can solve the problem of reversing the conditional probability by using Bayes’ theorem to give

p(B

=

r|F

=

o)

=

p(F

=

o|B = r)p(B p(F = o)

=

r)

=

3 4

×

4 10

×

20 9

=

2 .
3

(1.23)

From the sum rule, it then follows that p(B = b|F = o) = 1 − 2/3 = 1/3. We can provide an important interpretation of Bayes’ theorem as follows. If
we had been asked which box had been chosen before being told the identity of the selected item of fruit, then the most complete information we have available is provided by the probability p(B). We call this the prior probability because it is the probability available before we observe the identity of the fruit. Once we are told that the fruit is an orange, we can then use Bayes’ theorem to compute the probability p(B|F ), which we shall call the posterior probability because it is the probability obtained after we have observed F . Note that in this example, the prior probability of selecting the red box was 4/10, so that we were more likely to select the blue box than the red one. However, once we have observed that the piece of selected fruit is an orange, we ﬁnd that the posterior probability of the red box is now 2/3, so that it is now more likely that the box we selected was in fact the red one. This result accords with our intuition, as the proportion of oranges is much higher in the red box than it is in the blue box, and so the observation that the fruit was an orange provides signiﬁcant evidence favouring the red box. In fact, the evidence is sufﬁciently strong that it outweighs the prior and makes it more likely that the red box was chosen rather than the blue one.
Finally, we note that if the joint distribution of two variables factorizes into the product of the marginals, so that p(X, Y ) = p(X)p(Y ), then X and Y are said to be independent. From the product rule, we see that p(Y |X) = p(Y ), and so the conditional distribution of Y given X is indeed independent of the value of X. For instance, in our boxes of fruit example, if each box contained the same fraction of apples and oranges, then p(F |B) = P (F ), so that the probability of selecting, say, an apple is independent of which box is chosen.

1.2.1 Probability densities
As well as considering probabilities deﬁned over discrete sets of events, we also wish to consider probabilities with respect to continuous variables. We shall limit ourselves to a relatively informal discussion. If the probability of a real-valued variable x falling in the interval (x, x + δx) is given by p(x)δx for δx → 0, then p(x) is called the probability density over x. This is illustrated in Figure 1.12. The probability that x will lie in an interval (a, b) is then given by

b
p(x ∈ (a, b)) = p(x) dx.
a

(1.24)

18

1. INTRODUCTION

Figure 1.12

The concept of probability for
discrete variables can be ex-
tended to that of a probability density p(x) over a continuous variable x and is such that the probability of x lying in the interval (x, x + δx) is given by p(x)δx for δx → 0. The probability density can be expressed as the
derivative of a cumulative distribution function P (x).

p(x)

P (x)

Exercise 1.4

δx

x

Because probabilities are nonnegative, and because the value of x must lie somewhere on the real axis, the probability density p(x) must satisfy the two conditions

p(x)

0

∞

p(x) dx = 1.

−∞

(1.25) (1.26)

Under a nonlinear change of variable, a probability density transforms differently
from a simple function, due to the Jacobian factor. For instance, if we consider
a change of variables x = g(y), then a function f (x) becomes f (y) = f (g(y)). Now consider a probability density px(x) that corresponds to a density py(y) with respect to the new variable y, where the sufﬁces denote the fact that px(x) and py(y) are different densities. Observations falling in the range (x, x + δx) will, for small values of δx, be transformed into the range (y, y + δy) where px(x)δx py(y)δy, and hence

dx py(y) = px(x) dy
= px(g(y)) |g (y)| .

(1.27)

One consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable.
The probability that x lies in the interval (−∞, z) is given by the cumulative distribution function deﬁned by

z
P (z) = p(x) dx
−∞

(1.28)

which satisﬁes P (x) = p(x), as shown in Figure 1.12.
If we have several continuous variables x1, . . . , xD, denoted collectively by the vector x, then we can deﬁne a joint probability density p(x) = p(x1, . . . , xD) such

1.2. Probability Theory

19

that the probability of x falling in an inﬁnitesimal volume δx containing the point x is given by p(x)δx. This multivariate probability density must satisfy

p(x)

0

(1.29)

p(x) dx = 1

(1.30)

in which the integral is taken over the whole of x space. We can also consider joint probability distributions over a combination of discrete and continuous variables.
Note that if x is a discrete variable, then p(x) is sometimes called a probability mass function because it can be regarded as a set of ‘probability masses’ concentrated at the allowed values of x.
The sum and product rules of probability, as well as Bayes’ theorem, apply equally to the case of probability densities, or to combinations of discrete and continuous variables. For instance, if x and y are two real variables, then the sum and product rules take the form

p(x) = p(x, y) dy

(1.31)

p(x, y) = p(y|x)p(x).

(1.32)

A formal justiﬁcation of the sum and product rules for continuous variables (Feller, 1966) requires a branch of mathematics called measure theory and lies outside the scope of this book. Its validity can be seen informally, however, by dividing each real variable into intervals of width ∆ and considering the discrete probability distribution over these intervals. Taking the limit ∆ → 0 then turns sums into integrals and gives the desired result.

1.2.2 Expectations and covariances
One of the most important operations involving probabilities is that of ﬁnding weighted averages of functions. The average value of some function f (x) under a probability distribution p(x) is called the expectation of f (x) and will be denoted by E[f ]. For a discrete distribution, it is given by

E[f ] = p(x)f (x)
x

(1.33)

so that the average is weighted by the relative probabilities of the different values of x. In the case of continuous variables, expectations are expressed in terms of an integration with respect to the corresponding probability density

E[f ] = p(x)f (x) dx.

(1.34)

In either case, if we are given a ﬁnite number N of points drawn from the probability distribution or probability density, then the expectation can be approximated as a

20

1. INTRODUCTION

Exercise 1.5 Exercise 1.6

ﬁnite sum over these points

E[f ]

1N N f (xn).
n=1

(1.35)

We shall make extensive use of this result when we discuss sampling methods in

Chapter 11. The approximation in (1.35) becomes exact in the limit N → ∞.

Sometimes we will be considering expectations of functions of several variables,

in which case we can use a subscript to indicate which variable is being averaged

over, so that for instance

Ex[f (x, y)]

(1.36)

denotes the average of the function f (x, y) with respect to the distribution of x. Note

that Ex[f (x, y)] will be a function of y. We can also consider a conditional expectation with respect to a conditional

distribution, so that

Ex[f |y] = p(x|y)f (x)

(1.37)

x

with an analogous deﬁnition for continuous variables. The variance of f (x) is deﬁned by

var[f ] = E (f (x) − E[f (x)])2

(1.38)

and provides a measure of how much variability there is in f (x) around its mean
value E[f (x)]. Expanding out the square, we see that the variance can also be written in terms of the expectations of f (x) and f (x)2

var[f ] = E[f (x)2] − E[f (x)]2.

(1.39)

In particular, we can consider the variance of the variable x itself, which is given by

var[x] = E[x2] − E[x]2.

(1.40)

For two random variables x and y, the covariance is deﬁned by

cov[x, y] = Ex,y [{x − E[x]} {y − E[y]}] = Ex,y[xy] − E[x]E[y]

(1.41)

which expresses the extent to which x and y vary together. If x and y are independent, then their covariance vanishes.
In the case of two vectors of random variables x and y, the covariance is a matrix

cov[x, y] = Ex,y {x − E[x]}{yT − E[yT]} = Ex,y[xyT] − E[x]E[yT].

(1.42)

If we consider the covariance of the components of a vector x with each other, then we use a slightly simpler notation cov[x] ≡ cov[x, x].

1.2. Probability Theory

21

1.2.3 Bayesian probabilities
So far in this chapter, we have viewed probabilities in terms of the frequencies of random, repeatable events. We shall refer to this as the classical or frequentist interpretation of probability. Now we turn to the more general Bayesian view, in which probabilities provide a quantiﬁcation of uncertainty.
Consider an uncertain event, for example whether the moon was once in its own orbit around the sun, or whether the Arctic ice cap will have disappeared by the end of the century. These are not events that can be repeated numerous times in order to deﬁne a notion of probability as we did earlier in the context of boxes of fruit. Nevertheless, we will generally have some idea, for example, of how quickly we think the polar ice is melting. If we now obtain fresh evidence, for instance from a new Earth observation satellite gathering novel forms of diagnostic information, we may revise our opinion on the rate of ice loss. Our assessment of such matters will affect the actions we take, for instance the extent to which we endeavour to reduce the emission of greenhouse gasses. In such circumstances, we would like to be able to quantify our expression of uncertainty and make precise revisions of uncertainty in the light of new evidence, as well as subsequently to be able to take optimal actions or decisions as a consequence. This can all be achieved through the elegant, and very general, Bayesian interpretation of probability.
The use of probability to represent uncertainty, however, is not an ad-hoc choice, but is inevitable if we are to respect common sense while making rational coherent inferences. For instance, Cox (1946) showed that if numerical values are used to represent degrees of belief, then a simple set of axioms encoding common sense properties of such beliefs leads uniquely to a set of rules for manipulating degrees of belief that are equivalent to the sum and product rules of probability. This provided the ﬁrst rigorous proof that probability theory could be regarded as an extension of Boolean logic to situations involving uncertainty (Jaynes, 2003). Numerous other authors have proposed different sets of properties or axioms that such measures of uncertainty should satisfy (Ramsey, 1931; Good, 1950; Savage, 1961; deFinetti, 1970; Lindley, 1982). In each case, the resulting numerical quantities behave precisely according to the rules of probability. It is therefore natural to refer to these quantities as (Bayesian) probabilities.
In the ﬁeld of pattern recognition, too, it is helpful to have a more general no-

Thomas Bayes
1701–1761
Thomas Bayes was born in Tunbridge Wells and was a clergyman as well as an amateur scientist and a mathematician. He studied logic and theology at Edinburgh University and was elected Fellow of the Royal Society in 1742. During the 18th century, issues regarding probability arose in connection with

gambling and with the new concept of insurance. One particularly important problem concerned so-called inverse probability. A solution was proposed by Thomas Bayes in his paper ‘Essay towards solving a problem in the doctrine of chances’, which was published in 1764, some three years after his death, in the Philosophical Transactions of the Royal Society. In fact, Bayes only formulated his theory for the case of a uniform prior, and it was Pierre-Simon Laplace who independently rediscovered the theory in general form and who demonstrated its broad applicability.

22

1. INTRODUCTION

tion of probability. Consider the example of polynomial curve ﬁtting discussed in Section 1.1. It seems reasonable to apply the frequentist notion of probability to the random values of the observed variables tn. However, we would like to address and quantify the uncertainty that surrounds the appropriate choice for the model parameters w. We shall see that, from a Bayesian perspective, we can use the machinery of probability theory to describe the uncertainty in model parameters such as w, or indeed in the choice of model itself.
Bayes’ theorem now acquires a new signiﬁcance. Recall that in the boxes of fruit example, the observation of the identity of the fruit provided relevant information that altered the probability that the chosen box was the red one. In that example, Bayes’ theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data. As we shall see in detail later, we can adopt a similar approach when making inferences about quantities such as the parameters w in the polynomial curve ﬁtting example. We capture our assumptions about w, before observing the data, in the form of a prior probability distribution p(w). The effect of the observed data D = {t1, . . . , tN } is expressed through the conditional probability p(D|w), and we shall see later, in Section 1.2.5, how this can be represented explicitly. Bayes’ theorem, which takes the form

p(w|D)

=

p(D|w)p(w) p(D)

(1.43)

then allows us to evaluate the uncertainty in w after we have observed D in the form of the posterior probability p(w|D).
The quantity p(D|w) on the right-hand side of Bayes’ theorem is evaluated for the observed data set D and can be viewed as a function of the parameter vector w, in which case it is called the likelihood function. It expresses how probable the observed data set is for different settings of the parameter vector w. Note that the likelihood is not a probability distribution over w, and its integral with respect to w
does not (necessarily) equal one.
Given this deﬁnition of likelihood, we can state Bayes’ theorem in words

posterior ∝ likelihood × prior

(1.44)

where all of these quantities are viewed as functions of w. The denominator in (1.43) is the normalization constant, which ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one. Indeed, integrating both sides of (1.43) with respect to w, we can express the denominator in Bayes’ theorem in terms of the prior distribution and the likelihood function

p(D) = p(D|w)p(w) dw.

(1.45)

In both the Bayesian and frequentist paradigms, the likelihood function p(D|w) plays a central role. However, the manner in which it is used is fundamentally different in the two approaches. In a frequentist setting, w is considered to be a ﬁxed parameter, whose value is determined by some form of ‘estimator’, and error bars

Section 2.1
Section 2.4.3 Section 1.3

1.2. Probability Theory

23

on this estimate are obtained by considering the distribution of possible data sets D. By contrast, from the Bayesian viewpoint there is only a single data set D (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over w.
A widely used frequentist estimator is maximum likelihood, in which w is set to the value that maximizes the likelihood function p(D|w). This corresponds to choosing the value of w for which the probability of the observed data set is maximized. In the machine learning literature, the negative log of the likelihood function is called an error function. Because the negative logarithm is a monotonically decreasing function, maximizing the likelihood is equivalent to minimizing the error.
One approach to determining frequentist error bars is the bootstrap (Efron, 1979; Hastie et al., 2001), in which multiple data sets are created as follows. Suppose our original data set consists of N data points X = {x1, . . . , xN }. We can create a new data set XB by drawing N points at random from X, with replacement, so that some points in X may be replicated in XB, whereas other points in X may be absent from XB. This process can be repeated L times to generate L data sets each of size N and each obtained by sampling from the original data set X. The statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets.
One advantage of the Bayesian viewpoint is that the inclusion of prior knowledge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three times and lands heads each time. A classical maximum likelihood estimate of the probability of landing heads would give 1, implying that all future tosses will land heads! By contrast, a Bayesian approach with any reasonable prior will lead to a much less extreme conclusion.
There has been much controversy and debate associated with the relative merits of the frequentist and Bayesian paradigms, which have not been helped by the fact that there is no unique frequentist, or even Bayesian, viewpoint. For instance, one common criticism of the Bayesian approach is that the prior distribution is often selected on the basis of mathematical convenience rather than as a reﬂection of any prior beliefs. Even the subjective nature of the conclusions through their dependence on the choice of prior is seen by some as a source of difﬁculty. Reducing the dependence on the prior is one motivation for so-called noninformative priors. However, these lead to difﬁculties when comparing different models, and indeed Bayesian methods based on poor choices of prior can give poor results with high conﬁdence. Frequentist evaluation methods offer some protection from such problems, and techniques such as cross-validation remain useful in areas such as model comparison.
This book places a strong emphasis on the Bayesian viewpoint, reﬂecting the huge growth in the practical importance of Bayesian methods in the past few years, while also discussing useful frequentist concepts as required.
Although the Bayesian framework has its origins in the 18th century, the practical application of Bayesian methods was for a long time severely limited by the difﬁculties in carrying through the full Bayesian procedure, particularly the need to marginalize (sum or integrate) over the whole of parameter space, which, as we shall

24

1. INTRODUCTION

Exercise 1.7

see, is required in order to make predictions or to compare different models. The development of sampling methods, such as Markov chain Monte Carlo (discussed in Chapter 11) along with dramatic improvements in the speed and memory capacity of computers, opened the door to the practical use of Bayesian techniques in an impressive range of problem domains. Monte Carlo methods are very ﬂexible and can be applied to a wide range of models. However, they are computationally intensive and have mainly been used for small-scale problems.
More recently, highly efﬁcient deterministic approximation schemes such as variational Bayes and expectation propagation (discussed in Chapter 10) have been developed. These offer a complementary alternative to sampling methods and have allowed Bayesian techniques to be used in large-scale applications (Blei et al., 2003).

1.2.4 The Gaussian distribution

We shall devote the whole of Chapter 2 to a study of various probability dis-

tributions and their key properties. It is convenient, however, to introduce here one

of the most important probability distributions for continuous variables, called the

normal or Gaussian distribution. We shall make extensive use of this distribution in

the remainder of this chapter and indeed throughout much of the book.

For the case of a single real-valued variable x, the Gaussian distribution is de-

ﬁned by

N x|µ, σ2

1 = (2πσ2)1/2 exp

−

1 2σ2

(x

−

µ)2

(1.46)

which is governed by two parameters: µ, called the mean, and σ2, called the variance. The square root of the variance, given by σ, is called the standard deviation, and the reciprocal of the variance, written as β = 1/σ2, is called the precision. We shall see the motivation for these terms shortly. Figure 1.13 shows a plot of the Gaussian distribution.
From the form of (1.46) we see that the Gaussian distribution satisﬁes

N (x|µ, σ2) > 0.

(1.47)

Also it is straightforward to show that the Gaussian is normalized, so that

Pierre-Simon Laplace
1749–1827
It is said that Laplace was seriously lacking in modesty and at one point declared himself to be the best mathematician in France at the time, a claim that was arguably true. As well as being proliﬁc in mathematics, he also made numerous contributions to astronomy, including the nebular hypothesis by which the

earth is thought to have formed from the condensation and cooling of a large rotating disk of gas and dust. In 1812 he published the ﬁrst edition of The´ orie Analytique des Probabilite´ s, in which Laplace states that “probability theory is nothing but common sense reduced to calculation”. This work included a discussion of the inverse probability calculation (later termed Bayes’ theorem by Poincare´ ), which he used to solve problems in life expectancy, jurisprudence, planetary masses, triangulation, and error estimation.

1.2. Probability Theory

25

Figure 1.13 Plot of the univariate Gaussian showing the mean µ and the standard deviation σ.

N (x|µ, σ2)

2σ

Exercise 1.8 Exercise 1.9

µ

x

∞
N x|µ, σ2 dx = 1.
−∞

(1.48)

Thus (1.46) satisﬁes the two requirements for a valid probability density. We can readily ﬁnd expectations of functions of x under the Gaussian distribu-
tion. In particular, the average value of x is given by

∞

E[x] =

N x|µ, σ2 x dx = µ.

−∞

(1.49)

Because the parameter µ represents the average value of x under the distribution, it is referred to as the mean. Similarly, for the second order moment

∞

E[x2] =

N x|µ, σ2 x2 dx = µ2 + σ2.

−∞

(1.50)

From (1.49) and (1.50), it follows that the variance of x is given by

var[x] = E[x2] − E[x]2 = σ2

(1.51)

and hence σ2 is referred to as the variance parameter. The maximum of a distribution is known as its mode. For a Gaussian, the mode coincides with the mean.
We are also interested in the Gaussian distribution deﬁned over a D-dimensional vector x of continuous variables, which is given by

1

1

N (x|µ, Σ) = (2π)D/2 |Σ|1/2 exp

− 1 (x − µ)TΣ−1(x − µ) 2

(1.52)

where the D-dimensional vector µ is called the mean, the D × D matrix Σ is called the covariance, and |Σ| denotes the determinant of Σ. We shall make use of the multivariate Gaussian distribution brieﬂy in this chapter, although its properties will be studied in detail in Section 2.3.

26

1. INTRODUCTION

Figure 1.14

Illustration of the likelihood function for a Gaussian distribution, shown by the red curve. Here the black points denote a data set of values {xn}, and the likelihood function given by (1.53) corresponds to the product of the blue values. Maximizing the likelihood involves adjusting the mean and variance of the Gaussian so as to maximize this product.

p(x)

N (xn|µ, σ2)

xn

x

Section 1.2.5

Now suppose that we have a data set of observations x = (x1, . . . , xN )T, representing N observations of the scalar variable x. Note that we are using the typeface x to distinguish this from a single observation of the vector-valued variable (x1, . . . , xD)T, which we denote by x. We shall suppose that the observations are drawn independently from a Gaussian distribution whose mean µ and variance σ2
are unknown, and we would like to determine these parameters from the data set.
Data points that are drawn independently from the same distribution are said to be
independent and identically distributed, which is often abbreviated to i.i.d. We have
seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately. Because our data set x is i.i.d., we can therefore write the probability of the data set, given µ and σ2, in the form

N
p(x|µ, σ2) = N xn|µ, σ2 .
n=1

(1.53)

When viewed as a function of µ and σ2, this is the likelihood function for the Gaussian and is interpreted diagrammatically in Figure 1.14.
One common criterion for determining the parameters in a probability distribution using an observed data set is to ﬁnd the parameter values that maximize the likelihood function. This might seem like a strange criterion because, from our foregoing discussion of probability theory, it would seem more natural to maximize the probability of the parameters given the data, not the probability of the data given the parameters. In fact, these two criteria are related, as we shall discuss in the context of curve ﬁtting.
For the moment, however, we shall determine values for the unknown parameters µ and σ2 in the Gaussian by maximizing the likelihood function (1.53). In practice, it is more convenient to maximize the log of the likelihood function. Because the logarithm is a monotonically increasing function of its argument, maximization of the log of a function is equivalent to maximization of the function itself. Taking the log not only simpliﬁes the subsequent mathematical analysis, but it also helps numerically because the product of a large number of small probabilities can easily underﬂow the numerical precision of the computer, and this is resolved by computing instead the sum of the log probabilities. From (1.46) and (1.53), the log likelihood

Exercise 1.11
Section 1.1 Exercise 1.12

1.2. Probability Theory

27

function can be written in the form

ln p

x|µ, σ2

1 =−
2σ2

N

(xn − µ)2 −

N 2

ln σ2 −

N 2

ln(2π).

n=1

(1.54)

Maximizing (1.54) with respect to µ, we obtain the maximum likelihood solution

given by

1N

µML = N

xn

n=1

(1.55)

which is the sample mean, i.e., the mean of the observed values {xn}. Similarly, maximizing (1.54) with respect to σ2, we obtain the maximum likelihood solution
for the variance in the form

σM2 L

=

1 N

N
(xn − µML)2

n=1

(1.56)

which is the sample variance measured with respect to the sample mean µML. Note that we are performing a joint maximization of (1.54) with respect to µ and σ2, but in the case of the Gaussian distribution the solution for µ decouples from that for σ2
so that we can ﬁrst evaluate (1.55) and then subsequently use this result to evaluate
(1.56).
Later in this chapter, and also in subsequent chapters, we shall highlight the sig-
niﬁcant limitations of the maximum likelihood approach. Here we give an indication
of the problem in the context of our solutions for the maximum likelihood param-
eter settings for the univariate Gaussian distribution. In particular, we shall show
that the maximum likelihood approach systematically underestimates the variance
of the distribution. This is an example of a phenomenon called bias and is related
to the problem of over-ﬁtting encountered in the context of polynomial curve ﬁtting. We ﬁrst note that the maximum likelihood solutions µML and σM2 L are functions of the data set values x1, . . . , xN . Consider the expectations of these quantities with respect to the data set values, which themselves come from a Gaussian distribution with parameters µ and σ2. It is straightforward to show that

E[µML] = µ

E[σM2 L] =

N − 1 σ2 N

(1.57) (1.58)

so that on average the maximum likelihood estimate will obtain the correct mean but

will underestimate the true variance by a factor (N − 1)/N . The intuition behind

this result is given by Figure 1.15.

From (1.58) it follows that the following estimate for the variance parameter is

unbiased

σ2

=

N N−

1

σM2 L

=

N

1 −

1

N
(xn − µML)2.

n=1

(1.59)

28

1. INTRODUCTION

Figure 1.15 Illustration of how bias arises in using max-

imum likelihood to determine the variance

of a Gaussian. The green curve shows

the true Gaussian distribution from which

data is generated, and the three red curves

show the Gaussian distributions obtained (a)

by ﬁtting to three data sets, each consist-

ing of two data points shown in blue, us-

ing the maximum likelihood results (1.55)

and (1.56). Averaged across the three data

sets, the mean is correct, but the variance is systematically under-estimated because

(b)

it is measured relative to the sample mean

and not relative to the true mean.

(c)

Section 1.1

In Section 10.1.3, we shall see how this result arises automatically when we adopt a Bayesian approach.
Note that the bias of the maximum likelihood solution becomes less signiﬁcant as the number N of data points increases, and in the limit N → ∞ the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data. In practice, for anything other than small N , this bias will not prove to be a serious problem. However, throughout this book we shall be interested in more complex models with many parameters, for which the bias problems associated with maximum likelihood will be much more severe. In fact, as we shall see, the issue of bias in maximum likelihood lies at the root of the over-ﬁtting problem that we encountered earlier in the context of polynomial curve ﬁtting.

1.2.5 Curve ﬁtting re-visited

We have seen how the problem of polynomial curve ﬁtting can be expressed in

terms of error minimization. Here we return to the curve ﬁtting example and view it

from a probabilistic perspective, thereby gaining some insights into error functions

and regularization, as well as taking us towards a full Bayesian treatment.

The goal in the curve ﬁtting problem is to be able to make predictions for the

target variable t given some new value of the input variable x on the basis of a set of

training data comprising N input values x = (x1, . . . , xN )T and their corresponding target values t = (t1, . . . , tN )T. We can express our uncertainty over the value of
the target variable using a probability distribution. For this purpose, we shall assume

that, given the value of x, the corresponding value of t has a Gaussian distribution

with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). Thus

we have

p(t|x, w, β) = N t|y(x, w), β−1

(1.60)

where, for consistency with the notation in later chapters, we have deﬁned a precision parameter β corresponding to the inverse variance of the distribution. This is illustrated schematically in Figure 1.16.

Figure 1.16 Schematic illustration of a Gaus-

sian conditional distribution for t given x given by

t

(1.60), in which the mean is given by the polyno-

mial function y(x, w), and the precision is given

by the parameter β, which is related to the variance by β−1 = σ2.

y(x0, w)

1.2. Probability Theory

29

y(x, w)

2σ p(t|x0, w, β)

x0

x

We now use the training data {x, t} to determine the values of the unknown parameters w and β by maximum likelihood. If the data are assumed to be drawn independently from the distribution (1.60), then the likelihood function is given by

N
p(t|x, w, β) = N tn|y(xn, w), β−1 .
n=1

(1.61)

As we did in the case of the simple Gaussian distribution earlier, it is convenient to maximize the logarithm of the likelihood function. Substituting for the form of the Gaussian distribution, given by (1.46), we obtain the log likelihood function in the form

ln p(t|x, w, β) = − β 2

N

{y(xn, w)

−

tn}2

+

N 2

ln β

−

N 2

ln(2π).

n=1

(1.62)

Consider ﬁrst the determination of the maximum likelihood solution for the polynomial coefﬁcients, which will be denoted by wML. These are determined by maximizing (1.62) with respect to w. For this purpose, we can omit the last two terms on the right-hand side of (1.62) because they do not depend on w. Also, we note that scaling the log likelihood by a positive constant coefﬁcient does not alter the location of the maximum with respect to w, and so we can replace the coefﬁcient β/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently minimize the negative log likelihood. We therefore see that maximizing likelihood is equivalent, so far as determining w is concerned, to minimizing the sum-of-squares error function deﬁned by (1.2). Thus the sum-of-squares error function has arisen as a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution.
We can also use maximum likelihood to determine the precision parameter β of the Gaussian conditional distribution. Maximizing (1.62) with respect to β gives

1 βML

=

1 N

N
{y(xn, wML) − tn}2 .
n=1

(1.63)

30

1. INTRODUCTION

Section 1.2.4

Again we can ﬁrst determine the parameter vector wML governing the mean and subsequently use this to ﬁnd the precision βML as was the case for the simple Gaussian distribution.
Having determined the parameters w and β, we can now make predictions for new values of x. Because we now have a probabilistic model, these are expressed in terms of the predictive distribution that gives the probability distribution over t,
rather than simply a point estimate, and is obtained by substituting the maximum
likelihood parameters into (1.60) to give

p(t|x, wML, βML) = N t|y(x, wML), βM−1L .

(1.64)

Now let us take a step towards a more Bayesian approach and introduce a prior distribution over the polynomial coefﬁcients w. For simplicity, let us consider a Gaussian distribution of the form

p(w|α) = N (w|0, α−1I) =

α

(M +1)/2
exp

− α wTw

2π

2

(1.65)

where α is the precision of the distribution, and M +1 is the total number of elements in the vector w for an M th order polynomial. Variables such as α, which control the distribution of model parameters, are called hyperparameters. Using Bayes’ theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function

p(w|x, t, α, β) ∝ p(t|x, w, β)p(w|α).

(1.66)

We can now determine w by ﬁnding the most probable value of w given the data, in other words by maximizing the posterior distribution. This technique is called maximum posterior, or simply MAP. Taking the negative logarithm of (1.66) and combining with (1.62) and (1.65), we ﬁnd that the maximum of the posterior is given by the minimum of

β 2

N

{y(xn, w) − tn}2 +

α wTw. 2

n=1

(1.67)

Thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-squares error function encountered earlier in the form (1.4), with a regularization parameter given by λ = α/β.

1.2.6 Bayesian curve ﬁtting
Although we have included a prior distribution p(w|α), we are so far still making a point estimate of w and so this does not yet amount to a Bayesian treatment. In a fully Bayesian approach, we should consistently apply the sum and product rules of probability, which requires, as we shall see shortly, that we integrate over all values of w. Such marginalizations lie at the heart of Bayesian methods for pattern recognition.

1.2. Probability Theory

31

In the curve ﬁtting problem, we are given the training data x and t, along with a new test point x, and our goal is to predict the value of t. We therefore wish to evaluate the predictive distribution p(t|x, x, t). Here we shall assume that the parameters α and β are ﬁxed and known in advance (in later chapters we shall discuss how such parameters can be inferred from data in a Bayesian setting).
A Bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability, which allow the predictive distribution to be written in the form

p(t|x, x, t) = p(t|x, w)p(w|x, t) dw.

(1.68)

Here p(t|x, w) is given by (1.60), and we have omitted the dependence on α and β to simplify the notation. Here p(w|x, t) is the posterior distribution over parameters, and can be found by normalizing the right-hand side of (1.66). We shall see in Section 3.3 that, for problems such as the curve-ﬁtting example, this posterior distribution is a Gaussian and can be evaluated analytically. Similarly, the integration in (1.68) can also be performed analytically with the result that the predictive distribution is given by a Gaussian of the form

p(t|x, x, t) = N t|m(x), s2(x)

(1.69)

where the mean and variance are given by
N
m(x) = βφ(x)TS φ(xn)tn
n=1
s2(x) = β−1 + φ(x)TSφ(x).

(1.70) (1.71)

Here the matrix S is given by

N
S−1 = αI + β φ(xn)φ(x)T
n=1

(1.72)

where I is the unit matrix, and we have deﬁned the vector φ(x) with elements φi(x) = xi for i = 0, . . . , M .
We see that the variance, as well as the mean, of the predictive distribution in
(1.69) is dependent on x. The ﬁrst term in (1.71) represents the uncertainty in the
predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution (1.64) through βM−1L. However, the second term arises from the uncertainty in the parameters w and is a consequence
of the Bayesian treatment. The predictive distribution for the synthetic sinusoidal
regression problem is illustrated in Figure 1.17.

32

1. INTRODUCTION

Figure 1.17 The predictive distribution result-

ing from a Bayesian treatment of

polynomial curve ﬁtting using an M = 9 polynomial, with the ﬁxed

1

parameters α = 5 × 10−3 and β = t

11.1 (corresponding to the known

noise variance), in which the red curve denotes the mean of the 0

predictive distribution and the red

region corresponds to ±1 stan-

dard deviation around the mean.

−1

0

x1

1.3. Model Selection
In our example of polynomial curve ﬁtting using least squares, we saw that there was an optimal order of polynomial that gave the best generalization. The order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity. With regularized least squares, the regularization coefﬁcient λ also controls the effective complexity of the model, whereas for more complex models, such as mixture distributions or neural networks there may be multiple parameters governing complexity. In a practical application, we need to determine the values of such parameters, and the principal objective in doing so is usually to achieve the best predictive performance on new data. Furthermore, as well as ﬁnding the appropriate values for complexity parameters within a given model, we may wish to consider a range of different types of model in order to ﬁnd the best one for our particular application.
We have already seen that, in the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-ﬁtting. If data is plentiful, then one approach is simply to use some of the available data to train a range of models, or a given model with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the best predictive performance. If the model design is iterated many times using a limited size data set, then some over-ﬁtting to the validation data can occur and so it may be necessary to keep aside a third test set on which the performance of the selected model is ﬁnally evaluated.
In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this dilemma is to use cross-validation, which is illustrated in Figure 1.18. This allows a proportion (S − 1)/S of the available data to be used for training while making use of all of the

1.4. The Curse of Dimensionality

Figure 1.18

The technique of S-fold cross-validation, illustrated here for the case of S = 4, involves taking the available data and partitioning it into S groups (in the simplest case these are of equal size). Then S − 1 of the groups are used to train a set of models that are then evaluated on the re-
maining group. This procedure is then repeated for all S possible choices for the held-out group, indicated here by the red blocks, and the performance scores from the S runs are then averaged.

33
run 1 run 2 run 3 run 4

data to assess performance. When data is particularly scarce, it may be appropriate

to consider the case S = N , where N is the total number of data points, which gives

the leave-one-out technique.

One major drawback of cross-validation is that the number of training runs that

must be performed is increased by a factor of S, and this can prove problematic for

models in which the training is itself computationally expensive. A further problem

with techniques such as cross-validation that use separate data to assess performance

is that we might have multiple complexity parameters for a single model (for in-

stance, there might be several regularization parameters). Exploring combinations

of settings for such parameters could, in the worst case, require a number of training

runs that is exponential in the number of parameters. Clearly, we need a better ap-

proach. Ideally, this should rely only on the training data and should allow multiple

hyperparameters and model types to be compared in a single training run. We there-

fore need to ﬁnd a measure of performance which depends only on the training data

and which does not suffer from bias due to over-ﬁtting.

Historically various ‘information criteria’ have been proposed that attempt to

correct for the bias of maximum likelihood by the addition of a penalty term to

compensate for the over-ﬁtting of more complex models. For example, the Akaike

information criterion, or AIC (Akaike, 1974), chooses the model for which the quan-

tity

ln p(D|wML) − M

(1.73)

is largest. Here p(D|wML) is the best-ﬁt log likelihood, and M is the number of adjustable parameters in the model. A variant of this quantity, called the Bayesian information criterion, or BIC, will be discussed in Section 4.4.1. Such criteria do not take account of the uncertainty in the model parameters, however, and in practice they tend to favour overly simple models. We therefore turn in Section 3.4 to a fully Bayesian approach where we shall see how complexity penalties arise in a natural and principled way.

1.4. The Curse of Dimensionality
In the polynomial curve ﬁtting example we had just one input variable x. For practical applications of pattern recognition, however, we will have to deal with spaces

34

1. INTRODUCTION

Figure 1.19 Scatter plot of the oil ﬂow data for input variables x6 and x7, in

2

which red denotes the ‘homoge-

nous’ class, green denotes the

‘annular’ class, and blue denotes

the ‘laminar’ class. Our goal is

1.5

to classify the new test point de-

noted by ‘×’.

x7 1

0.5

0

0

0.25

0.5

0.75

1

x6

of high dimensionality comprising many input variables. As we now discuss, this poses some serious challenges and is an important factor inﬂuencing the design of pattern recognition techniques.
In order to illustrate the problem we consider a synthetically generated data set representing measurements taken from a pipeline containing a mixture of oil, water, and gas (Bishop and James, 1993). These three materials can be present in one of three different geometrical conﬁgurations known as ‘homogenous’, ‘annular’, and ‘laminar’, and the fractions of the three materials can also vary. Each data point comprises a 12-dimensional input vector consisting of measurements taken with gamma ray densitometers that measure the attenuation of gamma rays passing along narrow beams through the pipe. This data set is described in detail in Appendix A. Figure 1.19 shows 100 points from this data set on a plot showing two of the measurements x6 and x7 (the remaining ten input values are ignored for the purposes of this illustration). Each data point is labelled according to which of the three geometrical classes it belongs to, and our goal is to use this data as a training set in order to be able to classify a new observation (x6, x7), such as the one denoted by the cross in Figure 1.19. We observe that the cross is surrounded by numerous red points, and so we might suppose that it belongs to the red class. However, there are also plenty of green points nearby, so we might think that it could instead belong to the green class. It seems unlikely that it belongs to the blue class. The intuition here is that the identity of the cross should be determined more strongly by nearby points from the training set and less strongly by more distant points. In fact, this intuition turns out to be reasonable and will be discussed more fully in later chapters.
How can we turn this intuition into a learning algorithm? One very simple approach would be to divide the input space into regular cells, as indicated in Figure 1.20. When we are given a test point and we wish to predict its class, we ﬁrst decide which cell it belongs to, and we then ﬁnd all of the training data points that

1.4. The Curse of Dimensionality

35

Figure 1.20

Illustration of a simple approach to the solution of a classiﬁcation problem in which the input space is divided into cells and any new test point is assigned to the class that has a majority number of representatives in the same cell as the test point. As we shall see shortly, this simplistic approach has some severe shortcomings.

2 1.5 x7 1

0.5

0

0

0.25

0.5

0.75

1

x6

Section 1.1

fall in the same cell. The identity of the test point is predicted as being the same as the class having the largest number of training points in the same cell as the test point (with ties being broken at random).
There are numerous problems with this naive approach, but one of the most severe becomes apparent when we consider its extension to problems having larger numbers of input variables, corresponding to input spaces of higher dimensionality. The origin of the problem is illustrated in Figure 1.21, which shows that, if we divide a region of a space into regular cells, then the number of such cells grows exponentially with the dimensionality of the space. The problem with an exponentially large number of cells is that we would need an exponentially large quantity of training data in order to ensure that the cells are not empty. Clearly, we have no hope of applying such a technique in a space of more than a few variables, and so we need to ﬁnd a more sophisticated approach.
We can gain further insight into the problems of high-dimensional spaces by returning to the example of polynomial curve ﬁtting and considering how we would

Figure 1.21 Illustration of the

curse of dimensionality, showing

how the number of regions of a

regular grid grows exponentially with the dimensionality D of the

x2

space. For clarity, only a subset of

the cubical regions are shown for

D = 3.

x2 x1

x1 D=1

x1 x3 D=2

D=3

36

1. INTRODUCTION

Exercise 1.16 Exercise 1.18 Exercise 1.20

extend this approach to deal with input spaces having several variables. If we have D input variables, then a general polynomial with coefﬁcients up to order 3 would take the form

D

DD

DDD

y(x, w) = w0 + wixi +

wij xixj +

wijkxixjxk. (1.74)

i=1

i=1 j=1

i=1 j=1 k=1

As D increases, so the number of independent coefﬁcients (not all of the coefﬁcients are independent due to interchange symmetries amongst the x variables) grows proportionally to D3. In practice, to capture complex dependencies in the data, we may need to use a higher-order polynomial. For a polynomial of order M , the growth in the number of coefﬁcients is like DM . Although this is now a power law growth,
rather than an exponential growth, it still points to the method becoming rapidly
unwieldy and of limited practical utility.
Our geometrical intuitions, formed through a life spent in a space of three di-
mensions, can fail badly when we consider spaces of higher dimensionality. As a simple example, consider a sphere of radius r = 1 in a space of D dimensions, and ask what is the fraction of the volume of the sphere that lies between radius r = 1− and r = 1. We can evaluate this fraction by noting that the volume of a sphere of radius r in D dimensions must scale as rD, and so we write

VD(r) = KDrD

(1.75)

where the constant KD depends only on D. Thus the required fraction is given by

VD(1) − VD(1 − ) = 1 − (1 − )D VD (1)

(1.76)

which is plotted as a function of for various values of D in Figure 1.22. We see that, for large D, this fraction tends to 1 even for small values of . Thus, in spaces of high dimensionality, most of the volume of a sphere is concentrated in a thin shell near the surface!
As a further example, of direct relevance to pattern recognition, consider the behaviour of a Gaussian distribution in a high-dimensional space. If we transform from Cartesian to polar coordinates, and then integrate out the directional variables, we obtain an expression for the density p(r) as a function of radius r from the origin. Thus p(r)δr is the probability mass inside a thin shell of thickness δr located at radius r. This distribution is plotted, for various values of D, in Figure 1.23, and we see that for large D the probability mass of the Gaussian is concentrated in a thin shell.
The severe difﬁculty that can arise in spaces of many dimensions is sometimes called the curse of dimensionality (Bellman, 1961). In this book, we shall make extensive use of illustrative examples involving input spaces of one or two dimensions, because this makes it particularly easy to illustrate the techniques graphically. The reader should be warned, however, that not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions.

1.4. The Curse of Dimensionality

37

Figure 1.22 Plot of the fraction of the volume of a sphere lying in the range r = 1−

1

to r = 1 for various values of the

dimensionality D.

0.8

0.6

0.4

D = 20 D =5 D =2 D =1

volume fraction

0.2

0

0

0.2 0.4 0.6 0.8

1

Although the curse of dimensionality certainly raises important issues for pattern recognition applications, it does not prevent us from ﬁnding effective techniques applicable to high-dimensional spaces. The reasons for this are twofold. First, real data will often be conﬁned to a region of the space having lower effective dimensionality, and in particular the directions over which important variations in the target variables occur may be so conﬁned. Second, real data will typically exhibit some smoothness properties (at least locally) so that for the most part small changes in the input variables will produce small changes in the target variables, and so we can exploit local interpolation-like techniques to allow us to make predictions of the target variables for new values of the input variables. Successful pattern recognition techniques exploit one or both of these properties. Consider, for example, an application in manufacturing in which images are captured of identical planar objects on a conveyor belt, in which the goal is to determine their orientation. Each image is a point

Figure 1.23

Plot of the probability density with respect to radius r of a Gaussian distribution for various values of the dimensionality D. In a high-dimensional space, most of the probability mass of a Gaussian is located within a thin shell at a speciﬁc radius.

p(r)

2
D=1
D=2 1

D = 20

0

0

2

4

r

38

1. INTRODUCTION

in a high-dimensional space whose dimensionality is determined by the number of pixels. Because the objects can occur at different positions within the image and in different orientations, there are three degrees of freedom of variability between images, and a set of images will live on a three dimensional manifold embedded within the high-dimensional space. Due to the complex relationships between the object position or orientation and the pixel intensities, this manifold will be highly nonlinear. If the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position, then there is only one degree of freedom of variability within the manifold that is signiﬁcant.

1.5. Decision Theory
We have seen in Section 1.2 how probability theory provides us with a consistent mathematical framework for quantifying and manipulating uncertainty. Here we turn to a discussion of decision theory that, when combined with probability theory, allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition.
Suppose we have an input vector x together with a corresponding vector t of target variables, and our goal is to predict t given a new value for x. For regression problems, t will comprise continuous variables, whereas for classiﬁcation problems t will represent class labels. The joint probability distribution p(x, t) provides a complete summary of the uncertainty associated with these variables. Determination of p(x, t) from a set of training data is an example of inference and is typically a very difﬁcult problem whose solution forms the subject of much of this book. In a practical application, however, we must often make a speciﬁc prediction for the value of t, or more generally take a speciﬁc action based on our understanding of the values t is likely to take, and this aspect is the subject of decision theory.
Consider, for example, a medical diagnosis problem in which we have taken an X-ray image of a patient, and we wish to determine whether the patient has cancer or not. In this case, the input vector x is the set of pixel intensities in the image, and output variable t will represent the presence of cancer, which we denote by the class C1, or the absence of cancer, which we denote by the class C2. We might, for instance, choose t to be a binary variable such that t = 0 corresponds to class C1 and t = 1 corresponds to class C2. We shall see later that this choice of label values is particularly convenient for probabilistic models. The general inference problem then involves determining the joint distribution p(x, Ck), or equivalently p(x, t), which gives us the most complete probabilistic description of the situation. Although this can be a very useful and informative quantity, in the end we must decide either to give treatment to the patient or not, and we would like this choice to be optimal in some appropriate sense (Duda and Hart, 1973). This is the decision step, and it is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities. We shall see that the decision stage is generally very simple, even trivial, once we have solved the inference problem.
Here we give an introduction to the key ideas of decision theory as required for

1.5. Decision Theory

39

the rest of the book. Further background, as well as more detailed accounts, can be found in Berger (1985) and Bather (2000).
Before giving a more detailed analysis, let us ﬁrst consider informally how we might expect probabilities to play a role in making decisions. When we obtain the X-ray image x for a new patient, our goal is to decide which of the two classes to assign to the image. We are interested in the probabilities of the two classes given the image, which are given by p(Ck|x). Using Bayes’ theorem, these probabilities can be expressed in the form

p(Ck|x)

=

p(x|Ck)p(Ck) . p(x)

(1.77)

Note that any of the quantities appearing in Bayes’ theorem can be obtained from the joint distribution p(x, Ck) by either marginalizing or conditioning with respect to the appropriate variables. We can now interpret p(Ck) as the prior probability for the class Ck, and p(Ck|x) as the corresponding posterior probability. Thus p(C1) represents the probability that a person has cancer, before we take the X-ray measurement. Similarly, p(C1|x) is the corresponding probability, revised using Bayes’ theorem in light of the information contained in the X-ray. If our aim is to minimize the chance
of assigning x to the wrong class, then intuitively we would choose the class having
the higher posterior probability. We now show that this intuition is correct, and we
also discuss more general criteria for making decisions.

1.5.1 Minimizing the misclassiﬁcation rate
Suppose that our goal is simply to make as few misclassiﬁcations as possible. We need a rule that assigns each value of x to one of the available classes. Such a rule will divide the input space into regions Rk called decision regions, one for each class, such that all points in Rk are assigned to class Ck. The boundaries between decision regions are called decision boundaries or decision surfaces. Note that each decision region need not be contiguous but could comprise some number of disjoint regions. We shall encounter examples of decision boundaries and decision regions in later chapters. In order to ﬁnd the optimal decision rule, consider ﬁrst of all the case of two classes, as in the cancer problem for instance. A mistake occurs when an input vector belonging to class C1 is assigned to class C2 or vice versa. The probability of this occurring is given by

p(mistake) = p(x ∈ R1, C2) + p(x ∈ R2, C1)

=

p(x, C2) dx + p(x, C1) dx.

R1

R2

(1.78)

We are free to choose the decision rule that assigns each point x to one of the two
classes. Clearly to minimize p(mistake) we should arrange that each x is assigned to
whichever class has the smaller value of the integrand in (1.78). Thus, if p(x, C1) > p(x, C2) for a given value of x, then we should assign that x to class C1. From the product rule of probability we have p(x, Ck) = p(Ck|x)p(x). Because the factor p(x) is common to both terms, we can restate this result as saying that the minimum

40

1. INTRODUCTION

p(x, C1)

x0

x

p(x, C2)

x

R1

R2

Figure 1.24

Schematic illustration of the joint probabilities p(x, Ck) for each of two classes plotted against x, together with the decision boundary x = xb. Values of x xb are classiﬁed as class C2 and hence belong to decision region R2, whereas points x < xb are classiﬁed as C1 and belong to R1. Errors arise from the blue, green, and red regions, so that for x < xb the errors are due to points from class C2 being misclassiﬁed as C1 (represented by the sum of the red and green regions), and conversely for points in the region x xb the errors are due to points from class C1 being misclassiﬁed as C2 (represented by the blue region). As we vary the location xb of the decision boundary, the combined areas of the
blue and green regions remains constant, whereas the size of the red region varies. The
optimal choice for xb is where the curves for p(x, C1) and p(x, C2) cross, corresponding to xb = x0, because in this case the red region disappears. This is equivalent to the minimum misclassiﬁcation rate decision rule, which assigns each value of x to the class having the higher posterior probability p(Ck|x).

probability of making a mistake is obtained if each value of x is assigned to the class for which the posterior probability p(Ck|x) is largest. This result is illustrated for two classes, and a single input variable x, in Figure 1.24.
For the more general case of K classes, it is slightly easier to maximize the
probability of being correct, which is given by

K

p(correct) =

p(x ∈ Rk, Ck)

k=1

K

=

p(x, Ck) dx

k=1 Rk

(1.79)

which is maximized when the regions Rk are chosen such that each x is assigned to the class for which p(x, Ck) is largest. Again, using the product rule p(x, Ck) = p(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see
that each x should be assigned to the class having the largest posterior probability
p(Ck|x).

Figure 1.25 An example of a loss matrix with elements Lkj for the cancer treatment problem. The rows correspond to the true class, whereas the columns correspond to the assignment of class made by our decision criterion.

1.5. Decision Theory

41

cancer normal

cancer 0 normal 1

1000 0

1.5.2 Minimizing the expected loss
For many applications, our objective will be more complex than simply minimizing the number of misclassiﬁcations. Let us consider again the medical diagnosis problem. We note that, if a patient who does not have cancer is incorrectly diagnosed as having cancer, the consequences may be some patient distress plus the need for further investigations. Conversely, if a patient with cancer is diagnosed as healthy, the result may be premature death due to lack of treatment. Thus the consequences of these two types of mistake can be dramatically different. It would clearly be better to make fewer mistakes of the second kind, even if this was at the expense of making more mistakes of the ﬁrst kind.
We can formalize such issues through the introduction of a loss function, also called a cost function, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred. Note that some authors consider instead a utility function, whose value they aim to maximize. These are equivalent concepts if we take the utility to be simply the negative of the loss, and throughout this text we shall use the loss function convention. Suppose that, for a new value of x, the true class is Ck and that we assign x to class Cj (where j may or may not be equal to k). In so doing, we incur some level of loss that we denote by Lkj, which we can view as the k, j element of a loss matrix. For instance, in our cancer example, we might have a loss matrix of the form shown in Figure 1.25. This particular loss matrix says that there is no loss incurred if the correct decision is made, there is a loss of 1 if a healthy patient is diagnosed as having cancer, whereas there is a loss of 1000 if a patient having cancer is diagnosed as healthy.
The optimal solution is the one which minimizes the loss function. However, the loss function depends on the true class, which is unknown. For a given input vector x, our uncertainty in the true class is expressed through the joint probability distribution p(x, Ck) and so we seek instead to minimize the average loss, where the average is computed with respect to this distribution, which is given by

E[L] =

Lkjp(x, Ck) dx.

k j Rj

(1.80)

Each x can be assigned independently to one of the decision regions Rj. Our goal is to choose the regions Rj in order to minimize the expected loss (1.80), which implies that for each x we should minimize k Lkjp(x, Ck). As before, we can use the product rule p(x, Ck) = p(Ck|x)p(x) to eliminate the common factor of p(x).
Thus the decision rule that minimizes the expected loss is the one that assigns each

42

1. INTRODUCTION

Figure 1.26 Illustration of the reject option. Inputs x such that the larger of the two poste- 1.0 rior probabilities is less than or equal to θ some threshold θ will be rejected.

p(C1|x)

p(C2|x)

Exercise 1.24

0.0

reject region

x

new x to the class j for which the quantity

Lkj p(Ck |x)
k

(1.81)

is a minimum. This is clearly trivial to do, once we know the posterior class probabilities p(Ck|x).

1.5.3 The reject option
We have seen that classiﬁcation errors arise from the regions of input space where the largest of the posterior probabilities p(Ck|x) is signiﬁcantly less than unity, or equivalently where the joint distributions p(x, Ck) have comparable values. These are the regions where we are relatively uncertain about class membership. In some applications, it will be appropriate to avoid making decisions on the difﬁcult cases in anticipation of a lower error rate on those examples for which a classiﬁcation decision is made. This is known as the reject option. For example, in our hypothetical medical illustration, it may be appropriate to use an automatic system to classify those X-ray images for which there is little doubt as to the correct class, while leaving a human expert to classify the more ambiguous cases. We can achieve this by introducing a threshold θ and rejecting those inputs x for which the largest of the posterior probabilities p(Ck|x) is less than or equal to θ. This is illustrated for the case of two classes, and a single continuous input variable x, in Figure 1.26. Note that setting θ = 1 will ensure that all examples are rejected, whereas if there are K classes then setting θ < 1/K will ensure that no examples are rejected. Thus the fraction of examples that get rejected is controlled by the value of θ.
We can easily extend the reject criterion to minimize the expected loss, when a loss matrix is given, taking account of the loss incurred when a reject decision is made.

1.5.4 Inference and decision
We have broken the classiﬁcation problem down into two separate stages, the inference stage in which we use training data to learn a model for p(Ck|x), and the

1.5. Decision Theory

43

subsequent decision stage in which we use these posterior probabilities to make optimal class assignments. An alternative possibility would be to solve both problems together and simply learn a function that maps inputs x directly into decisions. Such a function is called a discriminant function.
In fact, we can identify three distinct approaches to solving decision problems, all of which have been used in practical applications. These are given, in decreasing order of complexity, by:

(a) First solve the inference problem of determining the class-conditional densities
p(x|Ck) for each class Ck individually. Also separately infer the prior class probabilities p(Ck). Then use Bayes’ theorem in the form

p(Ck|x)

=

p(x|Ck)p(Ck) p(x)

(1.82)

to ﬁnd the posterior class probabilities p(Ck|x). As usual, the denominator in Bayes’ theorem can be found in terms of the quantities appearing in the
numerator, because

p(x) = p(x|Ck)p(Ck).
k

(1.83)

Equivalently, we can model the joint distribution p(x, Ck) directly and then normalize to obtain the posterior probabilities. Having found the posterior probabilities, we use decision theory to determine class membership for each new input x. Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.

(b) First solve the inference problem of determining the posterior class probabilities p(Ck|x), and then subsequently use decision theory to assign each new x to one of the classes. Approaches that model the posterior probabilities directly
are called discriminative models.

(c) Find a function f (x), called a discriminant function, which maps each input x directly onto a class label. For instance, in the case of two-class problems, f (·) might be binary valued and such that f = 0 represents class C1 and f = 1 represents class C2. In this case, probabilities play no role.
Let us consider the relative merits of these three alternatives. Approach (a) is the most demanding because it involves ﬁnding the joint distribution over both x and Ck. For many applications, x will have high dimensionality, and consequently we may need a large training set in order to be able to determine the class-conditional densities to reasonable accuracy. Note that the class priors p(Ck) can often be estimated simply from the fractions of the training set data points in each of the classes. One advantage of approach (a), however, is that it also allows the marginal density of data p(x) to be determined from (1.83). This can be useful for detecting new data points that have low probability under the model and for which the predictions may

44

1. INTRODUCTION

class densities

5
4
3
2 p(x|C1)
1

p(x|C2)

1.2 p(C1|x)
1
0.8
0.6
0.4
0.2

p(C2|x)

0

0

0.2 0.4 0.6 0.8

1

x

0

0

0.2 0.4 0.6 0.8

1

x

Figure 1.27 Example of the class-conditional densities for two classes having a single input variable x (left plot) together with the corresponding posterior probabilities (right plot). Note that the left-hand mode of the class-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities. The vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiﬁcation rate.

be of low accuracy, which is known as outlier detection or novelty detection (Bishop, 1994; Tarassenko, 1995).
However, if we only wish to make classiﬁcation decisions, then it can be wasteful of computational resources, and excessively demanding of data, to ﬁnd the joint distribution p(x, Ck) when in fact we only really need the posterior probabilities p(Ck|x), which can be obtained directly through approach (b). Indeed, the classconditional densities may contain a lot of structure that has little effect on the posterior probabilities, as illustrated in Figure 1.27. There has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning, and in ﬁnding ways to combine them (Jebara, 2004; Lasserre et al., 2006).
An even simpler approach is (c) in which we use the training data to ﬁnd a discriminant function f (x) that maps each x directly onto a class label, thereby combining the inference and decision stages into a single learning problem. In the example of Figure 1.27, this would correspond to ﬁnding the value of x shown by the vertical green line, because this is the decision boundary giving the minimum probability of misclassiﬁcation.
With option (c), however, we no longer have access to the posterior probabilities p(Ck|x). There are many powerful reasons for wanting to compute the posterior probabilities, even if we subsequently use them to make decisions. These include:
Minimizing risk. Consider a problem in which the elements of the loss matrix are subjected to revision from time to time (such as might occur in a ﬁnancial

1.5. Decision Theory

45

application). If we know the posterior probabilities, we can trivially revise the minimum risk decision criterion by modifying (1.81) appropriately. If we have only a discriminant function, then any change to the loss matrix would require that we return to the training data and solve the classiﬁcation problem afresh.
Reject option. Posterior probabilities allow us to determine a rejection criterion that will minimize the misclassiﬁcation rate, or more generally the expected loss, for a given fraction of rejected data points.
Compensating for class priors. Consider our medical X-ray problem again, and suppose that we have collected a large number of X-ray images from the general population for use as training data in order to build an automated screening system. Because cancer is rare amongst the general population, we might ﬁnd that, say, only 1 in every 1,000 examples corresponds to the presence of cancer. If we used such a data set to train an adaptive model, we could run into severe difﬁculties due to the small proportion of the cancer class. For instance, a classiﬁer that assigned every point to the normal class would already achieve 99.9% accuracy and it would be difﬁcult to avoid this trivial solution. Also, even a large data set will contain very few examples of X-ray images corresponding to cancer, and so the learning algorithm will not be exposed to a broad range of examples of such images and hence is not likely to generalize well. A balanced data set in which we have selected equal numbers of examples from each of the classes would allow us to ﬁnd a more accurate model. However, we then have to compensate for the effects of our modiﬁcations to the training data. Suppose we have used such a modiﬁed data set and found models for the posterior probabilities. From Bayes’ theorem (1.82), we see that the posterior probabilities are proportional to the prior probabilities, which we can interpret as the fractions of points in each class. We can therefore simply take the posterior probabilities obtained from our artiﬁcially balanced data set and ﬁrst divide by the class fractions in that data set and then multiply by the class fractions in the population to which we wish to apply the model. Finally, we need to normalize to ensure that the new posterior probabilities sum to one. Note that this procedure cannot be applied if we have learned a discriminant function directly instead of determining posterior probabilities.
Combining models. For complex applications, we may wish to break the problem into a number of smaller subproblems each of which can be tackled by a separate module. For example, in our hypothetical medical diagnosis problem, we may have information available from, say, blood tests as well as X-ray images. Rather than combine all of this heterogeneous information into one huge input space, it may be more effective to build one system to interpret the Xray images and a different one to interpret the blood data. As long as each of the two models gives posterior probabilities for the classes, we can combine the outputs systematically using the rules of probability. One simple way to do this is to assume that, for each class separately, the distributions of inputs for the X-ray images, denoted by xI, and the blood data, denoted by xB, are

46

1. INTRODUCTION

Section 8.2 Section 8.2.2 Section 1.1 Appendix D

independent, so that

p(xI, xB|Ck) = p(xI|Ck)p(xB|Ck).

(1.84)

This is an example of conditional independence property, because the independence holds when the distribution is conditioned on the class Ck. The posterior probability, given both the X-ray and blood data, is then given by

p(Ck|xI, xB) ∝ p(xI, xB|Ck)p(Ck)

∝ p(xI|Ck)p(xB|Ck)p(Ck)

∝

p(Ck |xI )p(Ck |xB ) p(Ck)

(1.85)

Thus we need the class prior probabilities p(Ck), which we can easily estimate from the fractions of data points in each class, and then we need to normalize the resulting posterior probabilities so they sum to one. The particular conditional independence assumption (1.84) is an example of the naive Bayes model. Note that the joint marginal distribution p(xI, xB) will typically not factorize under this model. We shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption (1.84).

1.5.5 Loss functions for regression
So far, we have discussed decision theory in the context of classiﬁcation problems. We now turn to the case of regression problems, such as the curve ﬁtting example discussed earlier. The decision stage consists of choosing a speciﬁc estimate y(x) of the value of t for each input x. Suppose that in doing so, we incur a loss L(t, y(x)). The average, or expected, loss is then given by

E[L] = L(t, y(x))p(x, t) dx dt.

(1.86)

A common choice of loss function in regression problems is the squared loss given by L(t, y(x)) = {y(x) − t}2. In this case, the expected loss can be written

E[L] = {y(x) − t}2p(x, t) dx dt.

(1.87)

Our goal is to choose y(x) so as to minimize E[L]. If we assume a completely

ﬂexible function y(x), we can do this formally using the calculus of variations to

give

δE[L] =2
δy(x)

{y(x) − t}p(x, t) dt = 0.

(1.88)

Solving for y(x), and using the sum and product rules of probability, we obtain

y(x) =

tp(x, t) dt =
p(x)

tp(t|x) dt = Et[t|x]

(1.89)

Figure 1.28 The regression function y(x),

which minimizes the expected

t

squared loss, is given by the

mean of the conditional distri-

bution p(t|x).

y(x0)

1.5. Decision Theory

47

y(x) p(t|x0)

Exercise 1.25

x0

x

which is the conditional average of t conditioned on x and is known as the regression function. This result is illustrated in Figure 1.28. It can readily be extended to multiple target variables represented by the vector t, in which case the optimal solution is the conditional average y(x) = Et[t|x].
We can also derive this result in a slightly different way, which will also shed light on the nature of the regression problem. Armed with the knowledge that the optimal solution is the conditional expectation, we can expand the square term as follows
{y(x) − t}2 = {y(x) − E[t|x] + E[t|x] − t}2 = {y(x) − E[t|x]}2 + 2{y(x) − E[t|x]}{E[t|x] − t} + {E[t|x] − t}2
where, to keep the notation uncluttered, we use E[t|x] to denote Et[t|x]. Substituting into the loss function and performing the integral over t, we see that the cross-term vanishes and we obtain an expression for the loss function in the form

E[L] = {y(x) − E[t|x]}2 p(x) dx + {E[t|x] − t}2p(x) dx.

(1.90)

The function y(x) we seek to determine enters only in the ﬁrst term, which will be minimized when y(x) is equal to E[t|x], in which case this term will vanish. This is simply the result that we derived previously and that shows that the optimal least squares predictor is given by the conditional mean. The second term is the variance of the distribution of t, averaged over x. It represents the intrinsic variability of the target data and can be regarded as noise. Because it is independent of y(x), it represents the irreducible minimum value of the loss function.
As with the classiﬁcation problem, we can either determine the appropriate probabilities and then use these to make optimal decisions, or we can build models that make decisions directly. Indeed, we can identify three distinct approaches to solving regression problems given, in order of decreasing complexity, by:
(a) First solve the inference problem of determining the joint density p(x, t). Then normalize to ﬁnd the conditional density p(t|x), and ﬁnally marginalize to ﬁnd the conditional mean given by (1.89).

48

1. INTRODUCTION

Section 5.6 Exercise 1.27

(b) First solve the inference problem of determining the conditional density p(t|x), and then subsequently marginalize to ﬁnd the conditional mean given by (1.89).

(c) Find a regression function y(x) directly from the training data.

The relative merits of these three approaches follow the same lines as for classiﬁcation problems above.
The squared loss is not the only possible choice of loss function for regression. Indeed, there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches. An important example concerns situations in which the conditional distribution p(t|x) is multimodal, as often arises in the solution of inverse problems. Here we consider brieﬂy one simple generalization of the squared loss, called the Minkowski loss, whose expectation is given by

E[Lq] = |y(x) − t|qp(x, t) dx dt

(1.91)

which reduces to the expected squared loss for q = 2. The function |y − t|q is plotted against y − t for various values of q in Figure 1.29. The minimum of E[Lq] is given by the conditional mean for q = 2, the conditional median for q = 1, and the conditional mode for q → 0.

1.6. Information Theory

Exercise 1.28

In this chapter, we have discussed a variety of concepts from probability theory and decision theory that will form the foundations for much of the subsequent discussion in this book. We close this chapter by introducing some additional concepts from the ﬁeld of information theory, which will also prove useful in our development of pattern recognition and machine learning techniques. Again, we shall focus only on the key concepts, and we refer the reader elsewhere for more detailed discussions (Viterbi and Omura, 1979; Cover and Thomas, 1991; MacKay, 2003) .
We begin by considering a discrete random variable x and we ask how much information is received when we observe a speciﬁc value for this variable. The amount of information can be viewed as the ‘degree of surprise’ on learning the value of x. If we are told that a highly improbable event has just occurred, we will have received more information than if we were told that some very likely event has just occurred, and if we knew that the event was certain to happen we would receive no information. Our measure of information content will therefore depend on the probability distribution p(x), and we therefore look for a quantity h(x) that is a monotonic function of the probability p(x) and that expresses the information content. The form of h(·) can be found by noting that if we have two events x and y that are unrelated, then the information gain from observing both of them should be the sum of the information gained from each of them separately, so that h(x, y) = h(x) + h(y). Two unrelated events will be statistically independent and so p(x, y) = p(x)p(y). From these two relationships, it is easily shown that h(x) must be given by the logarithm of p(x) and so we have

2 q = 0.3
1

1.6. Information Theory

49

2 q=1
1

|y − t|q

|y − t|q

0

−2

−1

0

1

2

y −t

2

q=2

1

0

−2

−1

0

1

2

y −t

2

q = 10

1

|y − t|q

|y − t|q

0

−2

−1

0

1

2

y −t

0

−2

−1

0

1

2

y −t

Figure 1.29 Plots of the quantity Lq = |y − t|q for various values of q.

h(x) = − log2 p(x)

(1.92)

where the negative sign ensures that information is positive or zero. Note that low probability events x correspond to high information content. The choice of basis for the logarithm is arbitrary, and for the moment we shall adopt the convention prevalent in information theory of using logarithms to the base of 2. In this case, as we shall see shortly, the units of h(x) are bits (‘binary digits’).
Now suppose that a sender wishes to transmit the value of a random variable to a receiver. The average amount of information that they transmit in the process is obtained by taking the expectation of (1.92) with respect to the distribution p(x) and is given by

H[x] = − p(x) log2 p(x).
x

(1.93)

This important quantity is called the entropy of the random variable x. Note that limp→0 p ln p = 0 and so we shall take p(x) ln p(x) = 0 whenever we encounter a value for x such that p(x) = 0.
So far we have given a rather heuristic motivation for the deﬁnition of informa-

50

1. INTRODUCTION

tion (1.92) and the corresponding entropy (1.93). We now show that these deﬁnitions indeed possess useful properties. Consider a random variable x having 8 possible states, each of which is equally likely. In order to communicate the value of x to a receiver, we would need to transmit a message of length 3 bits. Notice that the entropy of this variable is given by

11 H[x] = −8 × 8 log2 8 = 3 bits.

Now consider an example (Cover and Thomas, 1991) of a variable having 8 pos-

sible states {a, b, c, d, e, f, g, h} for which the respective probabilities are given by

(

1 2

,

1 4

,

1 8

,

1 16

,

1 64

,

1 64

,

1 64

,

1 64

).

The

entropy

in

this

case

is

given

by

1 11 11 1 1

14

1

H[x] = − 2 log2 2 − 4 log2 4 − 8 log2 8 − 16 log2 16 − 64 log2 64 = 2 bits.

We see that the nonuniform distribution has a smaller entropy than the uniform one, and we shall gain some insight into this shortly when we discuss the interpretation of entropy in terms of disorder. For the moment, let us consider how we would transmit the identity of the variable’s state to a receiver. We could do this, as before, using a 3-bit number. However, we can take advantage of the nonuniform distribution by using shorter codes for the more probable events, at the expense of longer codes for the less probable events, in the hope of getting a shorter average code length. This can be done by representing the states {a, b, c, d, e, f, g, h} using, for instance, the following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111. The average length of the code that has to be transmitted is then

average

code

length

=

1 2

×

1

+

1 4

×

2

+

1 8

×

3

+

1 16

×

4

+

4

×

1 64

×

6

=

2

bits

which again is the same as the entropy of the random variable. Note that shorter code strings cannot be used because it must be possible to disambiguate a concatenation of such strings into its component parts. For instance, 11001110 decodes uniquely into the state sequence c, a, d.
This relation between entropy and shortest coding length is a general one. The noiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable.
From now on, we shall switch to the use of natural logarithms in deﬁning entropy, as this will provide a more convenient link with ideas elsewhere in this book. In this case, the entropy is measured in units of ‘nats’ instead of bits, which differ simply by a factor of ln 2.
We have introduced the concept of entropy in terms of the average amount of information needed to specify the state of a random variable. In fact, the concept of entropy has much earlier origins in physics where it was introduced in the context of equilibrium thermodynamics and later given a deeper interpretation as a measure of disorder through developments in statistical mechanics. We can understand this alternative view of entropy by considering a set of N identical objects that are to be divided amongst a set of bins, such that there are ni objects in the ith bin. Consider

Appendix E

1.6. Information Theory

51

the number of different ways of allocating the objects to the bins. There are N ways to choose the ﬁrst object, (N − 1) ways to choose the second object, and so on, leading to a total of N ! ways to allocate all N objects to the bins, where N ! (pronounced ‘factorial N ’) denotes the product N × (N − 1) × · · · × 2 × 1. However,
we don’t wish to distinguish between rearrangements of objects within each bin. In the ith bin there are ni! ways of reordering the objects, and so the total number of ways of allocating the N objects to the bins is given by

N! W=
i ni!

(1.94)

which is called the multiplicity. The entropy is then deﬁned as the logarithm of the multiplicity scaled by an appropriate constant

1

1

1

H = ln W = ln N ! −

N

N

N

ln ni!.

i

(1.95)

We now consider the limit N → ∞, in which the fractions ni/N are held ﬁxed, and apply Stirling’s approximation

ln N ! N ln N − N

(1.96)

which gives

H = − lim
N →∞ i

ni ln ni = −

N

N

pi ln pi

i

(1.97)

where we have used i ni = N . Here pi = limN→∞(ni/N ) is the probability of an object being assigned to the ith bin. In physics terminology, the speciﬁc ar-
rangements of objects in the bins is called a microstate, and the overall distribution of occupation numbers, expressed through the ratios ni/N , is called a macrostate. The multiplicity W is also known as the weight of the macrostate.
We can interpret the bins as the states xi of a discrete random variable X, where p(X = xi) = pi. The entropy of the random variable X is then

H[p] = − p(xi) ln p(xi).
i

(1.98)

Distributions p(xi) that are sharply peaked around a few values will have a relatively low entropy, whereas those that are spread more evenly across many values will
have higher entropy, as illustrated in Figure 1.30. Because 0 pi 1, the entropy is nonnegative, and it will equal its minimum value of 0 when one of the pi = 1 and all other pj=i = 0. The maximum entropy conﬁguration can be found by maximizing H using a Lagrange multiplier to enforce the normalization constraint
on the probabilities. Thus we maximize

H = − p(xi) ln p(xi) + λ

p(xi) − 1

i

i

(1.99)

52

1. INTRODUCTION

0.5

H = 1.77

0.25

0.5 H = 3.09
0.25

probabilities probabilities

0

0

Figure 1.30 Histograms of two probability distributions over 30 bins illustrating the higher value of the entropy H for the broader distribution. The largest entropy would arise from a uniform distribution that would give H = − ln(1/30) = 3.40.

Exercise 1.29

from which we ﬁnd that all of the p(xi) are equal and are given by p(xi) = 1/M where M is the total number of states xi. The corresponding value of the entropy is then H = ln M . This result can also be derived from Jensen’s inequality (to be
discussed shortly). To verify that the stationary point is indeed a maximum, we can
evaluate the second derivative of the entropy, which gives

∂H

1

∂p(xi)∂p(xj) = −Iij pi

(1.100)

where Iij are the elements of the identity matrix. We can extend the deﬁnition of entropy to include distributions p(x) over con-
tinuous variables x as follows. First divide x into bins of width ∆. Then, assuming p(x) is continuous, the mean value theorem (Weisstein, 1999) tells us that, for each such bin, there must exist a value xi such that

(i+1)∆
p(x) dx = p(xi)∆.
i∆

(1.101)

We can now quantize the continuous variable x by assigning any value x to the value xi whenever x falls in the ith bin. The probability of observing the value xi is then p(xi)∆. This gives a discrete distribution for which the entropy takes the form

H∆ = − p(xi)∆ ln (p(xi)∆) = − p(xi)∆ ln p(xi) − ln ∆

i

i

(1.102)

where we have used i p(xi)∆ = 1, which follows from (1.101). We now omit the second term − ln ∆ on the right-hand side of (1.102) and then consider the limit

1.6. Information Theory

53

∆ → 0. The ﬁrst term on the right-hand side of (1.102) will approach the integral of p(x) ln p(x) in this limit so that

lim
∆→0

p(xi)∆ ln p(xi) = − p(x) ln p(x) dx

i

(1.103)

where the quantity on the right-hand side is called the differential entropy. We see that the discrete and continuous forms of the entropy differ by a quantity ln ∆, which diverges in the limit ∆ → 0. This reﬂects the fact that to specify a continuous variable very precisely requires a large number of bits. For a density deﬁned over multiple continuous variables, denoted collectively by the vector x, the differential entropy is given by

H[x] = − p(x) ln p(x) dx.

(1.104)

In the case of discrete distributions, we saw that the maximum entropy conﬁguration corresponded to an equal distribution of probabilities across the possible states of the variable. Let us now consider the maximum entropy conﬁguration for a continuous variable. In order for this maximum to be well deﬁned, it will be necessary to constrain the ﬁrst and second moments of p(x) as well as preserving the normalization constraint. We therefore maximize the differential entropy with the

Ludwig Boltzmann
1844–1906
Ludwig Eduard Boltzmann was an Austrian physicist who created the ﬁeld of statistical mechanics. Prior to Boltzmann, the concept of entropy was already known from classical thermodynamics where it quantiﬁes the fact that when we take energy from a system, not all of that energy is typically available to do useful work. Boltzmann showed that the thermodynamic entropy S, a macroscopic quantity, could be related to the statistical properties at the microscopic level. This is expressed through the famous equation S = k ln W in which W represents the number of possible microstates in a macrostate, and k 1.38 × 10−23 (in units of Joules per Kelvin) is known as Boltzmann’s constant. Boltzmann’s ideas were disputed by many scientists of they day. One difﬁculty they saw arose from the second law of thermo-

dynamics, which states that the entropy of a closed system tends to increase with time. By contrast, at the microscopic level the classical Newtonian equations of physics are reversible, and so they found it difﬁcult to see how the latter could explain the former. They didn’t fully appreciate Boltzmann’s arguments, which were statistical in nature and which concluded not that entropy could never decrease over time but simply that with overwhelming probability it would generally increase. Boltzmann even had a longrunning dispute with the editor of the leading German physics journal who refused to let him refer to atoms and molecules as anything other than convenient theoretical constructs. The continued attacks on his work lead to bouts of depression, and eventually he committed suicide. Shortly after Boltzmann’s death, new experiments by Perrin on colloidal suspensions veriﬁed his theories and conﬁrmed the value of the Boltzmann constant. The equation S = k ln W is carved on Boltzmann’s tombstone.

54

1. INTRODUCTION

Appendix E Appendix D Exercise 1.34 Exercise 1.35

three constraints

∞
p(x) dx = 1
−∞ ∞
xp(x) dx = µ
−∞ ∞
(x − µ)2p(x) dx = σ2.
−∞

(1.105) (1.106) (1.107)

The constrained maximization can be performed using Lagrange multipliers so that we maximize the following functional with respect to p(x)

∞

∞

− p(x) ln p(x) dx + λ1

p(x) dx − 1

−∞

−∞

∞

∞

+λ2

xp(x) dx − µ + λ3

(x − µ)2p(x) dx − σ2 .

−∞

−∞

Using the calculus of variations, we set the derivative of this functional to zero giving

p(x) = exp −1 + λ1 + λ2x + λ3(x − µ)2 .

(1.108)

The Lagrange multipliers can be found by back substitution of this result into the three constraint equations, leading ﬁnally to the result

1 p(x) = (2πσ2)1/2 exp

− (x − µ)2 2σ2

(1.109)

and so the distribution that maximizes the differential entropy is the Gaussian. Note that we did not constrain the distribution to be nonnegative when we maximized the entropy. However, because the resulting distribution is indeed nonnegative, we see with hindsight that such a constraint is not necessary.
If we evaluate the differential entropy of the Gaussian, we obtain

H[x] = 1 1 + ln(2πσ2) . 2

(1.110)

Thus we see again that the entropy increases as the distribution becomes broader, i.e., as σ2 increases. This result also shows that the differential entropy, unlike the discrete entropy, can be negative, because H(x) < 0 in (1.110) for σ2 < 1/(2πe).
Suppose we have a joint distribution p(x, y) from which we draw pairs of values
of x and y. If a value of x is already known, then the additional information needed to specify the corresponding value of y is given by − ln p(y|x). Thus the average additional information needed to specify y can be written as

H[y|x] = − p(y, x) ln p(y|x) dy dx

(1.111)

Exercise 1.37

1.6. Information Theory

55

which is called the conditional entropy of y given x. It is easily seen, using the product rule, that the conditional entropy satisﬁes the relation

H[x, y] = H[y|x] + H[x]

(1.112)

where H[x, y] is the differential entropy of p(x, y) and H[x] is the differential entropy of the marginal distribution p(x). Thus the information needed to describe x and y is given by the sum of the information needed to describe x alone plus the additional information required to specify y given x.

1.6.1 Relative entropy and mutual information
So far in this section, we have introduced a number of concepts from information theory, including the key notion of entropy. We now start to relate these ideas to pattern recognition. Consider some unknown distribution p(x), and suppose that we have modelled this using an approximating distribution q(x). If we use q(x) to construct a coding scheme for the purpose of transmitting values of x to a receiver, then the average additional amount of information (in nats) required to specify the value of x (assuming we choose an efﬁcient coding scheme) as a result of using q(x) instead of the true distribution p(x) is given by

KL(p q) = − p(x) ln q(x) dx − − p(x) ln p(x) dx

=−

p(x) ln q(x) p(x)

dx.

(1.113)

This is known as the relative entropy or Kullback-Leibler divergence, or KL divergence (Kullback and Leibler, 1951), between the distributions p(x) and q(x). Note that it is not a symmetrical quantity, that is to say KL(p q) ≡ KL(q p).
We now show that the Kullback-Leibler divergence satisﬁes KL(p q) 0 with equality if, and only if, p(x) = q(x). To do this we ﬁrst introduce the concept of convex functions. A function f (x) is said to be convex if it has the property that every chord lies on or above the function, as shown in Figure 1.31. Any value of x in the interval from x = a to x = b can be written in the form λa + (1 − λ)b where 0 λ 1. The corresponding point on the chord is given by λf (a) + (1 − λ)f (b),

Claude Shannon
1916–2001
After graduating from Michigan and MIT, Shannon joined the AT&T Bell Telephone laboratories in 1941. His paper ‘A Mathematical Theory of Communication’ published in the Bell System Technical Journal in 1948 laid the foundations for modern information the-

ory. This paper introduced the word ‘bit’, and his concept that information could be sent as a stream of 1s and 0s paved the way for the communications revolution. It is said that von Neumann recommended to Shannon that he use the term entropy, not only because of its similarity to the quantity used in physics, but also because “nobody knows what entropy really is, so in any discussion you will always have an advantage”.

56

1. INTRODUCTION

Figure 1.31 A convex function f (x) is one for which every chord (shown in blue) lies on or above the function (shown in red).

chord

f (x)

Exercise 1.36 Exercise 1.38

a

xλ

b

x

and the corresponding value of the function is f (λa + (1 − λ)b). Convexity then

implies

f (λa + (1 − λ)b) λf (a) + (1 − λ)f (b).

(1.114)

This is equivalent to the requirement that the second derivative of the function be everywhere positive. Examples of convex functions are x ln x (for x > 0) and x2. A function is called strictly convex if the equality is satisﬁed only for λ = 0 and λ = 1.
If a function has the opposite property, namely that every chord lies on or below the
function, it is called concave, with a corresponding deﬁnition for strictly concave. If a function f (x) is convex, then −f (x) will be concave.
Using the technique of proof by induction, we can show from (1.114) that a convex function f (x) satisﬁes

M

f

λixi

i=1

M
λif (xi)
i=1

(1.115)

where λi 0 and i λi = 1, for any set of points {xi}. The result (1.115) is known as Jensen’s inequality. If we interpret the λi as the probability distribution over a discrete variable x taking the values {xi}, then (1.115) can be written

f (E[x]) E[f (x)]

(1.116)

where E[·] denotes the expectation. For continuous variables, Jensen’s inequality takes the form

f xp(x) dx

f (x)p(x) dx.

(1.117)

We can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler divergence (1.113) to give

KL(p q) = −

p(x) ln q(x) dx p(x)

− ln

q(x) dx = 0

(1.118)

Exercise 1.41

1.6. Information Theory

57

where we have used the fact that − ln x is a convex function, together with the normalization condition q(x) dx = 1. In fact, − ln x is a strictly convex function, so the equality will hold if, and only if, q(x) = p(x) for all x. Thus we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and q(x).
We see that there is an intimate relationship between data compression and density estimation (i.e., the problem of modelling an unknown probability distribution) because the most efﬁcient compression is achieved when we know the true distri-

bution. If we use a distribution that is different from the true one, then we must necessarily have a less efﬁcient coding, and on average the additional information that must be transmitted is (at least) equal to the Kullback-Leibler divergence be-

tween the two distributions. Suppose that data is being generated from an unknown distribution p(x) that we
wish to model. We can try to approximate this distribution using some parametric distribution q(x|θ), governed by a set of adjustable parameters θ, for example a multivariate Gaussian. One way to determine θ is to minimize the Kullback-Leibler divergence between p(x) and q(x|θ) with respect to θ. We cannot do this directly because we don’t know p(x). Suppose, however, that we have observed a ﬁnite set of training points xn, for n = 1, . . . , N , drawn from p(x). Then the expectation with respect to p(x) can be approximated by a ﬁnite sum over these points, using
(1.35), so that
N

KL(p q)

{− ln q(xn|θ) + ln p(xn)} .

(1.119)

n=1

The second term on the right-hand side of (1.119) is independent of θ, and the ﬁrst term is the negative log likelihood function for θ under the distribution q(x|θ) evaluated using the training set. Thus we see that minimizing this Kullback-Leibler

divergence is equivalent to maximizing the likelihood function. Now consider the joint distribution between two sets of variables x and y given
by p(x, y). If the sets of variables are independent, then their joint distribution will factorize into the product of their marginals p(x, y) = p(x)p(y). If the variables are not independent, we can gain some idea of whether they are ‘close’ to being independent by considering the Kullback-Leibler divergence between the joint distribution and the product of the marginals, given by

I[x, y] ≡ KL(p(x, y) p(x)p(y))

=−

p(x)p(y) p(x, y) ln
p(x, y)

dx dy

(1.120)

which is called the mutual information between the variables x and y. From the

properties of the Kullback-Leibler divergence, we see that I(x, y) 0 with equal-

ity if, and only if, x and y are independent. Using the sum and product rules of

probability, we see that the mutual information is related to the conditional entropy

through

I[x, y] = H[x] − H[x|y] = H[y] − H[y|x].

(1.121)

58

1. INTRODUCTION

Thus we can view the mutual information as the reduction in the uncertainty about x by virtue of being told the value of y (or vice versa). From a Bayesian perspective, we can view p(x) as the prior distribution for x and p(x|y) as the posterior distribution after we have observed new data y. The mutual information therefore represents the reduction in uncertainty about x as a consequence of the new observation y.

Exercises
1.1

( ) www Consider the sum-of-squares error function given by (1.2) in which

the function y(x, w) is given by the polynomial (1.1). Show that the coefﬁcients

w = {wi} that minimize this error function are given by the solution to the following set of linear equations
M

Aij wj = Ti

(1.122)

j=0

where

N
Aij = (xn)i+j ,

N
Ti = (xn)itn.

(1.123)

n=1

n=1

Here a sufﬁx i or j denotes the index of a component, whereas (x)i denotes x raised to the power of i.

1.2 ( ) Write down the set of coupled linear equations, analogous to (1.122), satisﬁed by the coefﬁcients wi which minimize the regularized sum-of-squares error function given by (1.4).

1.3 ( ) Suppose that we have three coloured boxes r (red), b (blue), and g (green). Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange, and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes. If a box is chosen at random with probabilities p(r) = 0.2, p(b) = 0.2, p(g) = 0.6, and a piece of fruit is removed from the box (with equal probability of selecting any of the items in the box), then what is the probability of selecting an apple? If we observe that the selected fruit is in fact an orange, what is the probability that it came from the green box?

1.4 ( ) www Consider a probability density px(x) deﬁned over a continuous variable x, and suppose that we make a nonlinear change of variable using x = g(y), so that the density transforms according to (1.27). By differentiating (1.27), show that the location y of the maximum of the density in y is not in general related to the location x of the maximum of the density over x by the simple functional relation x = g(y) as a consequence of the Jacobian factor. This shows that the maximum of a probability density (in contrast to a simple function) is dependent on the choice of variable. Verify that, in the case of a linear transformation, the location of the maximum transforms in the same way as the variable itself.

1.5 ( ) Using the deﬁnition (1.38) show that var[f (x)] satisﬁes (1.39).

Exercises

59

1.6 ( ) Show that if two variables x and y are independent, then their covariance is zero.

1.7 ( ) www In this exercise, we prove the normalization condition (1.48) for the univariate Gaussian. To do this consider, the integral

I=

∞
exp
−∞

−

1 2σ2

x2

dx

(1.124)

which we can evaluate by ﬁrst writing its square in the form

I2 =

∞ −∞

∞
exp
−∞

−

1 2σ

2

x2

−

1 2σ2

y2

dx dy.

(1.125)

Now make the transformation from Cartesian coordinates (x, y) to polar coordinates (r, θ) and then substitute u = r2. Show that, by performing the integrals over θ and
u, and then taking the square root of both sides, we obtain

I = 2πσ2 1/2 .

(1.126)

Finally, use this result to show that the Gaussian distribution N (x|µ, σ2) is normalized.

1.8 ( ) www By using a change of variables, verify that the univariate Gaussian distribution given by (1.46) satisﬁes (1.49). Next, by differentiating both sides of the normalization condition

∞
N x|µ, σ2 dx = 1
−∞

(1.127)

with respect to σ2, verify that the Gaussian satisﬁes (1.50). Finally, show that (1.51) holds.

1.9 ( ) www Show that the mode (i.e. the maximum) of the Gaussian distribution (1.46) is given by µ. Similarly, show that the mode of the multivariate Gaussian (1.52) is given by µ.
1.10 ( ) www Suppose that the two variables x and z are statistically independent. Show that the mean and variance of their sum satisﬁes

E[x + z] = E[x] + E[z] var[x + z] = var[x] + var[z].

(1.128) (1.129)

1.11 ( ) By setting the derivatives of the log likelihood function (1.54) with respect to µ and σ2 equal to zero, verify the results (1.55) and (1.56).

60

1. INTRODUCTION

1.12 ( ) www Using the results (1.49) and (1.50), show that

E[xnxm] = µ2 + Inmσ2

(1.130)

where xn and xm denote data points sampled from a Gaussian distribution with mean µ and variance σ2, and Inm satisﬁes Inm = 1 if n = m and Inm = 0 otherwise. Hence prove the results (1.57) and (1.58).

1.13 ( ) Suppose that the variance of a Gaussian is estimated using the result (1.56) but
with the maximum likelihood estimate µML replaced with the true value µ of the mean. Show that this estimator has the property that its expectation is given by the true variance σ2.

1.14

( ) Show that an arbitrary square matrix with elements wij can be written in the form wij = wiSj + wiAj where wiSj and wiAj are symmetric and anti-symmetric matrices, respectively, satisfying wiSj = wjSi and wiAj = −wjAi for all i and j. Now consider the second order term in a higher order polynomial in D dimensions, given

by

DD

wij xixj .

(1.131)

i=1 j=1

Show that

DD

DD

wij xixj =

wiSj xixj

i=1 j=1

i=1 j=1

(1.132)

so that the contribution from the anti-symmetric matrix vanishes. We therefore see
that, without loss of generality, the matrix of coefﬁcients wij can be chosen to be symmetric, and so not all of the D2 elements of this matrix can be chosen independently. Show that the number of independent parameters in the matrix wiSj is given by D(D + 1)/2.

1.15 ( ) www In this exercise and the next, we explore how the number of independent parameters in a polynomial grows with the order M of the polynomial and with the dimensionality D of the input space. We start by writing down the M th order term for a polynomial in D dimensions in the form

DD

D

···

wi1i2···iM xi1 xi2 · · · xiM .

i1=1 i2=1 iM =1

(1.133)

The coefﬁcients wi1i2···iM comprise DM elements, but the number of independent parameters is signiﬁcantly fewer due to the many interchange symmetries of the factor xi1xi2 · · · xiM . Begin by showing that the redundancy in the coefﬁcients can be removed by rewriting this M th order term in the form

D i1

iM −1

···

wi1i2···iM xi1 xi2 · · · xiM .

i1=1 i2=1 iM =1

(1.134)

Exercises

61

Note that the precise relationship between the w coefﬁcients and w coefﬁcients need not be made explicit. Use this result to show that the number of independent parameters n(D, M ), which appear at order M , satisﬁes the following recursion relation

D
n(D, M ) = n(i, M − 1).
i=1
Next use proof by induction to show that the following result holds

(1.135)

D (i + M − 2)!

(D + M − 1)!

=

(i − 1)! (M − 1)! (D − 1)! M !

i=1

(1.136)

which can be done by ﬁrst proving the result for D = 1 and arbitrary M by making use of the result 0! = 1, then assuming it is correct for dimension D and verifying that it is correct for dimension D + 1. Finally, use the two previous results, together with proof by induction, to show

n(D, M )

=

(D + M − 1)! (D − 1)! M !

.

(1.137)

To do this, ﬁrst show that the result is true for M = 2, and any value of D 1,
by comparison with the result of Exercise 1.14. Then make use of (1.135), together with (1.136), to show that, if the result holds at order M − 1, then it will also hold at order M

1.16 ( ) In Exercise 1.15, we proved the result (1.135) for the number of independent parameters in the M th order term of a D-dimensional polynomial. We now ﬁnd an expression for the total number N (D, M ) of independent parameters in all of the terms up to and including the M 6th order. First show that N (D, M ) satisﬁes

M
N (D, M ) = n(D, m)

(1.138)

m=0

where n(D, m) is the number of independent parameters in the term of order m. Now make use of the result (1.137), together with proof by induction, to show that

(D + M )!

N (d, M ) =

.

D! M!

(1.139)

This can be done by ﬁrst proving that the result holds for M = 0 and arbitrary D 1, then assuming that it holds at order M , and hence showing that it holds at order M + 1. Finally, make use of Stirling’s approximation in the form

n! nne−n

(1.140)

for large n to show that, for D M , the quantity N (D, M ) grows like DM , and for M D it grows like M D. Consider a cubic (M = 3) polynomial in D
dimensions, and evaluate numerically the total number of independent parameters for (i) D = 10 and (ii) D = 100, which correspond to typical small-scale and
medium-scale machine learning applications.

62

1. INTRODUCTION

1.17 ( ) www The gamma function is deﬁned by

∞

Γ(x) ≡

ux−1e−u du.

0

(1.141)

Using integration by parts, prove the relation Γ(x + 1) = xΓ(x). Show also that Γ(1) = 1 and hence that Γ(x + 1) = x! when x is an integer.

1.18 ( ) www We can use the result (1.126) to derive an expression for the surface area SD, and the volume VD, of a sphere of unit radius in D dimensions. To do this, consider the following result, which is obtained by transforming from Cartesian to
polar coordinates

D∞

∞

e−x2i dxi = SD

e−r2 rD−1 dr.

i=1 −∞

0

(1.142)

Using the deﬁnition (1.141) of the Gamma function, together with (1.126), evaluate both sides of this equation, and hence show that

2πD/2

SD

=

. Γ(D/2)

(1.143)

Next, by integrating with respect to radius from 0 to 1, show that the volume of the unit sphere in D dimensions is given by

VD

=

SD . D

(1.144)

√ Finally, use the results Γ(1) = 1 and Γ(3/2) = π/2 to show that (1.143) and

(1.144) reduce to the usual expressions for D = 2 and D = 3.

1.19 ( ) Consider a sphere of radius a in D-dimensions together with the concentric hypercube of side 2a, so that the sphere touches the hypercube at the centres of each of its sides. By using the results of Exercise 1.18, show that the ratio of the volume of the sphere to the volume of the cube is given by

volume of sphere

πD/2

volume of cube = D2D−1Γ(D/2) .

(1.145)

Now make use of Stirling’s formula in the form

Γ(x + 1) (2π)1/2e−xxx+1/2

(1.146)

which is valid for x 1, to show that, as D → ∞, the ratio (1.145) goes to zero. Show also that the ratio of the distance from the centre of the hypercub√e to one of the corners, divided by the perpendicular distance to one of the sides, is D, which therefore goes to ∞ as D → ∞. From these results we see that, in a space of high dimensionality, most of the volume of a cube is concentrated in the large number of corners, which themselves become very long ‘spikes’!

Exercises

63

1.20 ( ) www In this exercise, we explore the behaviour of the Gaussian distribution

in high-dimensional spaces. Consider a Gaussian distribution in D dimensions given

by

1

x2

p(x) = (2πσ2)D/2 exp − 2σ2 .

(1.147)

We wish to ﬁnd the density with respect to radius in polar coordinates in which the direction variables have been integrated out. To do this, show that the integral of the probability density over a thin shell of radius r and thickness , where 1, is given by p(r) where

p(r)

=

SD rD−1 (2πσ2)D/2

exp

r2 − 2σ2

(1.148)

where SD is the surface area of a unit sphere in D dimensions. √Show that the function p(r) has a single stationary point located, for large D, at r Dσ. By considering p(r + ) where r, show that for large D,

32 p(r + ) = p(r) exp − 2σ2

(1.149)

which shows that r is a maximum of the radial probability density and also that p(r) decays exponentially away from its maximum at r with length scale σ. We have already seen that σ r for large D, and so we see that most of the probability mass is concentrated in a thin shell at large radius. Finally, show that the probability density p(x) is larger at the origin than at the radius r by a factor of exp(D/2). We therefore see that most of the probability mass in a high-dimensional Gaussian distribution is located at a different radius from the region of high probability density. This property of distributions in spaces of high dimensionality will have important consequences when we consider Bayesian inference of model parameters in later chapters.

1.21 ( ) Consider two nonnegative numbers a and b, and show that, if a b, then a (ab)1/2. Use this result to show that, if the decision regions of a two-class
classiﬁcation problem are chosen to minimize the probability of misclassiﬁcation,
this probability will satisfy

p(mistake)

{p(x, C1)p(x, C2)}1/2 dx.

(1.150)

1.22 ( ) www Given a loss matrix with elements Lkj, the expected risk is minimized if, for each x, we choose the class that minimizes (1.81). Verify that, when the loss matrix is given by Lkj = 1 − Ikj, where Ikj are the elements of the identity matrix, this reduces to the criterion of choosing the class having the largest posterior probability. What is the interpretation of this form of loss matrix?
1.23 ( ) Derive the criterion for minimizing the expected loss when there is a general loss matrix and general prior probabilities for the classes.

64

1. INTRODUCTION

1.24 ( ) www Consider a classiﬁcation problem in which the loss incurred when an input vector from class Ck is classiﬁed as belonging to class Cj is given by the loss matrix Lkj, and for which the loss incurred in selecting the reject option is λ. Find the decision criterion that will give the minimum expected loss. Verify that this
reduces to the reject criterion discussed in Section 1.5.3 when the loss matrix is given by Lkj = 1 − Ikj. What is the relationship between λ and the rejection threshold θ?

1.25 ( ) www Consider the generalization of the squared loss function (1.87) for a single target variable t to the case of multiple target variables described by the vector t given by

E[L(t, y(x))] =

y(x) − t 2p(x, t) dx dt.

(1.151)

Using the calculus of variations, show that the function y(x) for which this expected loss is minimized is given by y(x) = Et[t|x]. Show that this result reduces to (1.89) for the case of a single target variable t.
1.26 ( ) By expansion of the square in (1.151), derive a result analogous to (1.90) and hence show that the function y(x) that minimizes the expected squared loss for the case of a vector t of target variables is again given by the conditional expectation of t.
1.27 ( ) www Consider the expected loss for regression problems under the Lq loss function given by (1.91). Write down the condition that y(x) must satisfy in order to minimize E[Lq]. Show that, for q = 1, this solution represents the conditional median, i.e., the function y(x) such that the probability mass for t < y(x) is the same as for t y(x). Also show that the minimum expected Lq loss for q → 0 is given by the conditional mode, i.e., by the function y(x) equal to the value of t that maximizes p(t|x) for each x.
1.28 ( ) In Section 1.6, we introduced the idea of entropy h(x) as the information gained on observing the value of a random variable x having distribution p(x). We saw that, for independent variables x and y for which p(x, y) = p(x)p(y), the entropy functions are additive, so that h(x, y) = h(x) + h(y). In this exercise, we derive the relation between h and p in the form of a function h(p). First show that h(p2) = 2h(p), and hence by induction that h(pn) = nh(p) where n is a positive integer. Hence show that h(pn/m) = (n/m)h(p) where m is also a positive integer. This implies that h(px) = xh(p) where x is a positive rational number, and hence by continuity when it is a positive real number. Finally, show that this implies h(p) must take the form h(p) ∝ ln p.
1.29 ( ) www Consider an M -state discrete random variable x, and use Jensen’s inequality in the form (1.115) to show that the entropy of its distribution p(x) satisﬁes H[x] ln M .
1.30 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians p(x) = N (x|µ, σ2) and q(x) = N (x|m, s2).

Table 1.3 The joint distribution p(x, y) for two binary variables x and y used in Exercise 1.39.

Exercises

65

y 01 x 0 1/3 1/3 1 0 1/3

1.31 ( ) www Consider two variables x and y having joint distribution p(x, y). Show that the differential entropy of this pair of variables satisﬁes

H[x, y] H[x] + H[y]

(1.152)

with equality if, and only if, x and y are statistically independent.
1.32 ( ) Consider a vector x of continuous variables with distribution p(x) and corresponding entropy H[x]. Suppose that we make a nonsingular linear transformation of x to obtain a new variable y = Ax. Show that the corresponding entropy is given by H[y] = H[x] + ln |A| where |A| denotes the determinant of A.
1.33 ( ) Suppose that the conditional entropy H[y|x] between two discrete random variables x and y is zero. Show that, for all values of x such that p(x) > 0, the variable y must be a function of x, in other words for each x there is only one value of y such that p(y|x) = 0.
1.34 ( ) www Use the calculus of variations to show that the stationary point of the functional (1.108) is given by (1.108). Then use the constraints (1.105), (1.106), and (1.107) to eliminate the Lagrange multipliers and hence show that the maximum entropy solution is given by the Gaussian (1.109).
1.35 ( ) www Use the results (1.106) and (1.107) to show that the entropy of the univariate Gaussian (1.109) is given by (1.110).
1.36 ( ) A strictly convex function is deﬁned as one for which every chord lies above the function. Show that this is equivalent to the condition that the second derivative of the function be positive.
1.37 ( ) Using the deﬁnition (1.111) together with the product rule of probability, prove the result (1.112).
1.38 ( ) www Using proof by induction, show that the inequality (1.114) for convex functions implies the result (1.115).
1.39 ( ) Consider two binary variables x and y having the joint distribution given in Table 1.3.
Evaluate the following quantities

(a) H[x] (b) H[y]

(c) H[y|x] (d) H[x|y]

(e) H[x, y] (f) I[x, y].

Draw a diagram to show the relationship between these various quantities.

66

1. INTRODUCTION

1.40 ( ) By applying Jensen’s inequality (1.115) with f (x) = ln x, show that the arithmetic mean of a set of real numbers is never less than their geometrical mean.
1.41 ( ) www Using the sum and product rules of probability, show that the mutual information I(x, y) satisﬁes the relation (1.121).

2
Probability Distributions
In Chapter 1, we emphasized the central role played by probability theory in the solution of pattern recognition problems. We turn now to an exploration of some particular examples of probability distributions and their properties. As well as being of great interest in their own right, these distributions can form building blocks for more complex models and will be used extensively throughout the book. The distributions introduced in this chapter will also serve another important purpose, namely to provide us with the opportunity to discuss some key statistical concepts, such as Bayesian inference, in the context of simple models before we encounter them in more complex situations in later chapters.
One role for the distributions discussed in this chapter is to model the probability distribution p(x) of a random variable x, given a ﬁnite set x1, . . . , xN of observations. This problem is known as density estimation. For the purposes of this chapter, we shall assume that the data points are independent and identically distributed. It should be emphasized that the problem of density estimation is fun-
67

68

2. PROBABILITY DISTRIBUTIONS

damentally ill-posed, because there are inﬁnitely many probability distributions that could have given rise to the observed ﬁnite data set. Indeed, any distribution p(x) that is nonzero at each of the data points x1, . . . , xN is a potential candidate. The issue of choosing an appropriate distribution relates to the problem of model selection that has already been encountered in the context of polynomial curve ﬁtting in Chapter 1 and that is a central issue in pattern recognition.
We begin by considering the binomial and multinomial distributions for discrete random variables and the Gaussian distribution for continuous random variables. These are speciﬁc examples of parametric distributions, so-called because they are governed by a small number of adaptive parameters, such as the mean and variance in the case of a Gaussian for example. To apply such models to the problem of density estimation, we need a procedure for determining suitable values for the parameters, given an observed data set. In a frequentist treatment, we choose speciﬁc values for the parameters by optimizing some criterion, such as the likelihood function. By contrast, in a Bayesian treatment we introduce prior distributions over the parameters and then use Bayes’ theorem to compute the corresponding posterior distribution given the observed data.
We shall see that an important role is played by conjugate priors, that lead to posterior distributions having the same functional form as the prior, and that therefore lead to a greatly simpliﬁed Bayesian analysis. For example, the conjugate prior for the parameters of the multinomial distribution is called the Dirichlet distribution, while the conjugate prior for the mean of a Gaussian is another Gaussian. All of these distributions are examples of the exponential family of distributions, which possess a number of important properties, and which will be discussed in some detail.
One limitation of the parametric approach is that it assumes a speciﬁc functional form for the distribution, which may turn out to be inappropriate for a particular application. An alternative approach is given by nonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set. Such models still contain parameters, but these control the model complexity rather than the form of the distribution. We end this chapter by considering three nonparametric methods based respectively on histograms, nearest-neighbours, and kernels.

2.1. Binary Variables

We begin by considering a single binary random variable x ∈ {0, 1}. For example, x might describe the outcome of ﬂipping a coin, with x = 1 representing ‘heads’, and x = 0 representing ‘tails’. We can imagine that this is a damaged coin so that the probability of landing heads is not necessarily the same as that of landing tails. The probability of x = 1 will be denoted by the parameter µ so that

p(x = 1|µ) = µ

(2.1)

Exercise 2.1 Section 2.4

2.1. Binary Variables

69

where 0 µ 1, from which it follows that p(x = 0|µ) = 1 − µ. The probability distribution over x can therefore be written in the form

Bern(x|µ) = µx(1 − µ)1−x

(2.2)

which is known as the Bernoulli distribution. It is easily veriﬁed that this distribution is normalized and that it has mean and variance given by

E[x] = µ

(2.3)

var[x] = µ(1 − µ).

(2.4)

Now suppose we have a data set D = {x1, . . . , xN } of observed values of x. We can construct the likelihood function, which is a function of µ, on the assumption
that the observations are drawn independently from p(x|µ), so that

N

N

p(D|µ) = p(xn|µ) = µxn (1 − µ)1−xn .

n=1

n=1

(2.5)

In a frequentist setting, we can estimate a value for µ by maximizing the likelihood function, or equivalently by maximizing the logarithm of the likelihood. In the case of the Bernoulli distribution, the log likelihood function is given by

N

N

ln p(D|µ) = ln p(xn|µ) = {xn ln µ + (1 − xn) ln(1 − µ)} .

n=1

n=1

(2.6)

At this point, it is worth noting that the log likelihood function depends on the N observations xn only through their sum n xn. This sum provides an example of a sufﬁcient statistic for the data under this distribution, and we shall study the important role of sufﬁcient statistics in some detail. If we set the derivative of ln p(D|µ) with respect to µ equal to zero, we obtain the maximum likelihood estimator

1N

µML = N

xn

n=1

(2.7)

Jacob Bernoulli
1654–1705
Jacob Bernoulli, also known as Jacques or James Bernoulli, was a Swiss mathematician and was the ﬁrst of many in the Bernoulli family to pursue a career in science and mathematics. Although compelled to study philosophy and theology against his will by his parents, he travelled extensively after graduating in order to meet with many of the leading scientists of

his time, including Boyle and Hooke in England. When he returned to Switzerland, he taught mechanics and became Professor of Mathematics at Basel in 1687. Unfortunately, rivalry between Jacob and his younger brother Johann turned an initially productive collaboration into a bitter and public dispute. Jacob’s most signiﬁcant contributions to mathematics appeared in The Art of Conjecture published in 1713, eight years after his death, which deals with topics in probability theory including what has become known as the Bernoulli distribution.

70

2. PROBABILITY DISTRIBUTIONS

Figure 2.1

Histogram plot of the binomial distribution (2.9) as a function of m for

0.3

N = 10 and µ = 0.25.

0.2

0.1

Exercise 2.3

0 0 1 2 3 4 5 6 7 8 9 10 m

which is also known as the sample mean. If we denote the number of observations of x = 1 (heads) within this data set by m, then we can write (2.7) in the form

m µML = N

(2.8)

so that the probability of landing heads is given, in this maximum likelihood frame-

work, by the fraction of observations of heads in the data set.

Now suppose we ﬂip a coin, say, 3 times and happen to observe 3 heads. Then

N = m = 3 and µML = 1. In this case, the maximum likelihood result would predict that all future observations should give heads. Common sense tells us that

this is unreasonable, and in fact this is an extreme example of the over-ﬁtting associ-

ated with maximum likelihood. We shall see shortly how to arrive at more sensible

conclusions through the introduction of a prior distribution over µ.

We can also work out the distribution of the number m of observations of x = 1,

given that the data set has size N . This is called the binomial distribution, and from (2.5) we see that it is proportional to µm(1 − µ)N−m. In order to obtain the

normalization coefﬁcient we note that out of N coin ﬂips, we have to add up all

of the possible ways of obtaining m heads, so that the binomial distribution can be

written

Bin(m|N, µ) = N µm(1 − µ)N−m m

(2.9)

where

N m

≡

(N

N! − m)!m!

(2.10)

is the number of ways of choosing m objects out of a total of N identical objects. Figure 2.1 shows a plot of the binomial distribution for N = 10 and µ = 0.25.
The mean and variance of the binomial distribution can be found by using the result of Exercise 1.10, which shows that for independent events the mean of the sum is the sum of the means, and the variance of the sum is the sum of the variances. Because m = x1 + . . . + xN , and for each observation the mean and variance are

Exercise 2.4
Exercise 2.5 Exercise 2.6

2.1. Binary Variables

71

given by (2.3) and (2.4), respectively, we have

N
E[m] ≡ mBin(m|N, µ) =
m=0 N
var[m] ≡ (m − E[m])2 Bin(m|N, µ) =
m=0
These results can also be proved directly using calculus.

Nµ N µ(1 − µ).

(2.11) (2.12)

2.1.1 The beta distribution

We have seen in (2.8) that the maximum likelihood setting for the parameter µ in the Bernoulli distribution, and hence in the binomial distribution, is given by the fraction of the observations in the data set having x = 1. As we have already noted, this can give severely over-ﬁtted results for small data sets. In order to develop a Bayesian treatment for this problem, we need to introduce a prior distribution p(µ) over the parameter µ. Here we consider a form of prior distribution that has a simple interpretation as well as some useful analytical properties. To motivate this prior, we note that the likelihood function takes the form of the product of factors of the form µx(1 − µ)1−x. If we choose a prior to be proportional to powers of µ and (1 − µ), then the posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same functional form as the prior. This property is called conjugacy and we will see several examples of it later in this chapter. We therefore choose a prior, called the beta distribution, given by

Beta(µ|a, b) = Γ(a + b) µa−1(1 − µ)b−1 Γ(a)Γ(b)

(2.13)

where Γ(x) is the gamma function deﬁned by (1.141), and the coefﬁcient in (2.13) ensures that the beta distribution is normalized, so that

1
Beta(µ|a, b) dµ = 1.

0

The mean and variance of the beta distribution are given by

E[µ] = a a+b

ab

var[µ] =

.

(a + b)2(a + b + 1)

(2.14)
(2.15) (2.16)

The parameters a and b are often called hyperparameters because they control the distribution of the parameter µ. Figure 2.2 shows plots of the beta distribution for various values of the hyperparameters.
The posterior distribution of µ is now obtained by multiplying the beta prior (2.13) by the binomial likelihood function (2.9) and normalizing. Keeping only the factors that depend on µ, we see that this posterior distribution has the form

p(µ|m, l, a, b) ∝ µm+a−1(1 − µ)l+b−1

(2.17)

72

2. PROBABILITY DISTRIBUTIONS

3 a = 0.1
b = 0.1 2

3 a=1
b=1 2

1

1

0

0

0

0.5

µ

1

0

0.5

µ

1

3 a=2

3 a=8

b=3 2

b=4 2

1

1

0

0

0

0.5

µ

1

0

0.5

µ

1

Figure 2.2 Plots of the beta distribution Beta(µ|a, b) given by (2.13) as a function of µ for various values of the hyperparameters a and b.

where l = N − m, and therefore corresponds to the number of ‘tails’ in the coin example. We see that (2.17) has the same functional dependence on µ as the prior distribution, reﬂecting the conjugacy properties of the prior with respect to the likelihood function. Indeed, it is simply another beta distribution, and its normalization coefﬁcient can therefore be obtained by comparison with (2.13) to give

p(µ|m, l, a, b) = Γ(m + a + l + b) µm+a−1(1 − µ)l+b−1. Γ(m + a)Γ(l + b)

(2.18)

We see that the effect of observing a data set of m observations of x = 1 and l observations of x = 0 has been to increase the value of a by m, and the value of b by l, in going from the prior distribution to the posterior distribution. This allows us to provide a simple interpretation of the hyperparameters a and b in the prior as an effective number of observations of x = 1 and x = 0, respectively. Note that a and b need not be integers. Furthermore, the posterior distribution can act as the prior if we subsequently observe additional data. To see this, we can imagine taking observations one at a time and after each observation updating the current posterior

2.1. Binary Variables

73

2 prior 1

2 likelihood function 1

2 posterior 1

0

0

0

0

0.5

1

0

0.5

1

0

0.5

1

µ

µ

µ

Figure 2.3 Illustration of one step of sequential Bayesian inference. The prior is given by a beta distribution with parameters a = 2, b = 2, and the likelihood function, given by (2.9) with N = m = 1, corresponds to a single observation of x = 1, so that the posterior is given by a beta distribution with parameters a = 3, b = 2.

Section 2.3.5

distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new, revised posterior distribution. At each stage, the posterior is a beta distribution with some total number of (prior and actual) observed values for x = 1 and x = 0 given by the parameters a and b. Incorporation of an additional observation of x = 1 simply corresponds to incrementing the value of a by 1, whereas for an observation of x = 0 we increment b by 1. Figure 2.3 illustrates one step in this process.
We see that this sequential approach to learning arises naturally when we adopt a Bayesian viewpoint. It is independent of the choice of prior and of the likelihood function and depends only on the assumption of i.i.d. data. Sequential methods make use of observations one at a time, or in small batches, and then discard them before the next observations are used. They can be used, for example, in real-time learning scenarios where a steady stream of data is arriving, and predictions must be made before all of the data is seen. Because they do not require the whole data set to be stored or loaded into memory, sequential methods are also useful for large data sets. Maximum likelihood methods can also be cast into a sequential framework.
If our goal is to predict, as best we can, the outcome of the next trial, then we must evaluate the predictive distribution of x, given the observed data set D. From the sum and product rules of probability, this takes the form

1

1

p(x = 1|D) = p(x = 1|µ)p(µ|D) dµ = µp(µ|D) dµ = E[µ|D]. (2.19)

0

0

Using the result (2.18) for the posterior distribution p(µ|D), together with the result (2.15) for the mean of the beta distribution, we obtain

p(x = 1|D) = m + a m+a+l+b

(2.20)

which has a simple interpretation as the total fraction of observations (both real observations and ﬁctitious prior observations) that correspond to x = 1. Note that in the limit of an inﬁnitely large data set m, l → ∞ the result (2.20) reduces to the maximum likelihood result (2.8). As we shall see, it is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an inﬁnitely

74

2. PROBABILITY DISTRIBUTIONS

Exercise 2.7 Exercise 2.8

large data set. For a ﬁnite data set, the posterior mean for µ always lies between the prior mean and the maximum likelihood estimate for µ corresponding to the relative frequencies of events given by (2.7).
From Figure 2.2, we see that as the number of observations increases, so the posterior distribution becomes more sharply peaked. This can also be seen from the result (2.16) for the variance of the beta distribution, in which we see that the variance goes to zero for a → ∞ or b → ∞. In fact, we might wonder whether it is a general property of Bayesian learning that, as we observe more and more data, the uncertainty represented by the posterior distribution will steadily decrease.
To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold. Consider a general Bayesian inference problem for a parameter θ for which we have observed a data set D, described by the joint distribution p(θ, D). The following result

Eθ[θ] = ED [Eθ[θ|D]]

(2.21)

where

Eθ[θ] ≡ p(θ)θ dθ

(2.22)

ED[Eθ[θ|D]] ≡

θp(θ|D) dθ p(D) dD

(2.23)

says that the posterior mean of θ, averaged over the distribution generating the data, is equal to the prior mean of θ. Similarly, we can show that

varθ[θ] = ED [varθ[θ|D]] + varD [Eθ[θ|D]] .

(2.24)

The term on the left-hand side of (2.24) is the prior variance of θ. On the righthand side, the ﬁrst term is the average posterior variance of θ, and the second term measures the variance in the posterior mean of θ. Because this variance is a positive quantity, this result shows that, on average, the posterior variance of θ is smaller than the prior variance. The reduction in variance is greater if the variance in the posterior
mean is greater. Note, however, that this result only holds on average, and that for a
particular observed data set it is possible for the posterior variance to be larger than
the prior variance.

2.2. Multinomial Variables
Binary variables can be used to describe quantities that can take one of two possible values. Often, however, we encounter discrete variables that can take on one of K possible mutually exclusive states. Although there are various alternative ways to express such variables, we shall see shortly that a particularly convenient representation is the 1-of-K scheme in which the variable is represented by a K-dimensional vector x in which one of the elements xk equals 1, and all remaining elements equal

Section 2.4 Appendix E

2.2. Multinomial Variables

75

0. So, for instance if we have a variable that can take K = 6 states and a particular

observation of the variable happens to correspond to the state where x3 = 1, then x

will be represented by

x = (0, 0, 1, 0, 0, 0)T.

(2.25)

Note that such vectors satisfy

K k=1

xk

=

1.

If

we

denote

the

probability

of

xk

=

1

by the parameter µk, then the distribution of x is given

K
p(x|µ) = µxkk
k=1

(2.26)

where µ = (µ1, . . . , µK)T, and the parameters µk are constrained to satisfy µk 0 and k µk = 1, because they represent probabilities. The distribution (2.26) can be regarded as a generalization of the Bernoulli distribution to more than two outcomes.
It is easily seen that the distribution is normalized

K

p(x|µ) = µk = 1

x

k=1

(2.27)

and that

E[x|µ] = p(x|µ)x = (µ1, . . . , µM )T = µ.

(2.28)

x

Now consider a data set D of N independent observations x1, . . . , xN . The corresponding likelihood function takes the form

N
p(D|µ) =

K µxknk = K µ(kPn xnk) = K µm k k .

n=1 k=1

k=1

k=1

(2.29)

We see that the likelihood function depends on the N data points only through the K quantities

mk = xnk

(2.30)

n

which represent the number of observations of xk = 1. These are called the sufﬁcient statistics for this distribution.
In order to ﬁnd the maximum likelihood solution for µ, we need to maximize ln p(D|µ) with respect to µk taking account of the constraint that the µk must sum to one. This can be achieved using a Lagrange multiplier λ and maximizing

K

K

mk ln µk + λ

µk − 1 .

k=1

k=1

Setting the derivative of (2.31) with respect to µk to zero, we obtain

(2.31)

µk = −mk/λ.

(2.32)

76

2. PROBABILITY DISTRIBUTIONS

Exercise 2.9

We can solve for the Lagrange multiplier λ by substituting (2.32) into the constraint

k µk = 1 to give λ = −N . Thus we obtain the maximum likelihood solution in

the form

µM k L

=

mk N

(2.33)

which is the fraction of the N observations for which xk = 1. We can consider the joint distribution of the quantities m1, . . . , mK, conditioned
on the parameters µ and on the total number N of observations. From (2.29) this

takes the form

Mult(m1, m2, . . . , mK|µ, N ) =

N m1m2 . . . mK

K
µm k k
k=1

(2.34)

which is known as the multinomial distribution. The normalization coefﬁcient is the

number of ways of partitioning N objects into K groups of size m1, . . . , mK and is

given by

N

N!

=

.

m1m2 . . . mK

m1!m2! . . . mK !

(2.35)

Note that the variables mk are subject to the constraint

K
mk = N.
k=1

(2.36)

2.2.1 The Dirichlet distribution

We now introduce a family of prior distributions for the parameters {µk} of the multinomial distribution (2.34). By inspection of the form of the multinomial
distribution, we see that the conjugate prior is given by

K
p(µ|α) ∝ µαk k−1
k=1

(2.37)

where 0 µk 1 and k µk = 1. Here α1, . . . , αK are the parameters of the distribution, and α denotes (α1, . . . , αK)T. Note that, because of the summation constraint, the distribution over the space of the {µk} is conﬁned to a simplex of dimensionality K − 1, as illustrated for K = 3 in Figure 2.4.
The normalized form for this distribution is by

Dir(µ|α)

=

Γ(α0) Γ(α1) · · · Γ(αK )

K
µαk k−1
k=1

(2.38)

which is called the Dirichlet distribution. Here Γ(x) is the gamma function deﬁned

by (1.141) while

K

α0 = αk.

(2.39)

k=1

2.2. Multinomial Variables

77

Figure 2.4 The Dirichlet distribution over three variables µ1, µ2, µ3

µ2

is conﬁned to a simplex (a bounded linear manifold) of

the form shownP, as a consequence of the constraints 0 µk 1 and k µk = 1.

µ1

µ3

Plots of the Dirichlet distribution over the simplex, for various settings of the parameters αk, are shown in Figure 2.5.
Multiplying the prior (2.38) by the likelihood function (2.34), we obtain the posterior distribution for the parameters {µk} in the form

K
p(µ|D, α) ∝ p(D|µ)p(µ|α) ∝ µαk k+mk−1.
k=1

(2.40)

We see that the posterior distribution again takes the form of a Dirichlet distribution, conﬁrming that the Dirichlet is indeed a conjugate prior for the multinomial. This allows us to determine the normalization coefﬁcient by comparison with (2.38) so that

p(µ|D, α) = Dir(µ|α + m)

=

Γ(α1

+

Γ(α0 m1) · ·

+ N) · Γ(αK

+

mK )

K k=1

µαk k+mk−1

(2.41)

where we have denoted m = (m1, . . . , mK)T. As for the case of the binomial distribution with its beta prior, we can interpret the parameters αk of the Dirichlet prior as an effective number of observations of xk = 1.
Note that two-state quantities can either be represented as binary variables and

Lejeune Dirichlet
1805–1859
Johann Peter Gustav Lejeune Dirichlet was a modest and reserved mathematician who made contributions in number theory, mechanics, and astronomy, and who gave the ﬁrst rigorous analysis of Fourier series. His family originated from Richelet in Belgium, and the name Lejeune Dirichlet comes

from ‘le jeune de Richelet’ (the young person from Richelet). Dirichlet’s ﬁrst paper, which was published in 1825, brought him instant fame. It concerned Fermat’s last theorem, which claims that there are no positive integer solutions to xn + yn = zn for n > 2. Dirichlet gave a partial proof for the case n = 5, which was sent to Legendre for review and who in turn completed the proof. Later, Dirichlet gave a complete proof for n = 14, although a full proof of Fermat’s last theorem for arbitrary n had to wait until the work of Andrew Wiles in the closing years of the 20th century.

78

2. PROBABILITY DISTRIBUTIONS

Figure 2.5 Plots of the Dirichlet distribution over three variables, where the two horizontal axes are coordinates in the plane of the simplex and the vertical axis corresponds to the value of the density. Here {αk} = 0.1 on the left plot, {αk} = 1 in the centre plot, and {αk} = 10 in the right plot.
modelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled using the multinomial distribution (2.34) with K = 2.

2.3. The Gaussian Distribution

Section 1.6 Exercise 2.14

The Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable x, the Gaussian distribution can be written in the form

N (x|µ, σ2)

=

1 (2πσ2)1/2

exp

−

1 2σ2

(x

−

µ)2

(2.42)

where µ is the mean and σ2 is the variance. For a D-dimensional vector x, the multivariate Gaussian distribution takes the form

1

1

N (x|µ, Σ) = (2π)D/2 |Σ|1/2 exp

− 1 (x − µ)TΣ−1(x − µ) 2

(2.43)

where µ is a D-dimensional mean vector, Σ is a D × D covariance matrix, and |Σ| denotes the determinant of Σ.
The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example, we have already seen that for a single real variable, the distribution that maximizes the entropy is the Gaussian. This property applies also to the multivariate Gaussian.
Another situation in which the Gaussian distribution arises is when we consider the sum of multiple random variables. The central limit theorem (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases (Walker, 1969). We can

