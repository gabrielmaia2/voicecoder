MANY-TO-MANY VOICE CONVERSION USING CYCLECONSISTENT VARIATIONAL AUTOENCODER WITH MULTIPLE
DECODERS
Keonnyeong Lee, In-Chul Yoo, and Dongsuk Yook
Artificial Intelligence Laboratory, Department of Computer Science and Engineering Korea University, Republic of Korea
{gnl0813, icyoo, yook}@ai.korea.ac.kr

ABSTRACT
One of the obstacles in many-to-many voice conversion is the requirement of the parallel training data, which contain pairs of utterances with the same linguistic content spoken by different speakers. Since collecting such parallel data is a highly expensive task, many works attempted to use non-parallel training data for many-to-many voice conversion. One of such approaches is using the variational autoencoder (VAE). Though it can handle many-to-many voice conversion without the parallel training, the VAE based voice conversion methods suffer from low sound qualities of the converted speech. One of the major reasons is because the VAE learns only the selfreconstruction path. The conversion path is not trained at all. In this paper, we propose a cycle consistency loss for VAE to explicitly learn the conversion path. In addition, we propose to use multiple decoders to further improve the sound qualities of the conventional VAE based voice conversion methods. The effectiveness of the proposed method is validated using objective and the subjective evaluations.
1. INTRODUCTION
Voice conversion (VC) is a task of converting the speakerrelated voice characteristics in an utterance while maintaining the linguistic information. Conventional VC methods require parallel speech data for the model training. The parallel speech data contain pairs of utterances that have the same linguistic contents spoken by different speakers. However, such parallel speech data are highly expensive that they restrict the use of VC in many applications. Therefore, many recent VC approaches attempted to use non-parallel training data. Early works using non-parallel training data adopt Gaussian mixture models (GMM) [1, 2, 3]. Recently, deep learning based VC approaches that have shown promising results use cycle-consistent adversarial networks (CycleGAN) [4, 5, 6, 7, 8], variational autoencoders (VAE) [9, 10, 11, 12], and VAE with generative adversarial networks (GAN) [13, 14].
In the CycleGAN [15] based VC approaches, the speech features of a source speaker are converted to match the characteristics of a target speaker using a GAN [16], and the converted speech features are again converted back through another GAN to match the original speech features from the source speaker. By using the cycle-consistency loss [17], the linguistic contents are forced to be retained in the converted speech. However, the CycleGAN can learn only one-to-one mapping between two speakers. To achieve complete mapping among ğ‘›ğ‘› speakers, ğ‘›ğ‘›(ğ‘›ğ‘› âˆ’ 1)â„2 CycleGAN models must be trained separately, which increases the training time and the

memory space prohibitively. Though the extensions of the CycleGAN for many-to-many VC have been proposed [6, 7, 8], they do not scale well as the number of speakers increases. For example, the number of speakers used in the experiments [6, 7, 8] were at most 4.
The VAE based VC approaches, on the other hand, can perform many-to-many VC for hundreds of speakers using nonparallel training data. A VAE [18] is composed of an encoder and a decoder. In the VC task, the encoder transforms the input speech features into the latent vectors containing the linguistic information of the input speech. Then, the latent vectors together with a target speaker identity vector, which is typically represented as a one-hot vector, are fed into the decoder to generate the converted speech features of the target speaker. Since the decoder is conditioned on a target speaker identity vector, it is sometimes called the conditional VAE.
Though the VAE models can be trained quickly, the sound qualities of the converted speech are usually low. To improve the sound quality, a VAE and Wasserstein generative adversarial network (WGAN) [19] hybrid called variational autoencoding Wasserstein generative adversarial networks (VAEWGAN) [13] was proposed. In this method, the decoder of the VAE is considered as the generator of a WGAN in order to train the decoder better. Though VAEWGAN based VC reduces some muffled sound, the qualities of the converted speech are still unsatisfactory.
One of the major drawbacks of the VAE based VC approaches is that the VAE models are not explicitly trained to convert the speech from a source speaker to a target speaker. Rather, they are trained to recover the same input speech from the source speaker using the latent vectors and the source speaker identity vector. In this paper, we propose to utilize a cycle consistency loss for the VAE to explicitly learn the mapping from a source speaker to a target speaker. To improve the sound quality further, we also propose a multi-decoder VAE which has a separate decoder for each target speaker. The cycle consistency loss and the multiple decoders can be incorporated into the VAEWGAN as well [20].
The rest of the paper is organized as follows. In Section 2, we describe the proposed methods in detail. Section 3 analyzes the experimental results, and Section 4 concludes the paper.
2. CYCLE-CONSISTENT VAE AND VAEWGAN
2.1. Variational Autoencoder
The loss function of the VAE is defined as follows:

â„’VAE(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹) = ğ”»ğ”»KL ï¿½ğ‘ğ‘ğœ™ğœ™(ğ‘§ğ‘§|ğ‘¥ğ‘¥) || ğ‘ğ‘(ğ‘§ğ‘§)ï¿½ âˆ’

ğ”¼ğ”¼ğ‘§ğ‘§~ğ‘ğ‘ğœ™ğœ™(ğ‘§ğ‘§|ğ‘¥ğ‘¥)ï¿½logï¿½ğ‘ğ‘ğœƒğœƒ(ğ‘¥ğ‘¥|ğ‘§ğ‘§, ğ‘‹ğ‘‹)ï¿½ï¿½ ,

(1)

where ğ”»ğ”»KL is the Kullback-Leibler divergence, ğ‘ğ‘ğœ™ğœ™(ğ‘§ğ‘§|ğ‘¥ğ‘¥) is an encoding model with parameter ğœ™ğœ™ that infers the linguistic information of input speech ğ‘¥ğ‘¥, ğ‘ğ‘(ğ‘§ğ‘§) is a prior distribution for latent vector ğ‘§ğ‘§ , and ğ‘ğ‘ğœƒğœƒ(ğ‘¥ğ‘¥|ğ‘§ğ‘§, ğ‘‹ğ‘‹) is a decoding model with parameter ğœƒğœƒ that generates the reconstructed speech using ğ‘§ğ‘§ and source speaker identity vector ğ‘‹ğ‘‹.
To convert the speech from a source speaker to a target
speaker, the source speaker identity vector ğ‘‹ğ‘‹ is replaced with the target speaker identity vector Y. By minimizing equation
(1), the VAE is trained to reconstruct the input speech from the
latent vector ğ‘§ğ‘§ and the source speaker identity vector ğ‘‹ğ‘‹. Due to the absence of explicit model training for the conversion
between the source speaker and the target speaker (i.e., only
self-reconstruction training), the VAE based VC methods
generally produce the converted speech with low sound quality.

2.2. Variational Autoencoder with Wasserstein Generative Adversarial Network
The VAEWGAN has been proposed to improve the sound quality of the VAE based VC method. In this approach, the decoder of the VAE is the generator of the WGAN. The loss function of the WGAN is defined as follows:

â„’WGAN(ğœƒğœƒ, ğœ“ğœ“; ğœ™ğœ™, ğ‘¥ğ‘¥, ğ‘Œğ‘Œ) = ğ”¼ğ”¼ğ‘¦ğ‘¦ï¿½ğ·ğ·ğœ“ğœ“(ğ‘¦ğ‘¦)ï¿½ âˆ’

ğ”¼ğ”¼ğ‘§ğ‘§~ğ‘ğ‘ğœ™ğœ™(ğ‘§ğ‘§|ğ‘¥ğ‘¥)ï¿½ğ·ğ·ğœ“ğœ“ï¿½ğºğºğœƒğœƒ(ğ‘§ğ‘§)ï¿½ï¿½ ,

(2)

where ğºğºğœƒğœƒ is the generator with parameter ğœƒğœƒ , ğ·ğ·ğœ“ğœ“ is the
discriminator with parameter ğœ“ğœ“, and ğ‘¦ğ‘¦ is the speech from the target speaker represented by speaker identity vector ğ‘Œğ‘Œ. Since the decoder of the VAE is the generator of the WGAN, ğºğºğœƒğœƒ is ğ‘ğ‘ğœƒğœƒ.
Now, the loss function of the VAEWGAN is defined as
follows:

â„’VAEWGAN(ğœ™ğœ™, ğœƒğœƒ, ğœ“ğœ“; ğ‘¥ğ‘¥, ğ‘Œğ‘Œ) = â„’VAE(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹) + ğœ†ğœ†1â„’WGAN(ğœƒğœƒ, ğœ“ğœ“; ğœ™ğœ™, ğ‘¥ğ‘¥, ğ‘Œğ‘Œ) , (3)
where ğœ†ğœ†1 is the weight of the WGAN loss. Equation (3) is minimized for the VAE and the generator, and it is maximized for the discriminator. First, the VAE is trained in the same way as in Section 2.1. Second, the VAE and the WGAN are jointly trained such that the VAE gets an additional error signal from the discriminator of the WGAN.
Though the VAEWGAN produces somewhat higher sound quality than the VAE, it can handle only one-to-one voice conversion. In the next sections, we propose the extensions of the VAE and the VAEWGAN, called cycle-consistent VAE (CycleVAE) and cycle-consistent VAEWGAN (CycleVAEWGAN), respectively, which can improve the performance for many-to-many voice conversion by using multiple decoders and explicitly learning many-to-many mapping functions.

2.3. Cycle-Consistent Variational Autoencoder (CycleVAE)
In order to improve the sound quality of the VAE based VC, we propose to use a separate decoder for each speaker instead of a

ğ‘¥ğ‘¥ Encoder

ğ‘¦ğ‘¦

ğ‘ğ‘ğœ™ğœ™

Decoder ğ‘ğ‘ğœƒğœƒğ‘‹ğ‘‹

ğ‘¦ğ‘¦Yâ€² â†’X ğ‘¥ğ‘¥Xâ€²â€² â†’Yâ†’X ğ‘¥ğ‘¥Xâ€² â†’X

Decoder ğ‘ğ‘ğœƒğœƒğ‘Œğ‘Œ

ğ‘¦ğ‘¦Yâ€²â€²â†’ Xâ†’Y ğ‘¦ğ‘¦Yâ€² â†’Y
ğ‘¥ğ‘¥Xâ€² â†’Y

Figure 1. CycleVAE.

single decoder for all speakers. We also propose to use the cycle consistency loss for explicit conversion path training. The speaker identity vectors are not needed for the multi-decoder VAE since each speaker has an independent decoder. It can be expected that the sound quality can be improved since each decoder learns its corresponding speakerâ€™s voice characteristics by the additional conversion path training while the conventional VAE must cover multiple speakers with only a single decoder by self-reconstruction training.
Fig. 1 shows the concept of the CycleVAE for two speakers.
When the speech ğ‘¥ğ‘¥ from speaker ğ‘‹ğ‘‹ is fed into the network, it passes through the encoder and is compressed into the latent
vector ğ‘§ğ‘§ . The reconstruction error is computed using the reconstructed speech ğ‘¥ğ‘¥ğ‘‹â€²ğ‘‹â†’ğ‘‹ğ‘‹ by the speaker ğ‘‹ğ‘‹ decoder model ğ‘ğ‘ğœƒğœƒğ‘‹ğ‘‹. Up to this point, the loss function is similar to the vanilla VAE except that it does not require the speaker identity vectors, which is as follows:

â„’Vâ€² AE(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹) = ğ”»ğ”»KL ï¿½ğ‘ğ‘ğœ™ğœ™(ğ‘§ğ‘§|ğ‘¥ğ‘¥) || ğ‘ğ‘(ğ‘§ğ‘§)ï¿½ âˆ’

ğ”¼ğ”¼ğ‘§ğ‘§~ğ‘ğ‘ğœ™ğœ™(ğ‘§ğ‘§|ğ‘¥ğ‘¥) ï¿½log ï¿½ğ‘ğ‘ğœƒğœƒğ‘‹ğ‘‹(ğ‘¥ğ‘¥|ğ‘§ğ‘§)ï¿½ï¿½ .

(4)

The same input speech ğ‘¥ğ‘¥ from speaker ğ‘‹ğ‘‹ goes through the
encoder and the speaker ğ‘Œğ‘Œ decoder model ğ‘ğ‘ğœƒğœƒğ‘Œğ‘Œ as well to generate the converted speech ğ‘¥ğ‘¥ğ‘‹â€²ğ‘‹â†’ğ‘Œğ‘Œ which has the same linguistic contents as ğ‘¥ğ‘¥ but in speaker ğ‘Œğ‘Œ â€™s voice. Then, the converted speech ğ‘¥ğ‘¥ğ‘‹â€²ğ‘‹â†’ğ‘Œğ‘Œ goes through the encoder and ğ‘ğ‘ğœƒğœƒğ‘‹ğ‘‹ to generate the converted back speech ğ‘¥ğ‘¥ğ‘‹â€²ğ‘‹â€²â†’ğ‘Œğ‘Œâ†’ğ‘‹ğ‘‹ which should recover the original speech ğ‘¥ğ‘¥ . This cyclic conversion
encourages the explicit training of voice conversion from ğ‘‹ğ‘‹ to
ğ‘Œğ‘Œ and ğ‘Œğ‘Œ to ğ‘‹ğ‘‹. The cycle consistency loss of the multi-decoder
VAE is defined as follows:

â„’cycle(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹, ğ‘Œğ‘Œ) = ğ”»ğ”»KL ï¿½ğ‘ğ‘ğœ™ğœ™ï¿½ğ‘§ğ‘§ï¿½ğ‘¥ğ‘¥ğ‘‹â€²ğ‘‹â†’ğ‘Œğ‘Œï¿½ || ğ‘ğ‘(ğ‘§ğ‘§)ï¿½ âˆ’ ğ”¼ğ”¼ğ‘§ğ‘§~ğ‘ğ‘ğœ™ğœ™ï¿½ğ‘§ğ‘§|ğ‘¥ğ‘¥ğ‘‹â€²ğ‘‹â†’ğ‘Œğ‘Œï¿½ ï¿½log ï¿½ğ‘ğ‘ğœƒğœƒğ‘‹ğ‘‹(ğ‘¥ğ‘¥|ğ‘§ğ‘§)ï¿½ï¿½ . (5)
Now, given the input speech ğ‘¥ğ‘¥ from speaker ğ‘‹ğ‘‹, the loss function of the CycleVAE for two speakers can be defined as follows:
â„’CycleVAE(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹, ğ‘Œğ‘Œ) = â„’Vâ€² AE(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹) + ğœ†ğœ†2â„’cycle(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹, ğ‘Œğ‘Œ) , (6)
where ğœ†ğœ†2 is the weight of the cycle consistency loss. It can be easily extended for more than two speakers by
summing over all pairs of the training speakers. The loss function of the CycleVAE for more than two speakers can be computed as follows for the input speech ğ‘¥ğ‘¥ from speaker ğ‘‹ğ‘‹:

âˆ‘ğ‘Œğ‘Œ â„’CycleVAE(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹, ğ‘Œğ‘Œ) .

(7)

2.4. Cycle-Consistent Variational Autoencoder with Wasserstein Generative Adversarial Network (CycleVAEWGAN)
The CycleVAE can be extended to utilize the WGAN as in the VAEWGAN case. In the CycleVAEWGAN, the decoders of the CycleVAE are shared with the generators of the WGANs. Each decoder has its own WGAN. Fig. 2 shows the concept of the CycleVAEWGAN for two speakers. Since there are multiple WGANs, equation (2) is modified as follows:

â„’Wâ€² GAN(ğœƒğœƒ, ğœ“ğœ“; ğœ™ğœ™, ğ‘¥ğ‘¥, ğ‘Œğ‘Œ) = ğ”¼ğ”¼ğ‘¦ğ‘¦ï¿½ğ·ğ·ğœ“ğœ“ğ‘Œğ‘Œ(ğ‘¦ğ‘¦)ï¿½ âˆ’ ğ”¼ğ”¼ğ‘§ğ‘§~ğ‘ğ‘ğœ™ğœ™(ğ‘§ğ‘§|ğ‘¥ğ‘¥) ï¿½ğ·ğ·ğœ“ğœ“ğ‘Œğ‘Œ ï¿½ğºğºğœƒğœƒğ‘Œğ‘Œ(ğ‘§ğ‘§)ï¿½ï¿½ , (8)

where ğºğºğœƒğœƒğ‘Œğ‘Œ is the generator with parameter ğœƒğœƒğ‘Œğ‘Œ for speaker ğ‘Œğ‘Œ, ğ·ğ·ğœ“ğœ“ğ‘Œğ‘Œ is the discriminator with parameter ğœ“ğœ“ğ‘Œğ‘Œ for speaker ğ‘Œğ‘Œ, and ğ‘¦ğ‘¦ is the speech from target speaker ğ‘Œğ‘Œ. Since the decoders of the CycleVAE are the generators of the WGANs, ğºğºğœƒğœƒğ‘Œğ‘Œ is ğ‘ğ‘ğœƒğœƒğ‘Œğ‘Œ.
Now, the loss function of the CycleVAEWGAN is defined
as follows:

â„’CycleVAEWGAN(ğœ™ğœ™, ğœƒğœƒ, ğœ“ğœ“; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹, ğ‘Œğ‘Œ) =

â„’CycleVAE(ğœ™ğœ™, ğœƒğœƒ; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹, ğ‘Œğ‘Œ) +

ğœ†ğœ†1â„’Wâ€² GAN(ğœƒğœƒ, ğœ“ğœ“; ğœ™ğœ™, ğ‘¥ğ‘¥, ğ‘‹ğ‘‹) +

ğœ†ğœ†1â„’Wâ€² GAN(ğœƒğœƒ, ğœ“ğœ“; ğœ™ğœ™, ğ‘¥ğ‘¥, ğ‘Œğ‘Œ) .

(9)

Note that â„’Wâ€² GAN is used twice in the equation, i.e., one for the self-reconstruction path and the other for the conversion path. Equation (9) is minimized for the CycleVAE and the generators, and is maximized for the discriminators. The first stage of the CycleVAEWGAN training is identical to the training procedure of the CycleVAE. In the second stage of the training, the CycleVAE and the WGANs are jointly optimized where the CycleVAE receives the additional error signals from the WGANs.
It also can be easily extended to more than two speakers by summing over all pairs of the training speakers. The loss function of the CycleVAEWGAN for more than two speakers can be computed as follows for the input speech ğ‘¥ğ‘¥ from speaker ğ‘‹ğ‘‹:

ğ‘¥ğ‘¥ Encoder

ğ‘¦ğ‘¦

ğ‘ğ‘ğœ™ğœ™

ğ‘¥ğ‘¥

Decoder

ğ‘ğ‘ğœƒğœƒğ‘‹ğ‘‹

ğ·ğ·ğœ“ğœ“ğ‘‹ğ‘‹

Decoder

ğ·ğ·ğœ“ğœ“ğ‘Œğ‘Œ

ğ‘ğ‘ğœƒğœƒğ‘Œğ‘Œ

ğ‘¦ğ‘¦

Figure 2. CycleVAEWGAN.

âˆ‘ğ‘Œğ‘Œ â„’CycleVAEWGAN(ğœ™ğœ™, ğœƒğœƒ, ğœ“ğœ“; ğ‘¥ğ‘¥, ğ‘‹ğ‘‹, ğ‘Œğ‘Œ) .

(10)

The proposed CycleVAEWGAN is different from [12] in that it can utilize multiple decoders and WGANs.

3. EXPERIMENTS
For the experiments, 2 male speakers and 2 female speakers, namely SF1, SF2, TM1 and TM2, from VCC2018 dataset [21] were used. The numbers of the training and the testing utterances per speaker were 81 and 35, respectively. The speech were down-sampled to 22.05 kHz, and 36-dimensional Melfrequency cepstral coefficients (MFCC), aperiodicities (AP), and fundamental frequency (F0) were extracted using the WORLD speech analyzer [22].
The encoders, the decoders, and the discriminators used the gated linear units (GLU) [23]. The batch normalization [24] was applied to each convolutional neural network (CNN) [25] layers. We built our models based on [11]. Fig. 3 shows the details of the encoder, decoder, and discriminator. We used the Adam optimizer [26] with a batch size of 8. ğœ†ğœ†1 and ğœ†ğœ†2 were set to 0 and 1, respectively, when training the CycleVAE. For the training of the CycleVAEWGAN, ğœ†ğœ†1 and ğœ†ğœ†2 were set to 1. All experiments were repeated 5 times starting with randomly initialized weights.

3.1. Objective Evaluations
One of drawbacks of the VAE based approaches is the oversmoothing of the generated data [27]. The global variance (GV) of MFCCs can be used to measure the degree of oversmoothing as the high GV values correlate with the sharpness

Encoder
H: 36 K: 3x9 W: 128 S: 1x1 C: 1 C: 5

K: 4x8 S: 2x2 C: 10

K: 4x8 S: 2x2 C: 10

K: 9x5 S: 9x1 C: 16

H: 1 W: 32 C: 16

Decoder

H: 1 W: 32 C: 8

K: 9x5 S: 9x1 C: 10

K: 4x8 S: 2x2 C: 10

K: 4x8 S: 2x2 C: 5

K: 3x9 S: 1x1 C: 2

H: 36 W: 128 C: 2

Discriminator
H: 36 K: 4x4 K: 4x4 K: 4x4 K: 3x4 K: 1x1 H: 4 H: 1 W: 128 S: 2x2 S: 2x2 S: 2x2 S: 1x2 S: 1x1 W: 8 W: 1 C: 1 C: 8 C: 16 C: 32 C: 16 C: 1 C: 1 C: 1

CNN
Fully Connected
Output

Input

DeCNN Output

Input

CNN Output

Input

CNN Batch Norm
GLU

DeCNN Batch Norm
GLU

CNN Batch Norm
GLU

Source ID C: 4

Target ID C: 4

Target ID C: 4

Figure 3. The architectures of the encoder, decoder, and discriminator used in the experiments. The target speaker identity vectors (Target ID) are not used for the multi-decoder CycleVAE and CycleVAEWGAN.

GV

10
Real

1

VAEWGAN

CycleVAE

0.1

0.01

0.001

1

6

11

16

21

26

31

36

MFCC Index

Figure 4. Global variance of MFCCs for real speech utterances and the converted utterances by the VAE and the CycleVAE.

of the spectra. We computed the GV for each of the MFCC indices. Fig. 4 shows the average GV over all evaluation utterances for the real speech and the converted speech by the conventional VAEWGAN and the proposed CycleVAE. The average GVs over all indices and all evaluation utterances were 0.247, 0.200, and 0.210 for the real speech and the converted speech by the VAEWGAN and CycleVAE, respectively.
For the case of the original and the converted speech utterances containing the same linguistic information, the difference between the MFCCs of the two speech utterances should be small. We used two metrics to measure this difference, i.e., the Mel-cepstral distortion (MCD) [27] and the modulation spectral distance (MSD) [28]. Tables 1 and 2 show MCD and MSD, respectively, for various VC methods. Firstly, by comparing the baseline VAE and VAEWGAN columns in the tables, we confirmed that the VAEWGAN outperforms the VAE [13]. Secondly, to measure the effectiveness of the cycle consistency loss alone, we built the CycleVAE and the CycleVAEWGAN that use a single common decoder (i.e., ğ‘ğ‘ğœƒğœƒğ‘‹ğ‘‹ and ğ‘ğ‘ğœƒğœƒğ‘Œğ‘Œ are shared in Fig. 1). The fourth and fifth columns of the tables shows these results. By comparing the second and the

fourth columns (or the third and the fifth columns) of the tables, we confirmed the effectiveness of the cycle consistency loss [12]. Finally, the results of the proposed multi-decoder approaches with the cycle consistency loss are shown in the last two columns of the tables. It can be seen that the multi-decoder approaches improve the performances further. It is interesting to note that unlike the VAE or the CycleVAE having a single common decoder, adding the WGANs to the CycleVAE with multiple decoders does not improve the performance further. It is believed that because the multi-decoder cycle consistency loss is effective enough in learning the conversion path explicitly, the additional WGANs for conversion path learning may not be necessary.
3.2. Subjective Evaluations
We also conducted two subjective evaluations, i.e., naturalness test and similarity test. A set of 16 utterances was selected randomly such that four utterances were assigned to each pair of F to F, M to F, F to M, and M to M conversions, where F and M represent female and male, respectively. A total of 48 utterances (16 target speakersâ€™ utterances, 16 converted utterances by the conventional VAEWGAN, and 16 converted utterances by the proposed CycleVAE with multiple decoders) were played to 10 listeners participated in the subjective evaluations.
The mean opinion score (MOS) was used for the naturalness test. The listeners evaluated the naturalness of the speech in the scales of 1 (bad) to 5 (excellent) when the utterances were played in random order. Table 3 shows that the proposed CycleVAE based VC generally exhibits higher naturalness scores than the conventional VAEWGAN based VC.
In the similarity test, a target speakerâ€™s utterance was played first, then a pair of two converted utterances by the two methods were played in random order. The listeners were asked to select

F to F M to F F to M M to M Average

VAE
7.31 Â± 0.41 7.75 Â± 0.57 7.32 Â± 0.44 7.40 Â± 0.33 7.45 Â± 0.44

Table 1. MCD with standard deviation.

VAEWGAN
7.33 Â± 0.38 7.54 Â± 0.52 7.35 Â± 0.40 7.27 Â± 0.31 7.37 Â± 0.40

CycleVAE CycleVAEWGAN CycleVAE CycleVAEWGAN (single decoder) (single decoder) (multi-decoder) (multi-decoder)

7.20 Â± 0.42

7.11 Â± 0.43

6.97 Â± 0.41

7.05 Â± 0.41

7.45 Â± 0.53

7.42 Â± 0.52

7.23 Â± 0.56

7.31 Â± 0.52

7.17 Â± 0.41

7.21 Â± 0.39

7.03 Â± 0.44

7.11 Â± 0.43

7.17 Â± 0.31

7.10 Â± 0.30

7.00 Â± 0.31

7.07 Â± 0.31

7.25 Â± 0.42

7.21 Â± 0.41

7.06 Â± 0.43

7.13 Â± 0.42

F to F M to F F to M M to M Average

VAE
1.87 Â± 0.16 1.85 Â± 0.14 1.84 Â± 0.17 1.85 Â± 0.16 1.86 Â± 0.16

Table 2. MSD with standard deviation.

VAEWGAN
1.85 Â± 0.16 1.83 Â± 0.14 1.83 Â± 0.17 1.84 Â± 0.17 1.84 Â± 0.16

CycleVAE CycleVAEWGAN CycleVAE CycleVAEWGAN (single decoder) (single decoder) (multi-decoder) (multi-decoder)

1.86 Â± 0.16

1.84 Â± 0.16

1.85 Â± 0.15

1.85 Â± 0.16

1.84 Â± 0.14

1.83 Â± 0.14

1.82 Â± 0.13

1.82 Â± 0.14

1.82 Â± 0.16

1.80 Â± 0.16

1.82 Â± 0.17

1.83 Â± 0.17

1.83 Â± 0.17

1.82 Â± 0.17

1.82 Â± 0.16

1.82 Â± 0.17

1.84 Â± 0.16

1.82 Â± 0.16

1.83 Â± 0.15

1.83 Â± 0.16

Table 3. Sound quality test (MOS and standard deviation).

F to F M to F F to M M to M Average

VAEWGAN 2.83 Â± 0.86 2.15 Â± 0.76 2.48 Â± 0.92 2.38 Â± 0.73 2.46 Â± 0.86

CycleVAE 3.08 Â± 0.91 2.38 Â± 0.91 2.65 Â± 0.94 2.73 Â± 0.87 2.71 Â± 0.94

Target Voice 4.89 Â± 0.39
4.88 Â± 0.40 4.88 Â± 0.39

Table 4. Similarity test (%).

VAEWGAN

Fair

F to F

15.0

47.5

M to F

15.0

25.0

F to M

2.5

45.0

M to M

15.0

37.5

Average

12.0

39.0

CycleVAE 37.5 60.0 52.5 47.5 49.0

the more similar utterance to the target speakerâ€™s speech or â€˜fairâ€™ if they cannot tell the difference. Table 4 shows that the proposed CycleVAE based VC outperforms the conventional VAEWGAN based VC significantly.
4. CONCLUSION
In this paper, we proposed the new many-to-many voice conversion methods based on the VAE. The proposed methods use multiple decoders and explicitly learn the conversion path for many-to-many voice conversion. The effectiveness of the proposed methods was validated using the objective evaluations and the subjective evaluations.
The proposed methods can be further extended by utilizing multiple encoders, i.e., one encoder for each source speaker. Also, replacing the vocoder with powerful neural vocoders such as the WaveNet [29] or the WaveRNN [30] can be another future research direction.
5. ACKNOWLEDGEMENTS
This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT and Future Planning (NRF-2017R1E1A1A01078157). Also, it was partly supported by the MSIT (Ministry of Science and ICT) under the ITRC (Information Technology Research Center) support program (IITP-2018-0-01405) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation), and IITP grant funded by the Korean government (MSIT) (No. 2018-0-00269).

6. REFERENCES
1. Yannis Stylianou, Olivier CappÃ©, and Eric Moulines, â€œContinuous probabilistic transform for voice conversion,â€ IEEE Transactions on Speech and Audio Processing, vol. 6, no. 2, pp. 131â€“142, 1998.
2. Tomoki Toda, Alan Black, and Keiichi Tokuda, â€œVoice conversion based on maximum-likelihood estimation of spectral parameter trajectory,â€ IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2222â€“2235, 2007.
3. Elina Helander, Tuomas Virtanen, Jani Nurminen, and Moncef Gabbouj, â€œVoice conversion using partial least squares regression,â€ IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 5, pp. 912â€“921, 2010.
4. Takuhiro Kaneko and Hirokazu Kameoka, â€œCycleGANVC: Non-parallel voice conversion using cycle-consistent adversarial networks,â€ European Signal Processing Conference, pp. 2114â€“2118, 2018.
5. Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, and Nobukatsu Hojo, â€œCycleGAN-VC2: Improved CycleGAN-based non-parallel voice conversion,â€ IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 6820â€“6824, 2019.
6. Dongsuk Yook, In-Chul Yoo, and Seungho Yoo, â€œVoice conversion using conditional CycleGAN,â€ International Conference on Computational Science and Computational Intelligence, pp. 1460â€“1461, 2018.
7. Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, and Nobukatsu Hojo, â€œStarGAN-VC: Non-parallel many-tomany voice conversion with star generative adversarial networks,â€ IEEE Spoken Language Technology Workshop, pp. 266â€“273, 2018.
8. Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, and Nobukatsu Hojo, â€œStarGAN-VC2: Rethinking conditional methods for StarGAN-based voice conversion,â€ Interspeech, pp. 679â€“683, 2019.
9. Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang, â€œVoice conversion from nonparallel corpora using variational autoencoder,â€ AsiaPacific Signal and Information Processing Association Annual Summit and Conference, pp. 1â€“6, 2016.
10. Aaron van den Oord and Oriol Vinyals, â€œNeural discrete representation learning,â€ Advances in Neural Information Processing Systems, pp. 6309â€“6318, 2017.
11. Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, and Nobukatsu Hojo, â€œACVAE-VC: Non-parallel voice conversion with auxiliary classifier variational autoencoder,â€ IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 9, pp. 1432â€“1443, 2019.
12. Patrick Lumban Tobing, Yi-Chiao Wu, Tomoki Hayashi, Kazuhiro Kobayashi, and Tomoki Toda, â€œNon-parallel voice conversion with cyclic variational autoencoder,â€ Interspeech, pp. 674â€“678, 2019.

13. Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang, â€œVoice conversion from unaligned corpora using variational autoencoding Wasserstein generative adversarial networks,â€ Interspeech, pp. 3364â€“3368, 2017.
14. Ju-chieh Chou, Cheng-chieh Yeh, Hung-yi Lee, and Linshan Lee, â€œMulti-target voice conversion without parallel data by adversarially learning disentangled audio representations,â€ Interspeech, pp. 501â€“505, 2018.
15. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros, â€œUnpaired image-to-image translation using cycleconsistent adversarial networks,â€ IEEE International Conference on Computer Vision, pp. 2223â€“2232, 2017.
16. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, â€œGenerative adversarial nets,â€ Advances in Neural Information Processing Systems, pp. 2672â€“2680, 2014.
17. Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei Efros, â€œLearning dense correspondence via 3D-guided cycle consistency,â€ IEEE Conference on Computer Vision and Pattern Recognition, pp. 117â€“126, 2016.
18. Diederik Kingma and Max Welling, â€œAuto-encoding variational Bayes,â€ arXiv:1312.6114, 2013.
19. Martin Arjovsky, Soumith Chintala, and Leon Bottou, â€œWasserstein generative adversarial networks,â€ International Conference on Machine Learning, pp. 214â€“ 223, 2017.
20. Keonnyeong Lee, In-Chul Yoo, and Dongsuk Yook, â€œVoice conversion using cycle-consistent variational autoencoder,â€ arXiv:1909.06805, 2019.
21. Jaime Lorenzo-Trueba, Junichi Yamagishi, Tomoki Toda, Daisuke Saito, Fernando Villavicencio, Tomi Kinnunen, and Zhenhua Ling, â€œThe voice conversion challenge 2018: Promoting development of parallel and nonparallel methods,â€ The Speaker and Language Recognition Workshop, pp. 195â€“202, 2018.
22. Masanori Morise, Fumiya Yokomori, and Kenji Ozawa, â€œWORLD: A vocoder-based high-quality speech synthesis system for real-time applications,â€ IEICE Transactions on Information and Systems, vol. 99, no. 7, pp. 1877â€“1884, 2016.
23. Yann Dauphin, Angela Fan, Michael Auli, and David Grangier, â€œLanguage modeling with gated convolutional networks,â€ International Conference on Machine Learning, pp. 933â€“941, 2017.
24. Sergey Ioffe and Christian Szegedy, â€œBatch normalization: accelerating deep network training by reducing internal covariate shift,â€ International Conference on Machine Learning, pp. 448â€“456, 2015.
25. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, â€œImagenet classification with deep convolutional neural networks,â€ Advances in Neural Information Processing Systems, pp. 1097â€“1105, 2012.
26. Diederik Kingma and Jimmy Lei Ba, â€œAdam: A method for stochastic optimization,â€ International Conference on Learning Representations, 2015.

27. Tomoki Toda, Alan Black, and Keiichi Tokuda, â€œVoice conversion based on maximum-likelihood estimation of spectral parameter trajectory,â€ IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2222â€“2235, 2007.
28. Shinnosuke Takamichi, Tomoki Toda, Alan Black, Graham Neubig, Sakriani Sakti, and Satoshi Nakamura, â€œPostfilters to modify the modulation spectrum for statistical parametric speech synthesis,â€ IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 4, pp. 755â€“767, 2016.
29. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu, â€œWavenet: A generative model for raw audio,â€ arXiv:1609.03499, 2016.
30. Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu, â€œEfficient neural audio synthesis,â€ arXiv:1802.08435, 2018.

